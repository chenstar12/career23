11. **请简要介绍下tensorflow的计算图。**

     ```
    tensorflow是一个通过计算图的形式来表述计算的编程系统，计算图也叫做数据流图，可以把计算图看做是一种有向图，Tensorflow中的每一个节点都是计算图上的一个Tensor，也就是张量，而节点之间的边描述了计算之间的依赖关系（定义时）和数学操作（运算时）
     ```

    如下图所示：

    a=x*y; b=a+z; c=tf.reduce_sum(b);

    ![](http://5b0988e595225.cdn.sohucs.com/images/20180710/724b10e2eb9741388a8e91f36b678ced.jpg)

    ![](http://5b0988e595225.cdn.sohucs.com/images/20180710/ef01099ff4614bb395a8b5a684713599.gif)

12. 你有哪些deep learning(run, cnn)调参的经验

    ```
    a. 参数初始化
    	下面几种方式,随便选一个,结果基本都差不多。但是一定要做。否则可能会减慢收敛速度，影响收敛结果，甚至造成Nan等一系列问题。
    	下面的n_in为网络的输入大小，n_out为网络的输出大小，n为n_in或(n_in+n_out)*0.5
    	
    	Xavier初始法论文：http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf
    	He初始化论文：https://arxiv.org/abs/1502.01852
    	uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])
    	Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)
    	He初始化，适用于ReLU：scale = np.sqrt(6/n)
    	normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0
    	Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)
    	He初始化，适用于ReLU：stdev = np.sqrt(2/n)
    	svd初始化：对RNN有比较好的效果。
    	
     b. 数据预处理方式
     	zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-centerX /= np.std(X, axis = 0) # normalize
     	PCA whitening,这个用的比较少.
     	
     c. 训练技巧
     	要做梯度归一化,即算出来的梯度除以minibatch size
     	clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15
     	dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入->RNN与RNN->输出的位置.关于RNN如何用dropout,可以参考这篇论文:http://arxiv.org/abs/1409.2329
     	adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。
     	除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.
     	1. sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0，很容易造成梯度消失问题。
     	2. 输入0均值，sigmoid函数的输出不是0均值的。
     	rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好。
     	word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果。
    
    d. 尽量对数据做shuffle
    	LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.
    	Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：Accelerating Deep Network Training by Reducing Internal Covariate Shift
    	如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: http://arxiv.org/abs/1505.00387
    	来自@张馨宇的技巧：一轮加正则，一轮不加正则，反复进行。（没有太明白）
    	
    e. ensemble
    	Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式
    	* 同样的参数,不同的初始化方式
    	* 不同的参数,通过cross-validation,选取最好的几组
    	* 同样的参数,模型训练的不同阶段，即不同迭代次数的模型。
    	* 不同的模型,进行线性融合. 例如RNN和传统模型。
    ```

13. **CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？**

    ```
    Deep Learning -Yann LeCun, Yoshua Bengio & Geoffrey Hinton
    Learn TensorFlow and deep learning, without a Ph.D.
    The Unreasonable Effectiveness of Deep Learning -LeCun 16 NIPS Keynote
    ```

    以上几个不相关问题的相关性在于，都存在局部与整体的关系，由低层次的特征经过组合，组成高层次的特征，并且得到不同特征之间的空间相关性。如下图：低层次的直线／曲线等特征，组合成为不同的形状，最后得到汽车的表示。

    ![](http://5b0988e595225.cdn.sohucs.com/images/20180710/89726c9e34f9469d9bd192c3cbb85c0c.jpg)

    CNN抓住此共性的手段主要有四个：局部连接 / 权值共享 / 池化操作 / 多层次结构

    局部连接使网络可以提取数据的局部特征：权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音/文本）中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图

    ![](http://5b0988e595225.cdn.sohucs.com/images/20180710/84f926b9aced4cf09763979b0cda5329.jpg)

    上图中，如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。

14. **LSTM结构推导，为什么比RNN好？**

    ```
    推导forget gate，input gate，cell state， hidden information等的变化；因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。
    ```

15. **Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数。**

    ```
    sigmoid、Tanh、ReLU的缺点在121问题中已有说明，为了解决ReLU的dead cell的情况，发明了Leaky Relu， 即在输入小于0时不让输出为0，而是乘以一个较小的系数，从而保证有导数存在。同样的目的，还有一个ELU，函数示意图如下。
    ```

    ![](http://5b0988e595225.cdn.sohucs.com/images/20180710/e9fc81364eed48a8b4342a6a25ade06c.jpg)

    还有一个激活函数是Maxout，即使用两套w,b参数，输出较大值。本质上Maxout可以看做Relu的泛化版本，因为如果一套w,b全都是0的话，那么就是普通的ReLU。Maxout可以克服Relu的缺点，但是参数数目翻倍。

    ![](http://5b0988e595225.cdn.sohucs.com/images/20180710/7e61a7057cbe41aba95391dd7aaa15f2.jpg)

16. **为什么引入非线性激励函数？**

    ```
    a. 对于神经网络来说，网络的每一层相当于f(wx+b)=f(w'x)，对于线性函数，其实相当于f(x)=x，那么在线性激活函数下，每一层相当于用一个矩阵去乘以x，那么多层就是反复的用矩阵去乘以输入。根据矩阵的乘法法则，多个矩阵相乘得到一个大矩阵。所以线性激励函数下，多层网络与一层网络相当。比如，两层的网络f(W1*f(W2x))=W1W2x=Wx。
    b. 非线性变换是深度学习有效的原因之一。原因在于非线性相当于对空间进行变换，变换完成后相当于对问题空间进行简化，原来线性不可解的问题现在变得可以解了。
    ```

    下图可以很形象的解释这个问题，左图用一根线是无法划分的。经过一系列变换后，就变成线性可解的问题了。

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS88nnX6ISHnwUWV8u74m6tGyp7ZYAZL0xL1rNwBcaic7J86JLXT5QjjZNmmNFZoKUiclYQCdh88zBcQ/640?wx_fmt=png)

    如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。

    正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释）。

17. **请问人工神经网络中为什么ReLu要好过于tanh和sigmoid function？**

    先看sigmoid、tanh和RelU的函数图

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS88nnX6ISHnwUWV8u74m6tGhCYNQs2QeQ4cwon1iaOjJ3QP47pczd1umicfw0aRGibTJwGCs7uia2XZ3A/640?wx_fmt=png)

    第一，采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法和指数运算，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。

    第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），这种现象称为饱和，从而无法完成深层网络的训练。而ReLU就不会有饱和倾向，不会有特别小的梯度出现。

    第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。

    多加一句，现在主流的做法，会多做一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。大家有兴趣可以看下。

18. **为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数，而不是选择统一一种sigmoid或者tanh？这样做的目的是什么？**

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS88nnX6ISHnwUWV8u74m6tGj6qclADW2xz4Nicd6qPpOZ91rmHOUhpeSia223huOJMYybxNfAhppqdQ/640?wx_fmt=png)

    sigmoid用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了

    tanh用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。

    二者目的不一样

19. **如何解决RNN梯度爆炸和弥散的问题？**

    为了解决梯度爆炸问题，Thomas Mikolov首先提出了一个简单的启发性的解决方案，就是当梯度大于一定阈值的的时候，将它截断为一个较小的数。具体如算法1所述：

    算法：当梯度爆炸时截断梯度（伪代码）

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS88nnX6ISHnwUWV8u74m6tGFyqytElKQFpv6tzIKiaK2akW1Nib0Z9XZib9hUXdiaLw44p6vhg7q7ibUXw/640?wx_fmt=jpeg)

    下图可视化了梯度截断的效果。它展示了一个小的rnn（其中W为权值矩阵，b为bias项）的决策面。这个模型是一个一小段时间的rnn单元组成；实心箭头表明每步梯度下降的训练过程。当梯度下降过程中，模型的目标函数取得了较高的误差时，梯度将被送到远离决策面的位置。截断模型产生了一个虚线，它将误差梯度拉回到离原始梯度接近的位置。

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS88nnX6ISHnwUWV8u74m6tGnRzc9OnzF8JaZq3XnSiaiahMl2QjA8ktuhPk2UjGC0sF2786xrPaHX3Q/640?wx_fmt=png)

    梯度爆炸，梯度截断可视化 

    为了解决梯度弥散的问题，我们介绍了两种方法。

    第一种方法是将随机初始化![640?wx_fmt=jpeg](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS88nnX6ISHnwUWV8u74m6tGS1CJtZp4Kt1UZer9Tic2H4COoibJfhgQxjXVTfuvTH6Oho79Ic08IdRA/640?wx_fmt=jpeg)改为一个有关联的矩阵初始化。

    第二种方法是使用ReLU（Rectified Linear Units）代替sigmoid函数。ReLU的导数不是0就是1.因此，神经元的梯度将始终为1，而不会当梯度传播了一定时间之后变小。

20. **什麽样的数据集不适合用深度学习？**

    1）数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。

    2）数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。


