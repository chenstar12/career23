21. **广义线性模型是怎被应用在深度学习中？**

    A Statistical View of Deep Learning (I): Recursive GLMs

    深度学习从统计学角度，可以看做递归的广义线性模型。

    广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。

    深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。

    下图是一个对照表

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpSibBsaR9xWNdic022dsmWOeFSRqlmPUmMiaOjiagIbkNrEK8pOnQmYMGS7Cla6XNQ2H7Siad209PuicsgIg/640?wx_fmt=png)

22. **如何解决梯度消失和梯度膨胀？**

    （1）梯度消失：

    根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0

    可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题。

    （2）梯度膨胀

    根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大

    可以通过激活函数来解决，或用Batch Normalization解决这个问题。

23. **简述神经网络的发展历史。**

    ```
    1949年Hebb提出了神经心理学学习范式——Hebbian学习理论
    1952年，IBM的Arthur Samuel写出了西洋棋程序
    1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型.
    3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好的应用到了感知器的训练中
    感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。
    尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。
    1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。
    时间终于走到了当下，随着计算资源的增长和数据量的增长。一个新的NN领域——深度学习出现了。
    简言之，MP模型+sgn—->单层感知机（只能线性）+sgn— Minsky 低谷 —>多层感知机+BP+sigmoid—- (低谷) —>深度学习+pre-training+ReLU/sigmoid
    ```

24. **深度学习常用方法。**

    全连接DNN（相邻层相互连接、层内无连接）

    AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）—–>特征探测器——>栈式叠加 贪心训练 

    RBM—->DBN 

    解决全连接DNN的全连接问题—–>CNN 

    解决全连接DNN的无法对时间序列上变化进行建模的问题—–>RNN—解决时间轴上的梯度消失问题——->LSTM

    DNN是传统的全连接网络，可以用于广告点击率预估，推荐等。其使用embedding的方式将很多离散的特征编码到神经网络中，可以很大的提升结果。

    CNN主要用于计算机视觉(Computer Vision)领域，CNN的出现主要解决了DNN在图像领域中参数过多的问题。同时，CNN特有的卷积、池化、batch normalization、Inception、ResNet、DeepNet等一系列的发展也使得在分类、物体检测、人脸识别、图像分割等众多领域有了长足的进步。同时，CNN不仅在图像上应用很多，在自然语言处理上也颇有进展，现在已经有基于CNN的语言模型能够达到比LSTM更好的效果。在最新的AlphaZero中，CNN中的ResNet也是两种基本算法之一。

    GAN是一种应用在生成模型的训练方法，现在有很多在CV方面的应用，例如图像翻译，图像超清化、图像修复等等。

    RNN主要用于自然语言处理(Natural Language Processing)领域，用于处理序列到序列的问题。普通RNN会遇到梯度爆炸和梯度消失的问题。所以现在在NLP领域，一般会使用LSTM模型。在最近的机器翻译领域，Attention作为一种新的手段，也被引入进来。

    除了DNN、RNN和CNN外， 自动编码器(AutoEncoder)、稀疏编码(Sparse Coding)、深度信念网络(DBM)、限制玻尔兹曼机(RBM)也都有相应的研究。

25. **请简述过拟合还是激活函数？的发展史。**

    sigmoid会饱和，造成梯度消失。于是有了ReLU。

    ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。

    强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。

    太深了，梯度传不下去，于是有了highway。

    干脆连highway的参数都不要，直接变残差，于是有了ResNet。

    强行稳定参数的均值和方差，于是有了BatchNorm。

    在梯度流中增加噪声，于是有了 Dropout。

    RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。

    LSTM简化一下，有了GRU。

    GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。

    WGAN对梯度的clip有问题，于是有了WGAN-GP。

26. **神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？**

    （1）非线性：即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。

    （2）几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。

    （3）计算简单：非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。

    （4）非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x>0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x<0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。

    （5）单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。

    （6）输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。

    （7）接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x>0时为线性。这个性质也让初始化参数范围的推导更为简单。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。

    （8）参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。

    （9）归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。

27. **梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？**

    深度神经网络“容易收敛到局部最优”，很可能是一种想象，实际情况是，我们可能从来没有找到过“局部最优”，更别说全局最优了。

    很多人都有一种看法，就是“局部最优是神经网络优化的主要难点”。这来源于一维优化问题的直观想象。在单变量的情形下，优化问题最直观的困难就是有很多局部极值，如

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS86e4frRmTJick1ZheWNiaWicQ6qa6fGaEWqSqjribqJl9F0oXqia9UHYRgy51TkQMicKttR8KNXuy1Tk5A/640?wx_fmt=jpeg)

    人们直观的想象，高维的时候这样的局部极值会更多，指数级的增加，于是优化到全局最优就更难了。然而单变量到多变量一个重要差异是，单变量的时候，Hessian矩阵只有一个特征值，于是无论这个特征值的符号正负，一个临界点都是局部极值。但是在多变量的时候，Hessian有多个不同的特征值，这时候各个特征值就可能会有更复杂的分布，如有正有负的不定型和有多个退化特征值（零特征值）的半定型

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS86e4frRmTJick1ZheWNiaWicQicB87LArQNZSu6WwatVOJhH3hOibmARW1ibETicgnibib5AdBflISfD9ibPoA/640?wx_fmt=jpeg)

    在后两种情况下，是很难找到局部极值的，更别说全局最优了。

    　　现在看来，神经网络的训练的困难主要是鞍点的问题。在实际中，我们很可能也从来没有真的遇到过局部极值。Bengio组这篇文章Eigenvalues of the Hessian in Deep Learning（https://arxiv.org/abs/1611.07476）里面的实验研究给出以下的结论：

    Training stops at a point that has a small gradient. The norm of the gradient is not zero, therefore it does not, technically speaking, converge to a critical point.

    There are still negative eigenvalues even when they are small in magnitude.

    另一方面，一个好消息是，即使有局部极值，具有较差的loss的局部极值的吸引域也是很小的Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes。（https://arxiv.org/abs/1706.10239）

    For the landscape of loss function for deep networks, the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima.

    所以，很可能我们实际上是在“什么也没找到”的情况下就停止了训练，然后拿到测试集上试试，“咦，效果还不错”。

    补充说明，这些都是实验研究结果。理论方面，各种假设下，深度神经网络的Landscape 的鞍点数目指数增加，而具有较差loss的局部极值非常少。

28. **简单说说CNN常用的几个模型。**

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_png/pu7ghYhibpS86e4frRmTJick1ZheWNiaWicQxecAsXwXNoibNjDVibSmphoON3ODaw1kxIeUg9oKU00AxNj6R5H5qBzg/640?wx_fmt=png)

29. **为什么很多做人脸的Paper会最后加入一个Local Connected Conv？**

    以FaceBook DeepFace 为例：

    ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_jpg/pu7ghYhibpS86e4frRmTJick1ZheWNiaWicQJUVN2zicBibnsegBM0sc63Ntwu2RHmP7XwXIicbaD1nUNHyr9VhiceW8jw/640?wx_fmt=jpeg)

    DeepFace 先进行了两次全卷积＋一次池化，提取了低层次的边缘／纹理等特征。后接了3个Local-Conv层，这里是用Local-Conv的原因是，人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。

30. **什么是梯度爆炸？**

    误差梯度是神经网络训练过程中计算的方向和数量，用于以正确的方向和合适的量更新网络权重。

    在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。

    网络层之间的梯度（值大于 1.0）重复相乘导致的指数级增长会产生梯度爆炸。