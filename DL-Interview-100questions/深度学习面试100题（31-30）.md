31. **梯度爆炸会引发什么问题？**

    在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。

    梯度爆炸导致学习过程不稳定。—《深度学习》，2016。

    在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。

32. **如何确定是否出现梯度爆炸？**

    训练过程中出现梯度爆炸会伴随一些细微的信号，如：

    ​	模型无法从训练数据中获得更新（如低损失）

    ​	模型不稳定，导致更新过程中的损失出现显著变化。

    ​	训练过程中，模型损失变成 NaN。

    ​	如果你发现这些问题，那么你需要仔细查看是否出现梯度爆炸问题。

    以下是一些稍微明显一点的信号，有助于确认是否出现梯度爆炸问题。

    ​	训练过程中模型梯度快速变大。

    ​	训练过程中模型权重变成 NaN 值。

    ​	训练过程中，每个节点和层的误差梯度值持续超过 1.0。

33. **如何修复梯度爆炸问题？**

    有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。

    （1） 重新设计网络模型

    在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。

    使用更小的批尺寸对网络训练也有好处。

    在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。

    （2）使用 ReLU 激活函数

    在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。

    使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。

    （3）使用长短期记忆网络

    在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。

    使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。

    采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。

    （4）使用梯度截断（Gradient Clipping）

    在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。
