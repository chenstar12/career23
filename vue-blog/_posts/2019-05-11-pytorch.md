---
layout: post
title:  "Pytorch学习笔记"
date:   2019-05-11 12:00:00
categories: 编程语言
tags: Pytorch pytorch Tensorflow Python 深度学习 框架
excerpt: Pytorch 编程技能汇总
author: 鹤啸九天
mathjax: true
permalink: /pytorch
---

* content
{:toc}

# 总结

【2020-8-31】[30天吃掉那只TensorFlow2](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)

# 深度学习框架

【2022-4-8】[中国开源深度学习框架第六年：百度飞桨国内综合份额第一，全球开发者超400万](https://mp.weixin.qq.com/s/OypvL9PcHEZMIJNcWtK1Tw)，[和讯网地址](http://tech.hexun.com/2022-04-08/205678997.html)
- 国际两大主流深度学习框架TensorFlow、PyTorch之外，中国的开源框架，发展怎么样？
- 百度飞桨（PaddlePaddle），深度学习开源框架的先头兵，在2016年就已率先对外发布。
- 2020年，国内开源框架迎来了第一波集中爆发。
  - 独角兽旷视拿出工业级深度学习框架**天元**（MegEngine），一流科技**OneFlow**、华为**昇思**（MindSpore）也在同年登场。
  - 学界方面，清华大学开源了支持即时编译的深度学习框架**计图**（Jittor）。
- 过去几年中，“开源”、“AI底层”成为了国内AI厂商们十分重视的发展战略。
![](http://i0.hexun.com/2022-04-08/205678988.jpg)


## 发展历史

- 【2021-3-3】[深度学习框架编年史：搞深度学习的那帮人，不是疯子，就是骗子！](https://mp.weixin.qq.com/s/PRg2P_7TtebhLAXwplEUdA)
- 2002年，瑞士是物理和数学领域的领跑者。也在2002年，瑞士戴尔莫尔感知人工智能（Idiap）研究所诞生了第一个机器学习库Torch。
  - 机器学习库Torch，出自“葡萄酒产区”研究所的一份研究报告（三位作者分别是：Ronan Collobert、Samy Bengio、Johnny Mariéthoz）。其中一位作者姓本吉奥（Bengio），没错，这位眉毛粗粗的科学家，就是深度学习三巨头之一，约舒亚·本吉奥（Yoshua Bengio）的兄弟。2007年他跳槽去了谷歌。
- 2007年，加拿大蒙特利尔大学开发了第一个深度学习框架Theano（行业祖师爷）。框架和图灵奖获得者颇有渊源，约舒亚·本吉奥（Yoshua Bengio）和伊恩·古德费洛（Ian Goodfellow）都有参与Theano。 
  - 库和框架的不同之处，在于境界。库是兵器库，框架则是一套武林绝学的世界观，程序员在这个世界观的约束下去练（编）拳（程）法（序），结果被框架所调用。框架接管了程序的主控制流。有了框架，才能做到只关注算法的原理和逻辑，不用去费事搞定底层系统、工程的事。
- 2013年，著名的Caffe框架诞生，发音和“咖啡”相似，是“快速特征提取的卷积框架”论文的英文简称。贾扬清第一个C++项目
  - 贾扬清已经在美国加州大学伯克利分校攻读博士学位，开启了计算机视觉的相关研究。那时候，他常被一个问题困扰：怎样训练和设计深度学习的网络？为此，贾扬清想造一个通用工具。
- 2013年，Parameter Server（参数服务器）的两位著名教授走向台前，邢波（Eric Xing）教授和Alex Smola教授，现在两位均在美国卡内基梅隆大学（CMU）任教。
  - 参数服务器是个编程框架，也支持其他AI算法，对深度学习框架有重要影响。
  - 高校实验室善于技术创新，深度学习框架的很多精髓创意源于此地。但是，深度学习框架复杂性高、工程量极大，长期负责复杂产品，高校并不擅长。多年后，高校出生的深度学习框架，都以某种方式“进入”企业，或者被企业赶超
- 2015年11月，谷歌大脑团队开发的TensorFlow开源，原创者之一是谷歌天才科学家，杰夫·迪恩（Jeff Dean）。谷歌的搜索、油管、广告、地图、街景和翻译的背后，都有其身影。 
  - TensorFlow直译，张量（tensor）在图中流动（flow）。由此也可获知，数据流图是框架的重要技术。数据流图由算子组成，算子又分为大算子和小算子。Caffe是大算子抽象，TensorFlow是小算子抽象。小算子好处是灵活，坏处是性能优化难。
  - 在2000年下半年的时候，Jeff Dean的代码速度突然激增了40倍，原因是他把自己的键盘升级到了USB 2.0。编译器从来不会给Jeff Dean警告，但Jeff Dean会警告编译器。—— 段子
- 2015 年是一个重要的年份，何恺明等人的研究成果，突破了边界，在准确率上再创新高，风头一时无二。
- 2015年，谷歌AI研究员弗朗索瓦·乔莱特（Francois Chollet）几乎是独自完成了著名的Keras 框架的开发，为谷歌再添一条护城河，大有“千秋万代，一统江湖”的势头。
- 2016年，微软CNTK（Cognitive Toolkit）伸手接过女神（内部孵化项目Minerva）的接力棒，可惜魔障难消，用的人少，没有推广开，于2019年停止维护。
- 2016年，贾扬清从谷歌TensorFlow团队离职，跳槽到了Facebook公司。与谷歌挥手道别，四载光阴（实习两年，工作两年），往事依稀，他的内心充满感怀。
- 2016年5月，陈天奇读博二开发的MXNet（读作“mixnet”，mix是中文“混合”之意）开源，浓缩了当时的精华，合并了几个原来有的项目，陈天奇cxxnet、参数服务器、智慧女神、颜水成学生林敏的purine2。仅一年时间里，就做出了完整的架构。团队中还有一位闻名遐迩的大神，李沐（现任亚马逊公司资深主任科学家，principal scientist）。2017年9月，MXNet被亚马逊选为官方开源平台。
- 2017年，祖师爷Theano官宣退休。贾扬清借鉴谷歌TensorFlow框架里面的一些新思想，实现了一个全新的开源Caffe2。三十而立的他，成长为遍历世界级产品的第一高手。
- 2018年，PyTorch接纳Caffe2后，意外崛起，上演令谷歌框架王冠落地的戏剧性一幕。易用性确实可以抢客户，但谷歌没有想到脸书抢了这么多。后来者确实可以居上，但谷歌没有想到脸书仅用如此短的时间。
  - 谷歌出发最早，为何没有独坐钓鱼台？为什么是脸书抢了市场？
  - 谷歌野心非常大，初期想做很大很全的工具。虽然完备性很强，但是，系统过度复杂。虽然以底层操作为主，有很多基础的功能，但是这些功能没能封装得很好，需要开发者自己解决（定义），手动工作过多。
  - 仅仅是丢市场还不够惨，PyTorch框架带火了背后的技术（动态执行等），脸书开始左右技术趋势。
- 微软在智慧女神和CNTK两次滑铁卢之后，依然斗志昂扬准备第三次入局。微软思路清奇地设计了ONNX（全称Open Neural Network Exchange），一种开放式深度学习神经网络模型的格式，用于统一模型格式标准。
- 2016 年8月，百度PaddlePaddle开源，PaddlePaddle作为国内唯一的开源深度学习框架，此后两年多，都是孤家寡人。2013年，百度第一位T11徐伟，同时也是百度深度学习框架PaddlePaddle的原创者和奠基人。
- 2019年，百度PaddlePaddle有了中文名，名叫“飞桨”。国外产品连个中文名都懒得起。
- 2019年2月，一流科技获得千万级Pre-A轮投资，袁进辉是创始人兼CEO。邢波教授团队和袁进辉团队双剑合璧开发了OneFlow
  - 2014年，微软亚研院副院长马维英（现任清华大学智能产业研究院讲席教授、首席科学家）找到一位研究员，名叫袁进辉，他是清华大学计算机专业的博士，师从张钹院士。
  - 来自美国CMU的教授，名叫邢波，此时任微软亚研院顾问一职，他擅长的领域包括大规模计算系统。
- 2020年，国产深度学习框架井喷。国产深度学习框架的“元年”。
  - 3月20日，清华大学计图（Jittor）。
  - 3月25日，旷视科技天元（MegEngine） 。
  - 3月28日，华为MindSpore。
  - 7月31日，一流科技OneFlow。OneFlow有两个创新点：一会自动安排数据通信。二把数据通信和计算的关系协调好，让整体效率更高。
- 守旧的经验是，既然国外开源了，就抓紧学。既然人家成了事实工业标准，就尽力参与。总是慢了好几拍，Linux这轮就是这样。
- 引用某游戏厂商的经典台词是：“别催了，在抄了，在抄了。”
- 可惜竞争从来不是游戏。
- 深度学习框架的台词是：“不能照抄，不能舔狗，舔到最后，一无所有。”
- 这世界上唯一能够碾压国内一线城市房价增速的，只有AI模型的规模，虽然硬件和软件的进步已经将每年的训练成本降低了37%；但是，AI模型越来越大，以每年10倍的速度增长。
- 前美国国防部咨询顾问，史蒂夫·马奎斯的说法是：“开源项目，来源于最纯粹的竞争。如果一个开源项目在商业世界获得了成功，那决不会是出于侥幸，决不会是因为其它竞争者恰好被规章制度所累、被知识产权法约束、被人傻钱多的金主拖垮。一个开源项目胜出了，背后只会有一个原因——它真的比其他竞争者都要好。”

## Tensorflow v.s. Pytorch

结论：
- 如果是**工程师**，应该优先选TensorFlow2.
- 如果是**学生**/**研究**人员，应该优先选择Pytorch.
- 如果时间足够，最好TensorFlow2和Pytorch都要学习掌握。
- 理由如下：
  - 1，在工业界最重要的是模型落地，目前国内的大部分互联网企业只支持TensorFlow模型的在线部署，不支持Pytorch。 并且工业界更加注重的是模型的高可用性，许多时候使用的都是成熟的模型架构，调试需求并不大。
  - 2，研究人员最重要的是快速迭代发表文章，需要尝试一些较新的模型架构。而Pytorch在易用性上相比TensorFlow2有一些优势，更加方便调试。 并且在2019年以来在学术界占领了大半壁江山，能够找到的相应最新研究成果更多。
  - 3，TensorFlow2和Pytorch实际上整体风格已经非常相似了，学会了其中一个，学习另外一个将比较容易。两种框架都掌握的话，能够参考的开源模型案例更多，并且可以方便地在两种框架之间切换。
- Keras库在2.3.0版本后将不再更新，用户应该使用tf.keras

总结：
- （1）**模型**可用性：pytorch更好
- （2）**部署**便捷性：TensorFlow更好
  - Serving 和 TFLite 比 PyTorch 的同类型工具要稳健一些。而且，将 TFLite 与谷歌的 Coral 设备一起用于本地 AI 的能力是许多行业的必备条件。相比之下，PyTorch Live 只专注于移动平台，而 TorchServe 仍处于起步阶段。
  - 既想用 TensorFlow 的部署基础设施，又想访问只能在 PyTorch 中使用的模型，作者推荐使用 ONNX 将模型从 PyTorch 移植到 TensorFlow。
- （3）**生态系统**对比：TensorFlow 胜出

[2022年了，PyTorch和TensorFlow你选哪个？](https://www.toutiao.com/i7043969108496515597)

选择指南：
- （1）工程师
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRNg3967KOj?from=pc)
  - TensorFlow 强大的部署框架和端到端的 TensorFlow Extended 平台是很珍贵的。能在 gRPC 服务器上进行轻松部署以及模型监控和工件跟踪是行业应用的关键
  - 仅在 PyTorch 中可用的 SOTA 模型，那也可以考虑使用 PyTorch（TorchServe）
  - 移动应用，也可以用pytorch live
  - 音频或视频输入，应该使用 TensorFlow
  - 用 AI 的嵌入式系统或 IoT 设备，鉴于 TFLite + Coral 生态系统，用tf
- （2）研究者
  - 大概率会使用 PyTorch，坚持使用，大多数 SOTA 模型都适用于 PyTorch；
  - 但强化学习领域的一些研究应该考虑使用 TensorFlow。原生 Agents 库，并且 DeepMind 的 Acme框架和Sonnet
  - 想用TPU训练，但不想用TensorFlow，那考虑探索谷歌的JAX。JAX 本身不是神经网络框架，而是更接近于具有自动微分能力的 GPU/TPU 的 NumPy 实现。
  - 不用TPU 训练，那最好是坚持使用 PyTorch
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRO25bBNaV1?from=pc)
- （3）教授，因课程重点而已
  - 培养具备行业技能的深度学习工程师，胜任整个端到端深度学习任务，而不仅仅是掌握深度学习理论，那应该使用 TensorFlow
  - 深度学习理论和理解深度学习模型的底层原理/高级课程/研究，那应该使用 PyTorch。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmROWHLI567V?from=pc)
- （4）职业转型
  - 对框架完全不熟悉，请使用 TensorFlow，因为它是首选的行业框架。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRQcBgRnxW0?from=pc)
- （5）业余爱好者
  - 大项目，部署到物联网/嵌入式设备，TensorFlow + TFLite
  - 了解深度学习，PyTorch
  - 入门即可，keras

HuggingFace使得深度学习从业者仅借助几行代码就能将训练、微调好的 SOTA 模型整合到其 pipeline 中。
- HuggingFace中大约有85%的模型只能在PyTorch上用，剩下的模型还有一半也可以在 PyTorch 上用。
- 相比之下，只有16%的模型能在 TensorFlow 上用，只有 8% 是 TensorFlow 所独有的。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmQr91qOhSid?from=pc)
- Top 30 个模型中，能在 TensorFlow 上用的还不到 2/3，但能在 PyTorch 上用的却达到了 100%，没有哪个模型只能在 TensorFlow 上用。
- 8个顶级研究期刊论文中框架采用情况：PyTorch 的采用率增长迅速，几年时间就从原来的 7% 长到了近 **80%**。 很多转向 PyTorch 的研究者都表示 TensorFlow 1 太难用了。尽管 2019 年发布的 TensorFlow 2 改掉了一些问题，但彼时，PyTorch 的增长势头已经难以遏制。
- ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmQtFATPcRfG?from=pc)
- 2018 年还在用 TensorFlow 的论文作者中，有 55% 的人在 2019 年转向了 PyTorch，但 2018 年就在用 PyTorch 的人有 85% 都留了下来。
- Papers with Code 本季度创建的 4500 个库中，有 60% 是在 PyTorch 中实现的，只有 11% 是在 TensorFlow 中实现的。相比之下，TensorFlow 的使用率在稳步下降，2019 年 TensorFlow 2 的发布也没有扭转这一趋势。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmQtlBjJz4JO?from=pc)
大公司：
- Google AI：谷歌发布的论文自然会用 TensorFlow。鉴于在论文方面谷歌比 Facebook 更高产，一些研究者可能会发现掌握 TensorFlow 还是很有用的。
- DeepMind：DeepMind 也用 TensorFlow，而且也比 Facebook 高产。他们创建了一个名叫 Sonnet 的 TensorFlow 高级 API，用于研究目的。有人管这个 API 叫「科研版 Keras」，那些考虑用 TensorFlow 做研究的人可能会用到它。此外，DeepMind 的 Acme 框架可能对于强化学习研究者很有用。
- OpenAI：OpenAI 在 2020 年宣布了全面拥抱 PyTorch 的决定。但他们之前的强化学习基线库都是在 TensorFlow 上部署的。基线提供了高质量强化学习算法的实现，因此 TensorFlow 可能还是强化学习从业者的最佳选择。
- JAX：谷歌还有另一个框架——JAX，它在研究社区中越来越受欢迎。与 PyTorch 和 TensorFlow 相比，JAX 的开销要小得多。但同时，JAX 和前两个框架差别也很大，因此迁移到 JAX 对于大多数人来说可能并不是一个好选择。目前，有越来越多的模型 / 论文已经在用 JAX，但未来几年的趋势依然不甚明朗。

模型部署

TensorFlow Serving：
- TensorFlow Serving 用于在服务器上部署 TensorFlow 模型，无论是在内部还是在云上，并在 TensorFlow Extended（TFX）端到端机器学习平台中使用。Serving 使得用模型标记（model tag）将模型序列化到定义良好的目录中变得很容易，并且可以选择在保持服务器架构和 API 静态的情况下使用哪个模型来进行推理请求。
- Serving 可以帮用户轻松地在 gRPC 服务器上部署模型，这些服务器运行谷歌为高性能 RPC 打造的开源框架。gRPC 的设计意图是连接不同的微服务生态系统，因此这些服务器非常适合模型部署。Serving 通过 Vertex AI 和 Google Cloud 紧密地集成在一起，还和 Kubernetes 以及 Docker 进行了集成。

TensorFlow Lite：
- TensorFlow Lite 用于在移动或物联网 / 嵌入式设备上部署 TensorFlow 模型。TFLite 对这些设备上的模型进行了压缩和优化，并解决了设备上的 AI 的 5 个约束——延迟、连接、隐私、大小和功耗。可以使用相同的 pipeline 同时导出基于标准 Keras 的 SavedModels（和 Serving 一起使用）和 TFLite 模型，这样就能比较模型的质量。
- TFLite 可用于 Android、iOS、微控制器和嵌入式 Linux。TensorFlow 针对 Python、Java、C++、JavaScript 和 Swift 的 API 为开发人员提供了广泛的语言选项。

PyTorch 用户需要使用 Flask 或 Django 在模型之上构建一个 REST API，但现在他们有了 TorchServe 和 PyTorch Live 的本地部署选项。

TorchServe：
- TorchServe 是 AWS 和 Facebook 合作的开源部署框架，于 2020 年发布。它具有端点规范、模型归档和指标观测等基本功能，但仍然不如 TensorFlow。TorchServe 同时支持 REST 和 gRPC API。

PyTorch Live：
- PyTorch 于 2019 年首次发布 PyTorch Mobile，旨在为部署优化的机器学习模型创建端到端工作流，适用于 Android、iOS 和 Linux。
- PyTorch Live 于 12 月初发布，以移动平台为基础。它使用 JavaScript 和 React Native 来创建带有相关 UI 的跨平台 iOS 和 Android AI 应用。设备上的推理仍然由 PyTorch Mobile 执行。Live 提供了示例项目来辅助入门，并计划在未来支持音频和视频输入。

## pytorch设计思路

Pytorch 的哲学是 **解决当务之急**，也就是说即时构建和运行计算图。

## 生态系统

### pytorch生态

- Hub：
  - PyTorch Hub 作为面向研究的官方平台，用于与预训练模型共享存储库。Hub 拥有广泛类别的模型，包括用于音频、视觉、NLP 任务的模型，还有用于生成任务的 GAN 模型。
- SpeechBrain：
  - SpeechBrain 是 PyTorch 的官方开源语音工具包。SpeechBrain 能够完成自动语音识别（ASR）、说话人识别、验证和分类等任务。如果你不想构建任何模型，而是想要一个具有情感分析、实体检测等功能的即插即用工具，你可以选择使用 AssemblyAI 的 Speech-to-Text API。
  - 当然，PyTorch 的工具页面还有很多其他有用的库，包括为计算机视觉和自然语言处理量身定制的库，例如 fast.ai。
- TorchElastic：
  - TorchElastic 是 AWS 和 Facebook 2020 年联合发布的分布式训练工具，可管理工作进程并协调重启行为，以便用户在计算节点集群上训练模型，这些节点可以动态变化而不会影响训练。因此，TorchElastic 可防止因服务器维护或网络问题等导致的灾难性故障，不会丢失训练进度。TorchElastic 具有与 Kubernetes 集成的特性，并已集成到 PyTorch 1.9+ 中。
- TorchX：
  - TorchX 是一个用于快速构建和部署机器学习应用程序的 SDK。TorchX 包括 Training Session Manager API，可在支持的调度程序上启动分布式 PyTorch 应用程序。TorchX 负责启动分布式作业，同时原生支持由 TorchElastic 局部管理的作业。
- Lightning：
  - PyTorch Lightning 有时被称为 PyTorch 的 Keras。虽然这种类比并不准确，但 Lightning 的确是简化 PyTorch 中模型工程和训练过程的有用工具，自 2019 年首次发布以来已经逐渐趋于成熟。Lightning 以面向对象的方式处理建模过程，定义了可重用和可跨项目使用的可共享组件。

### TensorFlow生态

- Hub：
  - TensorFlow Hub 是一个经过训练的机器学习模型库，可以进行微调，让用户只需几行代码就能使用像 BERT 这样的模型。Hub 包含适用于不同用例的 TensorFlow、TensorFlow Lite 和 TensorFlow.js 模型，可用于图像、视频、音频和文本处理。
- Model Garden：
  - 如果现成的预训练模型不适用于用户的应用，那么 TensorFlow 的存储库 Model Garden 可以提供 SOTA 模型的源代码。对于想要深入了解模型工作原理，或根据自己的需要修改模型的用户，Model Garden 将非常有用。
  - Model Garden 包含谷歌维护的官方模型、研究人员维护的研究模型和社区维护的精选社区模型。TensorFlow 的长期目标是在 Hub 上提供来自 Model Garden 的模型的预训练版本，并使 Hub 上的预训练模型在 Model Garden 中具有可用的源代码。
- Extended（TFX）：
  - TensorFlow Extended 是 TensorFlow 用于模型部署的端到端平台。该平台的功能强大，包括：加载、验证、分析和转换数据；训练和评估模型；使用 Serving 或 Lite 部署模型；跟踪 artifact 及其依赖项。TFX 还可以与 Jupyter 或 Colab 一起使用，并且可以使用 Apache Airflow/Beam 或 Kubernetes 进行编排。TFX 与 Google Cloud 紧密集成，可与 Vertex AI Pipelines 一起使用。
- Vertex AI：
  - Vertex AI 是 Google Cloud 今年刚刚发布的统一机器学习平台，旨在统一 GCP、AI Platform 和 AutoML，成为一个平台。Vertex AI 能够以无服务器方式编排工作流，帮助用户自动化、监控和管理机器学习系统。Vertex AI 还可以存储工作流的 artifact，让用户可以跟踪依赖项和模型的训练数据、超参数和源代码。
- Coral：
  - 尽管有各种各样的 SaaS 公司依赖基于云的人工智能，但许多行业对本地人工智能的需求也在不断增长，Google Coral 就是为了满足这一需求而创建的。Coral 是一个完整的工具包，可以使用本地 AI 构建产品。Coral 于 2020 年发布，解决了部署部分 TFLite 中提到的实现板载 AI 的问题，克服了隐私和效率等方面的困难。
  - Coral 提供了一系列用于原型设计、生产和传感的硬件产品，其中一些本质上是增强型的树莓派，专为 AI 应用程序创建，能够利用 Edge TPU 在低功耗设备上进行高性能推理。Coral 还提供用于图像分割、姿态估计、语音识别等任务的预编译模型，为希望创建本地 AI 系统的开发人员提供支持。创建模型的基本步骤如下面的流程图所示。
  - ![](https://p26.toutiaoimg.com/origin/tos-cn-i-tjoges91tu/SsBmRMl2J55OqA?from=pc)
- TensorFlow.js：
  - TensorFlow.js 是一个用于机器学习的 JavaScript 库，允许用户使用 Node.js 在浏览器和服务器端训练和部署模型。
- Cloud：
  - TensorFlow Cloud 是一个可以将本地环境连接到 Google Cloud 的库，它的 API 旨在弥补本地机器上模型构建和调试与 GCP 上分布式训练和超参数调整之间的差距，而无需使用 Cloud Console。
- Colab：
  - Google Colab 是一个基于云的 notebook 环境，与 Jupyter 非常相似。Colab 易于连接到 Google Cloud 进行 GPU 或 TPU 训练，并且 Colab 还可以和 PyTorch 一起使用。
- Playground：
  - Playground 是一个小而精致的可视化工具，用于帮助用户理解神经网络的基础知识。要户可以更改 Playground 内置神经网络的层数和大小，以实时查看神经网络是如何学习特征的，用户还可以看到改变学习率和正则化强度等超参数如何影响不同数据集的学习过程。Playground 允许实时播放学习过程，以高度直观的方式查看输入在训练过程中是如何转换的。Playground 还提供了一个开源的小型神经网络库，是它自身的构建基础，用户能够查看其源代码的具体细节。
- Datasets：
  - 谷歌研究院的 Datasets 是谷歌定期发布的数据集的整合资源。谷歌还提供了数据集搜索以访问更广泛的数据集资源。当然，PyTorch 用户也可以利用这些数据集。

## TorchServe

- 【2020-4-27】PyTorch 1.5发布，与AWS联手推出TorchServe
- PyTorch 1.5 发布，升级了主要的 torch**vision** (视觉)，torch**text**（文本） 和 torch**audio**（语音） 库，并推出将模型从 Python API 转换为 C++ API 等功能。
- 除此之外，Facebook 还和 Amazon 合作，推出了两个重磅的工具：TorchServe 模型服务框架 和 TorchElastic Kubernetes 控制器。
  - TorchServe 旨在为大规模部署 PyTorch 模型推理，提供一个干净、兼容性好的工业级路径。
  - TorchElastic Kubernetes 控制器，可让开发人员快速使用 Kubernetes 集群，在 PyTorch 中创建容错分布式训练作业。
- 【2021-1-19】pytorch模型部署工具[TorchServe](https://github.com/pytorch/serve/blob/master/README.md#serve-a-model)
- [如何评价 PyTorch 在 2020 年 4 月推出的 TorchServe？](https://www.zhihu.com/question/389731764)
- TorchServe 旨在为大规模部署 PyTorch 模型推理，提供一个干净、兼容性好的工业级路径。其主要的特点包括有：
  - 原生态 API：支持用于预测的推理 API，和用于管理模型服务器的管理 API。
  - 安全部署：包括对安全部署的  HTTPS 支持。
  - 强大的模型管理功能：允许通过命令行接口、配置文件或运行时 API 对模型、版本和单个工作线程进行完整配置。
  - 模型归档：提供执行「模型归档」的工具，这是一个将模型、参数和支持文件打包到单个持久工件的过程。使用一个简单的命令行界面，可以打包和导出为单个「 .mar」文件，其中包含提供 PyTorch 模型所需的一切。该 .mar 文件可以共享和重用。
  - 内置的模型处理程序：支持涵盖最常见用例，如图像分类、对象检测、文本分类、图像分割的模型处理程序。TorchServe 还支持自定义处理程序。
  - 日志记录和指标：支持可靠的日志记录和实时指标，以监视推理服务和端点、性能、资源利用率和错误。还可以生成自定义日志并定义自定义指标。
  - 模型管理：支持同时管理多个模型或同一模型的多个版本。你可以使用模型版本回到早期版本，或者将流量路由到不同的版本进行 A/B 测试。
  - 预构建的图像：准备就绪后，可以在基于 CPU 和 NVIDIA GPU 的环境中，部署 TorchServe 的 Dockerfile 和 Docker 镜像。
- 综上可知，这次的 TorchServe  在推理任务上，将会有很大的使用空间，对于广大开发者来说是一件好事。这一点可以留着以后去慢慢验证。对于这次 Facebook 和 AWS 合作，明显可以看出双方在各取所长，试图打造一个可以反抗谷歌 TensorFlow 垄断的方案。
- 问题：
  - 为什么TorchServe采用Java开发，而没有像TensorFlow Serving一样采用更好性能的C++，抑或是采用Golang?
- TorchServe Architecture
  - ![](https://user-images.githubusercontent.com/880376/83180095-c44cc600-a0d7-11ea-97c1-23abb4cdbe4d.jpg)

- Model Server for PyTorch Documentation

**Basic Features**

* [Serving Quick Start](../README.md#serve-a-model) - Basic server usage tutorial
* [Model Archive Quick Start](../model-archiver#creating-a-model-archive) - Tutorial that shows you how to package a model archive file.
* [Installation](../README.md##install-torchserve) - Installation procedures
* [Serving Models](server.md) - Explains how to use `torchserve`.
  * [REST API](rest_api.md) - Specification on the API endpoint for TorchServe
  * [gRPC API](grpc_api.md) - Specification on the gRPC API endpoint for TorchServe
* [Packaging Model Archive](../model-archiver/README.md) - Explains how to package model archive file, use `model-archiver`.
* [Logging](logging.md) - How to configure logging
* [Metrics](metrics.md) - How to configure metrics
* [Batch inference with TorchServe](batch_inference_with_ts.md) - How to create and serve a model with batch inference in TorchServe
* [Model Snapshots](snapshot.md) - Describes how to use snapshot feature for resiliency due to a planned or unplanned service stop

 **Advanced Features**

* [Advanced settings](configuration.md) - Describes advanced TorchServe configurations.
* [Custom Model Service](custom_service.md) - Describes how to develop custom inference services.
* [Unit Tests](../ts/tests/README.md) - Housekeeping unit tests for TorchServe.
* [Benchmark](../benchmarks/README.md) - Use JMeter to run TorchServe through the paces and collect benchmark data

**Default Handlers**

* [Image Classifier](../ts/torch_handler/image_classifier.py) - This handler takes an image and returns the name of object in that image
* [Text Classifier](../ts/torch_handler/text_classifier.py) - This handler takes a text (string) as input and returns the classification text based on the model vocabulary
* [Object Detector](../ts/torch_handler/object_detector.py) - This handler takes an image and returns list of detected classes and bounding boxes respectively
* [Image Segmenter](../ts/torch_handler/image_segmenter.py) - This handler takes an image and returns output shape as [CL H W], CL - number of classes, H - height and W - width


# Pytorch教程

Pytorch的哲学是解决当务之急，也就是说<span style='color:blue'>即时构建</span>和<span style='color:blue'>运行计算图</span>。

[PyTorch经验分享：新手如何搭建PyTorch程序](https://blog.csdn.net/jiongnima/article/details/103324839)

## 安装

### Pytorch

安装：

```shell
pip install torch torchvision
```
验证：

```python
python -c "import torch;x = torch.rand(5, 3);print(x)"
```

### tensorflow

```shell
pip install --upgrade tensorflow
```
验证：

```shell
python -c "import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))"
```

## 基本组件

深度学习任务中，简单的PyTorch程序主要包含以下这4个模块。
- **网络模型定义**文件，如 network.py。
- **数据读取**文件，如 dataset.py。
- **训练**接口程序，如 train.py。
- **测试**接口程序，如 evaluate.py。
- ![](https://img-blog.csdnimg.cn/20191130161751508.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppb25nbmltYQ==,size_16,color_FFFFFF,t_70)
- 包含之前提到的 network，dataset，train 和 evaluate。当然也包含定制的模块，比如
  - cal_metric s用来计算实验指标
  - cfg 用来设置某些超参数
  - image_io_and_process 用来对图像进行预处理和后处理
  - loss_functions 用来定义与实现某些损失
  - utils 用来承载一些其他操作，比如读写txt文档。

## 组件详解

详解：
1. 网络**模型定义**文件，比如名称为network.py。
  - 定义一个Python类，并继承 torch.nn.Module 这个类。
  - 重写构造函数 \__init__ 来定义网络中使用的层，在定义层或者模块的时候，经常会使用到 torch.nn 这个库。
  - 再重写父类的 forward 函数，在其中使用 \__init__ 函数中定义的层实现网络的前传过程
2. **数据读取**文件，比如名称为dataset.py。
  - PyTorch与其他框架具有鲜明差异的一点，就是数据读取**接口非常规范**。
  - PyTorch通过继承 torch.utils.data.Dataset 这个类实现自己的数据集读取。
  - 继承时，主要需要重写三个函数：__init__函数，__getitem__函数和__len__函数。
    - __**init**__函数：初始化，比如读取一下所需的记录数据的txt或者xml文件，传入一些预处理参数等等。
    - __**getitem**__函数：规定每个batch训练给网络喂数据的时候，应该采用怎样的数据读取方式，以及做怎样的预处理。
    - __**len**__函数：数据集中有多少数据
  - 训练程序中，只需要使用 torch.utils.data.**DataLoader** 开始在每一次训练中对数据集进行读取与遍历就行
    - train_data_loader = torch.utils.data.**DataLoader**(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False,timeout=0, worker_init_fn=None)
  - 除去dataset，用的比较多的是batch_size，shuffle，num_workers和pin_memory这几个参数。
    - batch_size 指定了一个批次的数据容量
    - shuffle 指定了是否打乱数据
    - num_workers 指定了用多少个线程对数据进行读取
    - 而pin_memory 则指定了是否将数据放入显存。
3. **训练**接口程序，比如名称为train.py。与其他深度学习框架类似，都是先搭建图(模型)，然后在图上进行训练
  - 四个步骤，引用数据、引用模型、定义损失函数、定义优化器：
    - 引用之前定义的**数据读取接口**。
    - 引用model，即网络，需要的话就载入预训练参数。
    - 定义loss。
    - 定义优化器。
  - 然后就可以开始在for循环中进行可训练参数更新了。
  - 最后，在PyTorch训练程序中，还需要注意
    1. 第一点是**保存模型**和**加载预训练参数**。
      - 保存模型的已训练参数(也可以保存整个模型)，通过 torch.save 函数完成；
      - 加载预训练参数，通过model的父类，即 torch.nn.Module 的 load_state_dict 完成。里面的“strict”参数表示是否需要将预训练模型里面的参数与model里面的完全对齐。
    2. 第二点是使用**GPU训练**
      - 如果需要使用GPU训练时，需要使用到 to(device) 函数，意思就是将数据或者模型放到GPU上面。
    3. 第三点是**参数更新**过程，在进行参数更新时，一共分成三步：
      - 将可训练参数**梯度置零** optimizer.zero_grad()
      - 根据损失值求**梯度** loss.backward()
      - **更新**可训练参数 optimizer.step()
    4. 第四点是将model置为训练模式
      - 对应代码中的model.train()
      - 训练模式会对某些定义的网络模块有影响，比如使用dropout层，在训练时会被激活。
4. **测试**接口程序，比如名称为evaluate.py。
  - 测试程序比训练程序简单很多，主要先进行两个步骤：
    - 定义网络
    - 载入已训练参数
  - 然后就可以读取测试样本进行前传了。需要注意的是，在进行模型的测试时，需要将model置为测试模式，对应代码中的model.eval()

PyTorch程序比较规范，简洁与清晰。并且在PyTorch中还使用到了非常多的面向对象的规范，有许多操作都是通过继承Python类进行实现的。

## 代码示例

代码示例

```python
# （1）======= 模型定义 =======
class My_Network(nn.Module):
    # 继承自nn.Module
    def __init__(self): # 重写构造函数定义网络结构
        super(network, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) # 3×3卷积
        self.bn1 = nn.BatchNorm2d(64) # BatchNorm
        self.relu = nn.ReLU() # 激活
        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) # 最大池化
        self.fc = nn.Linear(64 * 14 * 14, 512) # 全连接，将[n, (64*14*14)]变成[n, 512]
        for m in self.modules(): # 对可训练参数进行初始化（上面定义的所有模块都存储在此）
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x): # 重写forward实现前传, 定义网络结构
        # 前传，输入x尺寸为[n, c, 28, 28]
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        x = x.view(x.size(0), -1) #将四维Tensor变成两维，作为全连接的输入
        x = self.fc(x)
        return x

# （2）======= 数据读取 =======
class My_Dataset(torch.utils.data.Dataset):
    
    def __init__(self, data_txt_path):
        data_list = read_txt(data_txt_path) #读取一下记录数据与标签的txt
        self.data_list = np.random.permutation(data_list) #打乱一下txt
        self.transform = T.ToTensor() #预处理，将数据转化成[n, c, h, w]的形式，并归一化到[0,1]

    def __getitem__(self, index):
        sample = self.data_list[index] #读取txt中的一行，是一个数据对(image_path, label)
        image_path = sample.split(' ')[0]
        label = int(sample.split(' ')[-1])
        resized_image = read_image(image_path) #读取一下图像
        image = self.transform(resized_image) #转化成Tensor
        return image.float(), label 

    def __len__(self):
        return len(self.data_list)

# （3）======= 训练 ==========
from __future__ import print_function
import torch.nn
from torch.nn import DataParallel
import torch.optim
import torch.utils.data
import argparse

#import 所需的各种模块
parser = argparse.ArgumentParser(description="")
#添加各种自定义参数
args = parser.parse_args()
def main():
    device = torch.device("cuda")
    train_dataset = My_Dataset(txt_path) 
    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle, num_workers)
    model = My_Network()
    #若有预训练参数，可以载入预训练参数
    model.load_state_dict(torch.load(ckpt_path), strict=False)
    #定义损失，比如criterion = torch.nn.CrossEntropyLoss()
    criterion = My_Loss
    model.to(device)
    #可以将模型训练放在多张GPU上并行
    model = DataParallel(model)
    optimizer = torch.optim.SGD(model.parameters(), lr, weight_decay)
    model.train() #将模型置为训练模式
     
    for i in range(epoch):
        for ii, data in enumerate(train_data_loader):
            data_input, label = data
            data_input = data_input.to(device)
            label = label.to(device).long()
            output = model(data_input)
            loss = criterion(output, label)
            print("loss =: ", loss)
            optimizer.zero_grad() #首先梯度置零
            loss.backward() #然后求梯度
            optimizer.step() #通过梯度更新参数
            iters = i * len(train_data_loader) + ii
            if iters % save_interval == 0:
                torch.save(model.state_dict(), save_path)
 
if __name__ == "__main__":
    main()

# （4）======= 测试 =======
from __future__ import print_function
import torch.nn
from torch.nn import DataParallel
import torch.optim
import torch.utils.data
import argparse
#import 所需的各种模块

parser = argparse.ArgumentParser(description="")
#添加各种自定义参数
args = parser.parse_args()
 
def main():
    device = torch.device("cuda")
    model = My_Network()
    model = DataParallel(model)
    model.load_state_dict(torch.load(snapshot_path)) #载入参数
    model.to(device)
    model.eval() #将模型置为测试模式
 
    for eval_data_path in eval_data_list:
        eval_data = read_image(eval_data_path)
        data = torch.from_numpy(eval_data) #将data转化为Tensor
        data = data.to(device)
        output = model(data) #前传得到结果
        #自定义操作，比如计算精度
 
if __name__ == "__main__":
    main(）
```

## 常用代码

[PyTorch常用代码段](https://zhuanlan.zhihu.com/p/104019160)

### 1. 基本配置

#### 导入包和版本查询

```python
import torch
import torch.nn as nn
import torchvision
print(torch.__version__)
print(torch.version.cuda)
print(torch.backends.cudnn.version())
print(torch.cuda.get_device_name(0))
```

#### 可复现性

在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。

```python
np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

#### 显卡设置


```python
# 如果只需要一张显卡
# Device configuration 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 如果需要指定多张显卡，比如0，1号显卡。
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'

# 清除显存
torch.cuda.empty_cache()
```

也可以在命令行运行代码时设置显卡：

```shell
# 命令行指定显卡
CUDA_VISIBLE_DEVICES=0,1 python train.py
# 命令行重置GPU的指令（清楚显存）
nvidia-smi --gpu-reset -i [gpu_id]
```


### 2. 张量(Tensor)处理

#### 张量原理

tensor数据采用**头信息区**（Tensor）和**存储区** （Storage）分开存储的形式；
- 定义张量A，其形状size、步长stride、索引信息都存储在头信息区，真实数据在存储区
- 对A进行修改操作，赋值给B，此时B的头信息区变化，但数据存储区与A共享——<spane style='color:red'>浅拷贝</span>

```python
import torch
a = torch.arange(5)  # 初始化张量 a 为 [0, 1, 2, 3, 4]
b = a[2:]            # 截取张量a的部分值并赋值给b，b其实只是改变了a对数据的索引方式
print('a:', a)
print('b:', b)
print('ptr of storage of a:', a.storage().data_ptr())  # 打印a的存储区地址
print('ptr of storage of b:', b.storage().data_ptr())  # 打印b的存储区地址,可以发现两者是共用存储区
print('==================================================================')
b[1] = 0    # 修改b中索引为1，即a中索引为3的数据为0
print('a:', a)
print('b:', b)
print('ptr of storage of a:', a.storage().data_ptr())  # 打印a的存储区地址,可以发现a的相应位置的值也跟着改变，说明两者是共用存储区
print('ptr of storage of b:', b.storage().data_ptr())  # 打印b的存储区地址
#  运行结果
# a: tensor([0, 1, 2, 3, 4])
# b: tensor([2, 3, 4])
# ptr of storage of a: 2862826251264
# ptr of storage of b: 2862826251264
# ==================================================================
# a: tensor([0, 1, 2, 0, 4])
# b: tensor([2, 0, 4])
# ptr of storage of a: 2862826251264
# ptr of storage of b: 2862826251264
```

#### 张量基本信息

torch的tensor也有步长属性。tensor的步长是从索引中的一个维度跨到下一个维度中间的**跨度**
- 卷积神经网络中卷积核对特征图的卷积操作也是有stride属性，但这两个stride可完全不是一个意思

```python
import torch

tensor = torch.randn(3,4,5)
print(tensor.type())  # 数据类型
print(tensor.size())  # 张量的shape，是个元组
print(tensor.dim())   # 维度的数量
print('size   of a:', a.size())    # 查看a的shape
print('stride of a:', a.stride())  # 查看a的stride
print(a.storage_offset()) # 初始偏移量
print(a.storage())    # 打印a的储存区真实的数据
print('ptr of storage of a: ', a.storage().data_ptr())  # 查看a的storage区的地址

a = torch.arange(6).reshape(2, 3)  # 初始化张量 a
b = torch.arange(6).view(3, 2)     # 初始化张量 b
print('a:', a)
print('stride of a:', a.stride())  # 打印a的stride
print('b:', b)
print('stride of b:', b.stride())  # 打印b的stride

#   运行结果
# a: tensor([[0, 1, 2],
#            [3, 4, 5]])
# stride of a: (3, 1)
#  
# b: tensor([[0, 1],
#            [2, 3],
#            [4, 5]])
# stride of b: (2, 1)
```

#### 命名张量

**张量命名**是一个非常有用的方法，这样可以方便地使用**维度名字**来做索引或其他操作，大大提高了可读性、易用性，防止出错。

```python
# 在PyTorch 1.3之前，需要使用注释
# Tensor[N, C, H, W]
images = torch.randn(32, 3, 56, 56)
images.sum(dim=1)
images.select(dim=1, index=0)
# PyTorch 1.3之后
NCHW = [‘N’, ‘C’, ‘H’, ‘W’]
images = torch.randn(32, 3, 56, 56, names=NCHW)
images.sum('C') # 使用维度名字索引
images.select('C', index=0)
# 也可以这么设置
tensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))
# 使用align_to可以对维度方便地排序
tensor = tensor.align_to('N', 'C', 'H', 'W')
```

#### 张量类型

PyTorch有9种CPU张量类型和9种GPU张量类型。
- ![](https://pic1.zhimg.com/80/v2-e49fdbb7cee05950a4108e607875bdd4_720w.jpg)

torch.Tensor是一种包含单一数据类型元素的多维矩阵。
- torch.Tensor是默认的tensor类型（torch.FlaotTensor）的简称。

实际上，Torch定义了**7种**CPU tensor类型和**8种**GPU tensor类型：

| Data type | 	CPU tensor |	GPU tensor |
|---|---|---|
| 32-bit floating point	| torch.FloatTensor	| torch.cuda.FloatTensor |
| 64-bit floating point	| torch.DoubleTensor |	torch.cuda.DoubleTensor |
| 16-bit floating point	| N/A	| torch.cuda.HalfTensor |
| 8-bit integer (unsigned)|	torch.ByteTensor	| torch.cuda.ByteTensor |
| 8-bit integer (signed) |	torch.CharTensor	| torch.cuda.CharTensor |
| 16-bit integer (signed)| torch.ShortTensor	| torch.cuda.ShortTensor |
| 32-bit integer (signed)|	torch.IntTensor	| torch.cuda.IntTensor |
| 64-bit integer (signed)|	torch.LongTensor	| torch.cuda.LongTensor |

每一个张量tensor都有一个相应的torch.Storage用来保存其数据。
- 类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算。

注意：
- **改变**tensor的函数操作会用一个**下划线**后缀来标示。否则，<span style="color:red">不会改变原tensor值！</span>
- torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor
- tensor.FloatTensor.abs()将会在一个新的tensor中计算结果。
- 类似的，in-place运算还有：acos_, asin_, add_, addbmm_, addcmul_, ceil_
- 更多信息，参考[官网](https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/)

```python
# 通过list或序列构建tensor
a = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
print(a[1][2]) # 6.0
a[0][1] = 8 # 索引和切片来获取和修改张量
print(a)
# 指定大小，创建tensor
torch.IntTensor(2, 4).zero_()
```

#### 数据类型转换

方法
- 直接转换：直接使用float, long转换
  - tensor.int()
  - tensor.type(torch.IntTensor)
- 参考转换：
  - type_as转成指定变量同类型

```python
# 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor
torch.set_default_tensor_type(torch.FloatTensor)
# 类型转换
tensor = tensor.cuda()
tensor = tensor.cpu()
newtensor = tensor.char() # char
tensor = tensor.float()
newtensor = tensor.byte() # short类型
tensor = tensor.long()
newtensor = tensor.half() # 半精度浮点类型
newtensor = tensor.int() # 整型
newtensor = tensor.double()

# type_as 转换为指定变量的类型
tensor1=torch.FloatTensor(4)
tensor2=torch.IntTensor(3)
tensor1=tensor1.type_as(tensor2) # type_as转换
tensor1.type(torch.FloatTensor) # type 转换
```

torch.Tensor与np.ndarray转换
- 除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。

```python
ndarray = tensor.cpu().numpy()
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.
```

Torch.tensor与PIL.Image转换

```python
# pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化
# torch.Tensor -> PIL.Image
image = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())
image = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way

# PIL.Image -> torch.Tensor
path = r'./figure.jpg'
tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))).permute(2,0,1).float() / 255
tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way
np.ndarray与PIL.Image的转换
image = PIL.Image.fromarray(ndarray.astype(np.uint8))

ndarray = np.asarray(PIL.Image.open(path))
# 从只包含一个元素的张量中提取值
value = torch.rand(1).item()
```

#### 张量形变

view 和 reshape 都是用来重塑tensor的shape的。
- view只适合对满足**连续性**条件（contiguous）的tensor进行操作
- 而reshape同时还可以对不满足连续性条件的tensor进行操作，具有更好的鲁棒性。
- view能干的reshape都能干，如果view不能干就可以用reshape来处理—— <span style='color:red'>view ＜ reshape </span>
- 满足tensor连续性条件时，a.reshape返回的结果与a.view()相同，否则返回的结果与a.contiguous().view()相同

如果不满足连续性条件
- 用contiguous()方法将原始tensor转换为满足连续条件的tensor，然后就可以使用view方法进行shape变换了。
- 或者直接使用reshape方法进行维度变换，但这种方法变换后的tensor就不是与原始tensor**共享内存**了，而是被重新开辟了一个空间。

最后总结一下view()、reshape()、reszie_()三者的关系和区别。
- view()只能对满足连续性要求的tensor使用。
- 当tensor满足**连续性**要求时，<span style='color:red'>reshape() = view()</span>，和原来tensor共用内存。
- 当tensor不满足连续性要求时，<span style='color:red'>reshape() = contiguous() + view()</span>，会产生新的存储区的tensor，与原来tensor不共用内存。
- resize_()可以随意获取**任意维度**的tensor，不用在意真实数据的个数限制，但是不推荐使用。

view函数的作用为重构张量的维度，相当于numpy中resize的功能，但是用法可能不太一样

```python
tt=torch.tensor([-0.3623, -0.6115,  0.7283,  0.4699,  2.3261,  0.1599])
result=tt.view(3,2) # 将一维的tt1重构成3x2维的张量
result=tt.view(-1) # 变成1维
result=tt.view(2,-1) # 变成2行，列数不定，由元素总数而定

# 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，
# 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。
tensor = torch.rand(2,3,4)
shape = (6, 4)
tensor = torch.reshape(tensor, shape)
# 打乱顺序
tensor = tensor[torch.randperm(tensor.size(0))]  # 打乱第一个维度
# 水平翻转
# pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现
# 假设张量的维度为[N, D, H, W].
tensor = tensor[:,:,:,torch.arange(tensor.size(3) - 1, -1, -1).long()]
```

#### 维度变化

permute(dims): 将tensor的维度换位
- 参考：[Pytorch之permute函数](https://zhuanlan.zhihu.com/p/76583143)

（1）transpose与permute的异同
- Tensor.**permute**(a,b,c,d, ...)：permute函数可以对任意高维矩阵进行转置，但没有 torch.permute() 这个调用方式， 只能 Tensor.permute()
- torch.**transpose**(Tensor, a,b)：transpose只能操作2D矩阵的转置，有两种调用方式
  - 连续使用transpose也可实现permute的效果
- permute相当于可以同时操作于tensor的若干维度，transpose只能同时作用于tensor的两个维度；

（2）permute函数与contiguous、view函数之关联
- contiguous：view只能作用在contiguous的variable上，如果在view之前调用了transpose、permute等，就需要调用contiguous()来返回一个contiguous copy；
- 一种可能的解释是：有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式；

判断ternsor是否为contiguous，可以调用torch.Tensor.is_contiguous()函数


```python
import torch 

x = torch.randn(2, 3, 5) 
x.size()  # torch.Size([2, 3, 5]) 
x.permute(2, 0, 1).size() # torch.Size([5, 2, 3])

torch.randn(2,3,4,5).permute(3,2,0,1).shape # torch.Size([5, 4, 2, 3])
torch.randn(2,3,4,5).transpose(3,0).transpose(2,1).transpose(3,2).shape # torch.Size([5, 4, 2, 3])
torch.randn(2,3,4,5).transpose(1,0).transpose(2,1).transpose(3,1).shape # torch.Size([3, 5, 2, 4])

x = torch.ones(10, 10) 
x.is_contiguous()                                 # True 
x.transpose(0, 1).is_contiguous()                 # False
x.transpose(0, 1).contiguous().is_contiguous()    # True

a=np.array([[[1,2,3],[4,5,6]]])
unpermuted=torch.tensor(a)
print(unpermuted.size())              #  ——>  torch.Size([1, 2, 3])
permuted=unpermuted.permute(2,0,1)
print(permuted.size())                #  ——>  torch.Size([3, 1, 2])
view_test = unpermuted.view(1,3,2)
print(view_test.size())               #  ——>  torch.Size([1, 3, 2])
```


#### 复制张量

| Operation                 |  New/Shared memory | Still in computation graph |
| --- | --- |--- |
| tensor.clone()            |        New         |          Yes               |
| tensor.detach()           |      Shared        |          No                |
| tensor.detach.clone()()   |        New         |          No                |

#### 张量拼接

注意
- torch.cat 和 torch.stack 的区别在于 torch.cat 沿着给定的维度拼接，
- 而 torch.stack 会新增一维。例如当参数是3个10x5的张量，torch.cat 的结果是30x5的张量，
- 而torch.stack的结果是3x10x5的张量。

```python
tensor = torch.cat(list_of_tensors, dim=0)
tensor = torch.stack(list_of_tensors, dim=0)
# 将整数标签转为one-hot编码
# pytorch的标记默认从0开始
tensor = torch.tensor([0, 2, 1, 3])
N = tensor.size(0)
num_classes = 4
one_hot = torch.zeros(N, num_classes).long()
one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())
```

#### 得到非零元素

```python
torch.nonzero(tensor)               # index of non-zero elements
torch.nonzero(tensor==0)            # index of zero elements
torch.nonzero(tensor).size(0)       # number of non-zero elements
torch.nonzero(tensor == 0).size(0)  # number of zero elements
```

#### 判断两个张量相等

```python
torch.allclose(tensor1, tensor2)  # float tensor
torch.equal(tensor1, tensor2)     # int tensor
```

#### 张量扩展

```python
# Expand tensor of shape 64*512 to shape 64*512*7*7.
tensor = torch.rand(64,512)
torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)
```

#### 矩阵乘法

```python
# Matrix multiplcation: (m*n) * (n*p) * -> (m*p).
result = torch.mm(tensor1, tensor2)
# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p)
result = torch.bmm(tensor1, tensor2)
# Element-wise multiplication.
result = tensor1 * tensor2
```

计算两组数据之间的两两欧式距离
- 利用broadcast机制

```python
dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))
```

### 3. 模型定义和操作

#### MLP 多层感知机

【2022-9-21】[pytorch教程：层和块](https://nbviewer.org/github/d2l-ai/d2l-zh-pytorch-slides/blob/main/chapter_deep-learning-computation/model-construction.ipynb)

定义模型结构的几种方法
- 直接顺序组装：nn.Sequential，不用定义网络结构
- 自定义网络结构：nn.Sequential，使用forward函数组装
- 根据参数顺序组装

快捷定义

```python
import torch
from torch import nn
from torch.nn import functional as F
# x -> FC -> Relu -> FC
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
X = torch.rand(2, 20)
net(X)
```

自定义
- nn.Sequential定义了一种特殊的Module

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)

    def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
# 实例化模型
net = MLP()
net(X)
```

参数传入各组件，顺序组装

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        # 依次遍历、组装各个传入的组件
        for idx, module in enumerate(args):
            self._modules[str(idx)] = module

    def forward(self, X):
        for block in self._modules.values():
            X = block(X)
        return X

net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

前向传播中执行代码

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        X = self.linear(X)
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()

net = FixedHiddenMLP()
net(X)
```

混合搭配各种方法

```python
class NestMLP(nn.Module): # 嵌套结构
    def __init__(self):
        super().__init__()
        # 顺序结构
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        # 自定义
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```


#### Embedding 词嵌入

nn.Embedding用法
- 一个简单的存储固定大小的**词典**的嵌入向量的**查找表**
- 给一个编号，**嵌入层**返回这个编号对应的**嵌入向量**，嵌入向量反映了各个编号代表的符号之间的语义关系。
- 输入为一个编号列表，输出为对应的符号嵌入向量列表。每个句子结尾要加EOS
  - input: ['i', 'am', 'a', 'boy']
  - output: [3, 6, 5, 10]

[通俗讲解pytorch中nn.Embedding原理及使用](https://www.jianshu.com/p/63e7acc5e890)

```python
torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None,
  max_norm=None,  norm_type=2.0,   scale_grad_by_freq=False, 
  sparse=False,  _weight=None)
# 建立词向量层
# 随机初始化建立了词向量层后，建立了一个“二维表”，存储了词典中每个词的词向量
embed = torch.nn.Embedding(n_vocabulary, embedding_size)
```

参数解释
- num_embeddings (python:int) – **词典大小**，比如总共出现5000个词，那就输入5000。此时index为（0-4999）
- embedding_dim (python:int) – **嵌入向量维度**，即用多少维来表示一个符号。
- padding_idx (python:int, optional) – **填充id**，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。（初始化为0）
- max_norm (python:float, optional) – **最大范数**，如果嵌入向量的范数超过了这个界限，就要进行再归一化。
- norm_type (python:float, optional) – 指定利用什么**范数**计算，并用于对比max_norm，默认为**2范数**。
- scale_grad_by_freq (boolean, optional) – 根据单词在mini-batch中出现的频率，对梯度进行**放缩**。默认为False.
- sparse (bool, optional) – 若为True,则与权重矩阵相关的梯度转变为**稀疏张量**。

```python
import torch
import torch.nn as nn

# an Embedding module containing 10 tensors of size 3
embedding = nn.Embedding(10, 3) # 10个单词的词库，每个词语3维
# a batch of 2 samples of 4 indices each
input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
print('默认填充：', embedding(input))

# example with padding_idx 加填充
embedding = nn.Embedding(10, 3, padding_idx=0)
input = torch.LongTensor([[0,2,0,5]])
print('padding填充：', embedding(input))

# example of changing `pad` vector 改变填充向量
padding_idx = 0
embedding = nn.Embedding(3, 3, padding_idx=padding_idx)
embedding.weight
with torch.no_grad():
    embedding.weight[padding_idx] = torch.ones(3)
print('自定义填充：', embedding.weight)
```


#### 卷积网络

一个简单两层卷积网络的示例

```python
# convolutional neural network (2 convolutional layers)
class ConvNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7*7*32, num_classes)
    
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out

model = ConvNet(num_classes).to(device)
```

卷积层的计算和展示可以用[这个网站](https://ezyang.github.io/convolution-visualizer/index.html)辅助。


#### 双线性汇合（bilinear pooling）

```python
X = torch.reshape(N, D, H * W)                        # Assume X has shape N*D*H*W
X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling
assert X.size() == (N, D, D)
X = torch.reshape(X, (N, D * D))
X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization
X = torch.nn.functional.normalize(X)                  # L2 normalization
```

#### 多卡同步 BN（Batch normalization）

当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。

```python
sync_bn = torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
```

将已有网络的所有BN层改为同步BN层

```python
def convertBNtoSyncBN(module, process_group=None):
    '''Recursively replace all BN layers to SyncBN layer.

    Args:
        module[torch.nn.Module]. Network
    '''
    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):
        sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum, 
                                         module.affine, module.track_running_stats, process_group)
        sync_bn.running_mean = module.running_mean
        sync_bn.running_var = module.running_var
        if module.affine:
            sync_bn.weight = module.weight.clone().detach()
            sync_bn.bias = module.bias.clone().detach()
        return sync_bn
    else:
        for name, child_module in module.named_children():
            setattr(module, name) = convert_syncbn_model(child_module, process_group=process_group))
        return module
```

#### 类似 BN 滑动平均

如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。

```python
class BN(torch.nn.Module)
    def __init__(self):
        ...
        self.register_buffer('running_mean', torch.zeros(num_features))

    def forward(self, X):
        ...
        self.running_mean += momentum * (current - self.running_mean)
```

#### torch.nn.functional

建图过程中往往有两种层
- 一种含参数、有Variable，如全连接层，卷积层、Batch Normlization层等；
- 另一种不含参数、无Variable，如Pooling层，Relu层，损失函数层等。

阅读源码发现
- nn.里的函数继承自 nn.module 初始化为实例化一个类，如果含参数，则会帮你初始化好参数。
- nn.functional.里的函数，则是相当于**直接**一个函数句柄给你，如果需要参数，则需要自己**输入参数**，且并不会“记住”这个参数。

选择
- 如果要涉及到参数计算的，那用nn.里的；
- 如果不需要涉及更新参数，只是一次性计算，那用nn.functional.里面的。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(512, 256)
        self.fc2 = nn.Linear(256, 256)
        self.fc3 = nn.Linear(256, 2)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(F.dropout(self.fc2(x), 0.5))
        x = F.dropout(self.fc3(x), 0.5)
        return x
```

三层网络为例。需要维持状态的主要是三个线性变换，所以在构造Module是，定义了三个nn.Linear对象，而在计算时，relu,dropout之类不需要保存状态的可以直接使用。
- 注：dropout的话有个坑，需要设置自行设定training的state。

对比示例
- 用nn.Xxx定义一个CNN
- 用nn.function.xxx定义一个与上面相同的CNN

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    
    def __init__(self):
        super(CNN, self).__init__()
        
        self.cnn1 = nn.Conv2d(in_channels=1,  out_channels=16, kernel_size=5,padding=0)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)
        
        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5,  padding=0)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)
        
        self.linear1 = nn.Linear(4 * 4 * 32, 10)
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        out = self.maxpool1(self.relu1(self.cnn1(x)))
        out = self.maxpool2(self.relu2(self.cnn2(out)))
        out = self.linear1(out.view(x.size(0), -1))
        return out
```


```python
class CNN(nn.Module):
    
    
    def __init__(self):
        super(CNN, self).__init__()
        
        self.cnn1_weight = nn.Parameter(torch.rand(16, 1, 5, 5))
        self.bias1_weight = nn.Parameter(torch.rand(16))
        
        self.cnn2_weight = nn.Parameter(torch.rand(32, 16, 5, 5))
        self.bias2_weight = nn.Parameter(torch.rand(32))
        
        self.linear1_weight = nn.Parameter(torch.rand(4 * 4 * 32, 10))
        self.bias3_weight = nn.Parameter(torch.rand(10))
        
    def forward(self, x):
        x = x.view(x.size(0), -1)
        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)
        out = F.relu(out)
        out = F.max_pool2d(out)
        
        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)
        out = F.relu(out)
        out = F.max_pool2d(out)
        
        out = F.linear(x, self.linear1_weight, self.bias3_weight)
        return out
```

两种定义方式得到CNN功能都是相同的，但PyTorch官方推荐：
- **有**学习参数的（例如，conv2d, linear, batch_norm)采用nn.Xxx方式
- **没有**学习参数的（例如，maxpool, loss func, activation func）等根据个人选择使用nn.functional.xxx或者nn.Xxx方式。

但关于dropout，强烈推荐使用nn.Xxx方式，因为一般情况下只有训练阶段才进行dropout，在eval阶段都不会进行dropout。使用nn.Xxx方式定义dropout，在调用model.eval()之后，model中所有的dropout layer都关闭，但以nn.function.dropout方式定义dropout，在调用model.eval()之后并不能关闭dropout。

[nn 与 nn.functional 有什么区别？](https://www.zhihu.com/question/66782101)

#### 计算模型整体参数量

```python
num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())
```

#### 查看网络中的参数

- 通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）

```python
params = list(model.named_parameters())
(name, param) = params[28]
print(name)
print(param.grad)
print('-------------------------------------------------')
(name2, param2) = params[29]
print(name2)
print(param2.grad)
print('----------------------------------------------------')
(name1, param1) = params[30]
print(name1)
print(param1.grad)
```

#### 模型可视化（使用pytorchviz）

[szagoruyko/pytorchviz](github.com/szagoruyko/pytorchviz) 类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）, [sksq96/pytorch-summary](​github.com/sksq96/pytorch-summary)

#### 模型权重初始化

注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。

```python
# Common practise for initialization.
for layer in model.modules():
    if isinstance(layer, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',
                                      nonlinearity='relu')
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.BatchNorm2d):
        torch.nn.init.constant_(layer.weight, val=1.0)
        torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.Linear):
        torch.nn.init.xavier_normal_(layer.weight)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)

# Initialization with given tensor.
layer.weight = torch.nn.Parameter(tensor)
```

#### 提取模型中的某一层

modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。

```python
# 取模型中的前两层
new_model = nn.Sequential(*list(model.children())[:2] 
# 如果希望提取出模型中的所有卷积层，可以像下面这样操作：
for layer in model.named_modules():
    if isinstance(layer[1],nn.Conv2d):
         conv_model.add_module(layer[0],layer[1])
```

#### 部分层使用预训练模型

注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是

```python
model.load_state_dict(torch.load('model.pth'), strict=False)
```

#### 将在 GPU 保存的模型加载到 CPU

```python
model.load_state_dict(torch.load('model.pth', map_location='cpu'))
```

#### 导入另一个模型的相同部分到新的模型

模型导入参数时，如果两个模型结构不一致，则直接导入参数会报错。用下面方法可以把另一个模型的相同的部分导入到新的模型中。

```python
# model_new代表新的模型
# model_saved代表其他模型，比如用torch.load导入的已保存的模型
model_new_dict = model_new.state_dict()
model_common_dict = {k:v for k, v in model_saved.items() if k in model_new_dict.keys()}
model_new_dict.update(model_common_dict)
model_new.load_state_dict(model_new_dict)
```

### 4. 数据处理

#### 计算数据集的均值和标准差

```python
import os
import cv2
import numpy as np
from torch.utils.data import Dataset
from PIL import Image

def compute_mean_and_std(dataset):
    # 输入PyTorch的dataset，输出均值和标准差
    mean_r = 0
    mean_g = 0
    mean_b = 0

    for img, _ in dataset:
        img = np.asarray(img) # change PIL Image to numpy array
        mean_r += np.mean(img[:, :, 0])
        mean_g += np.mean(img[:, :, 1])
        mean_b += np.mean(img[:, :, 2])

    mean_r /= len(dataset)
    mean_g /= len(dataset)
    mean_b /= len(dataset)

    diff_r = 0
    diff_g = 0
    diff_b = 0

    N = 0

    for img, _ in dataset:
        img = np.asarray(img)

        diff_r += np.sum(np.power(img[:, :, 0] - mean_r, 2))
        diff_g += np.sum(np.power(img[:, :, 1] - mean_g, 2))
        diff_b += np.sum(np.power(img[:, :, 2] - mean_b, 2))

        N += np.prod(img[:, :, 0].shape)

    std_r = np.sqrt(diff_r / N)
    std_g = np.sqrt(diff_g / N)
    std_b = np.sqrt(diff_b / N)

    mean = (mean_r.item() / 255.0, mean_g.item() / 255.0, mean_b.item() / 255.0)
    std = (std_r.item() / 255.0, std_g.item() / 255.0, std_b.item() / 255.0)
    return mean, std
```

#### 得到视频数据基本信息

```python
import cv2
video = cv2.VideoCapture(mp4_path)
height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))
width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))
num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
fps = int(video.get(cv2.CAP_PROP_FPS))
video.release()
```

#### TSN 每段（segment）采样一帧视频

```python
K = self._num_segments
if is_train:
    if num_frames > K:
        # Random index for each segment.
        frame_indices = torch.randint(
            high=num_frames // K, size=(K,), dtype=torch.long)
        frame_indices += num_frames // K * torch.arange(K)
    else:
        frame_indices = torch.randint(
            high=num_frames, size=(K - num_frames,), dtype=torch.long)
        frame_indices = torch.sort(torch.cat((
            torch.arange(num_frames), frame_indices)))[0]
else:
    if num_frames > K:
        # Middle index for each segment.
        frame_indices = num_frames / K // 2
        frame_indices += num_frames // K * torch.arange(K)
    else:
        frame_indices = torch.sort(torch.cat((                              
            torch.arange(num_frames), torch.arange(K - num_frames))))[0]
assert frame_indices.size() == (K,)
return [frame_indices[i] for i in range(K)]
```

#### 常用训练和验证数据预处理

其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。

```python
train_transform = torchvision.transforms.Compose([
    torchvision.transforms.RandomResizedCrop(size=224,
                                             scale=(0.08, 1.0)),
    torchvision.transforms.RandomHorizontalFlip(),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                     std=(0.229, 0.224, 0.225)),
 ])
 val_transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),
                                     std=(0.229, 0.224, 0.225)),
])
```

### 5. 模型训练和测试

#### 分类模型训练代码

```python
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
total_step = len(train_loader)
for epoch in range(num_epochs):
    for i ,(images, labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward and optimizer
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if (i+1) % 100 == 0:
            print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'
                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))
```

#### 分类模型测试代码

```python
# Test the model
model.eval()  # eval mode(batch norm uses moving mean/variance 
              #instead of mini-batch mean/variance)
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
    print('Test accuracy of the model on the 10000 test images: {} %'
          .format(100 * correct / total))
```

#### 损失函数

```python
MSE = torch.nn.MSELoss()
MAE = torch.nn.L1Loss()
```


#### 自定义loss

继承torch.nn.Module类写自己的loss。

```python
class MyLoss(torch.nn.Moudle):
    def __init__(self):
        super(MyLoss, self).__init__()
        
    def forward(self, x, y):
        loss = torch.mean((x - y) ** 2)
        return loss
```

#### 标签平滑（label smoothing）

写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：

```python
import torch
import torch.nn as nn


class LSR(nn.Module):

    def __init__(self, e=0.1, reduction='mean'):
        super().__init__()

        self.log_softmax = nn.LogSoftmax(dim=1)
        self.e = e
        self.reduction = reduction
    
    def _one_hot(self, labels, classes, value=1):
        """
            Convert labels to one hot vectors
        
        Args:
            labels: torch tensor in format [label1, label2, label3, ...]
            classes: int, number of classes
            value: label value in one hot vector, default to 1
        
        Returns:
            return one hot format labels in shape [batchsize, classes]
        """

        one_hot = torch.zeros(labels.size(0), classes)

        #labels and value_added  size must match
        labels = labels.view(labels.size(0), -1)
        value_added = torch.Tensor(labels.size(0), 1).fill_(value)

        value_added = value_added.to(labels.device)
        one_hot = one_hot.to(labels.device)

        one_hot.scatter_add_(1, labels, value_added)

        return one_hot

    def _smooth_label(self, target, length, smooth_factor):
        """convert targets to one-hot format, and smooth
        them.
        Args:
            target: target in form with [label1, label2, label_batchsize]
            length: length of one-hot format(number of classes)
            smooth_factor: smooth factor for label smooth
        
        Returns:
            smoothed labels in one hot format
        """
        one_hot = self._one_hot(target, length, value=1 - smooth_factor)
        one_hot += smooth_factor / (length - 1)

        return one_hot.to(target.device)

    def forward(self, x, target):

        if x.size(0) != target.size(0):
            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'
                    .format(x.size(0), target.size(0)))

        if x.dim() < 2:
            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'
                    .format(x.size(0)))

        if x.dim() != 2:
            raise ValueError('Only 2 dimension tensor are implemented, (got {})'
                    .format(x.size()))


        smoothed_target = self._smooth_label(target, x.size(1), self.e)
        x = self.log_softmax(x)
        loss = torch.sum(- x * smoothed_target, dim=1)

        if self.reduction == 'none':
            return loss
        
        elif self.reduction == 'sum':
            return torch.sum(loss)
        
        elif self.reduction == 'mean':
            return torch.mean(loss)
        
        else:
            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')
```

或者直接在训练文件里做label smoothing

```python
for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    N = labels.size(0)
    # C is the number of classes.
    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()
    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)

    score = model(images)
    log_prob = torch.nn.functional.log_softmax(score, dim=1)
    loss = -torch.sum(log_prob * smoothed_labels) / N
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

#### Mixup训练

```python
beta_distribution = torch.distributions.beta.Beta(alpha, alpha)
for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()

    # Mixup images and labels.
    lambda_ = beta_distribution.sample([]).item()
    index = torch.randperm(images.size(0)).cuda()
    mixed_images = lambda_ * images + (1 - lambda_) * images[index, :]
    label_a, label_b = labels, labels[index]

    # Mixup loss.
    scores = model(mixed_images)
    loss = (lambda_ * loss_function(scores, label_a)
            + (1 - lambda_) * loss_function(scores, label_b))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

#### L1 正则化

```python
l1_regularization = torch.nn.L1Loss(reduction='sum')
loss = ...  # Standard cross-entropy loss
for param in model.parameters():
    loss += torch.sum(torch.abs(param))
loss.backward()
```

#### 不对偏置项进行权重衰减（weight decay）

pytorch里的weight decay相当于l2正则

```python
bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias')
others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias')
parameters = [{'parameters': bias_list, 'weight_decay': 0},                
              {'parameters': others_list}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
```

#### 梯度裁剪（gradient clipping）

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)
```

#### 得到当前学习率

```python
# If there is one global learning rate (which is the common case).
lr = next(iter(optimizer.param_groups))['lr']

# If there are multiple learning rates for different layers.
all_lr = []
for param_group in optimizer.param_groups:
    all_lr.append(param_group['lr'])
```

另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0]['lr']

#### 学习率衰减

```python
# Reduce learning rate when validation accuarcy plateau.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)
for t in range(0, 80):
    train(...)
    val(...)
    scheduler.step(val_acc)

# Cosine annealing learning rate.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)
# Reduce learning rate by 10 at given epochs.
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)
for t in range(0, 80):
    scheduler.step()    
    train(...)
    val(...)

# Learning rate warmup by 10 epochs.
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)
for t in range(0, 10):
    scheduler.step()
    train(...)
    val(...)
```

#### 优化器链式更新

从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。

```python
import torch
from torch.optim import SGD
from torch.optim.lr_scheduler import ExponentialLR, StepLR
model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)
for epoch in range(4):
    print(epoch, scheduler2.get_last_lr()[0])
    optimizer.step()
    scheduler1.step()
    scheduler2.step()
```

#### 模型训练可视化

PyTorch可以使用tensorboard来可视化训练过程。

安装和运行TensorBoard。

```shell
pip install tensorboard
tensorboard --logdir=runs
```

使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如'Loss/train'和'Loss/test'。

```python
from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
```

#### 保存与加载断点

注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。

```python
start_epoch = 0
# Load checkpoint.
if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1
    model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    assert os.path.isfile(model_path)
    checkpoint = torch.load(model_path)
    best_acc = checkpoint['best_acc']
    start_epoch = checkpoint['epoch']
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    print('Load checkpoint at epoch {}.'.format(start_epoch))
    print('Best accuracy so far {}.'.format(best_acc))

# Train the model
for epoch in range(start_epoch, num_epochs): 
    ... 

    # Test the model
    ...
        
    # save checkpoint
    is_best = current_acc > best_acc
    best_acc = max(current_acc, best_acc)
    checkpoint = {
        'best_acc': best_acc,
        'epoch': epoch + 1,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    model_path = os.path.join('model', 'checkpoint.pth.tar')
    best_model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    torch.save(checkpoint, model_path)
    if is_best:
        shutil.copy(model_path, best_model_path)
```

提取 ImageNet 预训练模型某层的卷积特征

```python
# VGG-16 relu5-3 feature.
model = torchvision.models.vgg16(pretrained=True).features[:-1]
# VGG-16 pool5 feature.
model = torchvision.models.vgg16(pretrained=True).features
# VGG-16 fc7 feature.
model = torchvision.models.vgg16(pretrained=True)
model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])
# ResNet GAP feature.
model = torchvision.models.resnet18(pretrained=True)
model = torch.nn.Sequential(collections.OrderedDict(
    list(model.named_children())[:-1]))

with torch.no_grad():
    model.eval()
    conv_representation = model(image)
```

提取 ImageNet 预训练模型多层的卷积特征

```python
class FeatureExtractor(torch.nn.Module):
    """Helper class to extract several convolution features from the given
    pre-trained model.

    Attributes:
        _model, torch.nn.Module.
        _layers_to_extract, list<str> or set<str>

    Example:
        >>> model = torchvision.models.resnet152(pretrained=True)
        >>> model = torch.nn.Sequential(collections.OrderedDict(
                list(model.named_children())[:-1]))
        >>> conv_representation = FeatureExtractor(
                pretrained_model=model,
                layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image)
    """
    def __init__(self, pretrained_model, layers_to_extract):
        torch.nn.Module.__init__(self)
        self._model = pretrained_model
        self._model.eval()
        self._layers_to_extract = set(layers_to_extract)

    def forward(self, x):
        with torch.no_grad():
            conv_representation = []
            for name, layer in self._model.named_children():
                x = layer(x)
                if name in self._layers_to_extract:
                    conv_representation.append(x)
            return conv_representation
```

#### 微调全连接层

```python
model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
model.fc = nn.Linear(512, 100)  # Replace the last fc layer
optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)
```

以较大学习率微调全连接层，较小学习率微调卷积层

```python
model = torchvision.models.resnet18(pretrained=True)
finetuned_parameters = list(map(id, model.fc.parameters()))
conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)
parameters = [{'params': conv_parameters, 'lr': 1e-3}, 
              {'params': model.fc.parameters()}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
```

### 6. 其他注意事项

- 不要使用太大的线性层。因为nn.Linear(m,n)使用的是 O(mn) 的内存，线性层太大很容易超出现有显存。
- 不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。
- model(x) 前用 model.train() 和 model.eval() 切换网络状态。
- 不需要计算梯度的代码块用 with torch.no_grad() 包含起来。
- model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。
- model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.
- torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。
- loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。
- torch.utils.data.DataLoader 中尽量设置 pin_memory=True，对特别小的数据集如 MNIST 设置 pin_memory=False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。
- 用 del 及时删除不用的中间变量，节约 GPU 存储。
- 使用 inplace 操作可节约 GPU 存储，如
  - x = torch.nn.functional.relu(x, inplace=True)
- 减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。
- 使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。
- 时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。
- 除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。
- 统计代码各部分耗时

```python
with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:
    ...
print(profile)
# 或者在命令行运行
python -m torch.utils.bottleneck main.py
```

使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。

```python
# pip install torchsnooper
import torchsnooper

# 对于函数，使用修饰器
@torchsnooper.snoop()

# 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。
with torchsnooper.snoop():
    原本的代码
```

[https://github.com/zasdfgbnm/TorchSnooper](​github.com/zasdfgbnm/TorchSnooper)
模型可解释性，使用[captum库](https://captum.ai/)

## pytorch可视化

[Pytorch可视化](https://mp.weixin.qq.com/s/O491K2-dY6mzBw_byrg8bA)

示例代码

```python
import torch
import torch.nn as nn
 
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 16, 3, 1, 1),
            nn.ReLU(),
            nn.AvgPool2d(2, 2)
        )

        self.conv2 = nn.Sequential(
            nn.Conv2d(16, 32, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )

        self.fc = nn.Sequential(
            nn.Linear(32 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )

        self.out = nn.Linear(64, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        output = self.out(x)
        return output

# 输出网络结构
MyConvNet = ConvNet()
print(MyConvNet)

```

有了基本的神经网络后，分别通过HiddenLayer和PyTorchViz库来可视化上述的卷积层神经网络。

### HiddenLayer可视化

安装库

```shell
pip install hiddenlayer
```

可视化

```python
import hiddenlayer as h
vis_graph = h.build_graph(MyConvNet, torch.zeros([1 ,1, 28, 28]))   # 获取绘制图像的对象
vis_graph.theme = h.graph.THEMES["blue"].copy()     # 指定主题颜色
vis_graph.save("./demo1.png")   # 保存图像的路径
```

可视化训练过程
- HiddenLayer 适用于小网络可视化，其它建议使用更复杂的 tensorboardX
- 不同于tensorboard，hiddenlayer会在程序运行的过程中动态生成图像，而不是模型训练完后

```python
import hiddenlayer as hl
import time

# 记录训练过程的指标
history = hl.History()
# 使用canvas进行可视化
canvas = hl.Canvas()

# 获取优化器和损失函数
optimizer = torch.optim.Adam(MyConvNet.parameters(), lr=3e-4)
loss_func = nn.CrossEntropyLoss()
log_step_interval = 100      # 记录的步数间隔

for epoch in range(5):
    print("epoch:", epoch)
    # 每一轮都遍历一遍数据加载器
    for step, (x, y) in enumerate(train_loader):
        # 前向计算->计算损失函数->(从损失函数)反向传播->更新网络
        predict = MyConvNet(x)
        loss = loss_func(predict, y)
        optimizer.zero_grad()   # 清空梯度（可以不写）
        loss.backward()     # 反向传播计算梯度
        optimizer.step()    # 更新网络
        global_iter_num = epoch * len(train_loader) + step + 1  # 计算当前是从训练开始时的第几步(全迭代次数)
        if global_iter_num % log_step_interval == 0:
            # 控制台输出一下
            print("global_step:{}, loss:{:.2}".format(global_iter_num, loss.item()))
            # 在测试集上预测并计算正确率
            test_predict = MyConvNet(test_data_x)
            _, predict_idx = torch.max(test_predict, 1)  # 计算softmax后的最大值的索引，即预测结果
            acc = accuracy_score(test_data_y, predict_idx)

            # 以epoch和step为索引，创建日志字典
            history.log((epoch, step),
                        train_loss=loss,
                        test_acc=acc,
                        hidden_weight=MyConvNet.fc[2].weight)

            # 可视化
            with canvas:
                canvas.draw_plot(history["train_loss"])
                canvas.draw_plot(history["test_acc"])
                canvas.draw_image(history["hidden_weight"])
```


### PyTorchViz可视化

安装

```shell
pip install torchviz
```

只使用可视化函数make_dot()来获取绘图对象，基本使用和HiddenLayer差不多，不同的地方在于PyTorch绘图之前可以指定一个网络的输入值和预测值。

```python
from torchviz import make_dot
x = torch.randn(1, 1, 28, 28).requires_grad_(True)  # 定义一个网络的输入值
y = MyConvNet(x)    # 获取网络的预测值
 
MyConvNetVis = make_dot(y, params=dict(list(MyConvNet.named_parameters()) + [('x', x)]))
MyConvNetVis.format = "png"
# 指定文件生成的文件夹
MyConvNetVis.directory = "data"
# 生成文件
MyConvNetVis.view()
```

打开与上述代码相同根目录下的data文件夹，里面会有一个.gv文件和一个.png文件，其中的.gv文件是Graphviz工具生成图片的脚本代码，.png是.gv文件编译生成的图片，直接打开.png文件就行。
- 默认情况下，上述程序运行后会自动打开.png文件

### tensorboardX 可视化

数据集准备

```python
import torchvision
import torch.utils.data as Data
# 准备训练用的MNIST数据集
train_data = torchvision.datasets.MNIST(
    root = "./data/MNIST",  # 提取数据的路径
    train=True, # 使用MNIST内的训练数据
    transform=torchvision.transforms.ToTensor(),    # 转换成torch.tensor
    download=False   # 如果是第一次运行的话，置为True，表示下载数据集到root目录
)

# 定义loader
train_loader = Data.DataLoader(
    dataset=train_data,
    batch_size=128,
    shuffle=True,
    num_workers=0
)

test_data = torchvision.datasets.MNIST(
    root="./data/MNIST",
    train=False,    # 使用测试数据
    download=False
)

# 将测试数据压缩到0-1
test_data_x = test_data.data.type(torch.FloatTensor) / 255.0
test_data_x = torch.unsqueeze(test_data_x, dim=1)
test_data_y = test_data.targets

# 打印一下测试数据和训练数据的shape
print("test_data_x.shape:", test_data_x.shape)
print("test_data_y.shape:", test_data_y.shape)

for x, y in train_loader:
    print(x.shape)
    print(y.shape)
    break
```

tensorboardX可视化训练过程
- `tensorboard` 是谷歌开发的深度学习框架tensorflow的一套深度学习可视化神器，pytorch团队开发出了 `tensorboardX` 来让pytorch的玩家也能享受tensorboard的福利。

先安装相关的库：

```shell
pip install tensorboardX
pip install tensorboard
```

并将tensorboard.exe所在的文件夹路径加入环境变量path中

tensorboardX的使用过程。
- 基本使用为，先通过tensorboardX下的SummaryWriter类获取一个日志编写器对象。
- 然后通过这个对象的一组方法往日志中添加事件，即生成相应的图片，最后启动前端服务器，在localhost中就可以看到最终的结果了。

```python
from tensorboardX import SummaryWriter

logger = SummaryWriter(log_dir="data/log")

# 获取优化器和损失函数
optimizer = torch.optim.Adam(MyConvNet.parameters(), lr=3e-4)
loss_func = nn.CrossEntropyLoss()
log_step_interval = 100      # 记录的步数间隔

for epoch in range(5):
    print("epoch:", epoch)
    # 每一轮都遍历一遍数据加载器
    for step, (x, y) in enumerate(train_loader):
        # 前向计算->计算损失函数->(从损失函数)反向传播->更新网络
        predict = MyConvNet(x)
        loss = loss_func(predict, y)
        optimizer.zero_grad()   # 清空梯度（可以不写）
        loss.backward()     # 反向传播计算梯度
        optimizer.step()    # 更新网络
        global_iter_num = epoch * len(train_loader) + step + 1  # 计算当前是从训练开始时的第几步(全迭代次数)
        if global_iter_num % log_step_interval == 0:
            # 控制台输出一下
            print("global_step:{}, loss:{:.2}".format(global_iter_num, loss.item()))
            # 添加的第一条日志：损失函数-全局迭代次数
            logger.add_scalar("train loss", loss.item() ,global_step=global_iter_num)
            # 在测试集上预测并计算正确率
            test_predict = MyConvNet(test_data_x)
            _, predict_idx = torch.max(test_predict, 1)     # 计算softmax后的最大值的索引，即预测结果
            acc = accuracy_score(test_data_y, predict_idx)
            # 添加第二条日志：正确率-全局迭代次数
            logger.add_scalar("test accuary", acc.item(), global_step=global_iter_num)
            # 添加第三条日志：这个batch下的128张图像
            img = vutils.make_grid(x, nrow=12)
            logger.add_image("train image sample", img, global_step=global_iter_num)
            # 添加第三条日志：网络中的参数分布直方图
            for name, param in MyConvNet.named_parameters():
                logger.add_histogram(name, param.data.numpy(), global_step=global_iter_num)
```

代码同一级的目录,输入指令，启动服务器
- tensorboard --logdir="./data/log"
- 浏览器中访问红框框中的url，便可得到可视化界面，点击上面的页面控件

### Visdom进行可视化

Visdom是Facebook为pytorch开发的一块可视化工具。类似于tensorboard，visdom也是通过在本地启动前端服务器来实现可视化的，而在具体操作上，visdom又类似于matplotlib.pyplot。所以使用起来很灵活。

```python
from visdom import Visdom
from sklearn.datasets import  load_iris
import torch
import numpy as np
from PIL import Image

# 绘制图像需要的数据
iris_x, iris_y = load_iris(return_X_y=True)

# 获取绘图对象，相当于plt
vis = Visdom()

# ===== 添加折线图 =======
x = torch.linspace(-6, 6, 100).view([-1, 1])
sigmoid = torch.nn.Sigmoid()
sigmoid_y = sigmoid(x)
tanh = torch.nn.Tanh()
tanh_y = tanh(x)
relu = torch.nn.ReLU()
relu_y = relu(x)
# 连接三个张量
plot_x = torch.cat([x, x, x], dim=1)
plot_y = torch.cat([sigmoid_y, tanh_y, relu_y], dim=1)
# 绘制线性图
vis.line(X=plot_x, Y=plot_y, win="line plot", env="main",
         opts={
             "dash" : np.array(["solid", "dash", "dashdot"]),
             "legend" : ["Sigmoid", "Tanh", "ReLU"]
         })
# ====== 绘制2D和3D散点图 ========
# 参数Y用来指定点的分布，win指定图像的窗口名称，env指定图像所在的环境，opts通过字典来指定一些样式
vis.scatter(iris_x[ : , 0 : 2], Y=iris_y+1, win="windows1", env="main")
vis.scatter(iris_x[ : , 0 : 3], Y=iris_y+1, win="3D scatter", env="main",
            opts={
                "markersize" : 4,   # 点的大小
                "xlabel" : "特征1",
                "ylabel" : "特征2"
            })
# ==== 添加茎叶图 ======
x = torch.linspace(-6, 6, 100).view([-1, 1])
y1 = torch.sin(x)
y2 = torch.cos(x)

# 连接张量
plot_x = torch.cat([x, x], dim=1)
plot_y = torch.cat([y1, y2], dim=1)
# 绘制茎叶图
vis.stem(X=plot_x, Y=plot_y, win="stem plot", env="main",
         opts={
             "legend" : ["sin", "cos"],
             "title" : "茎叶图"
         })

# 计算鸢尾花数据集特征向量的相关系数矩阵
iris_corr = torch.from_numpy(np.corrcoef(iris_x, rowvar=False))
# ==== 绘制热力图 =====
vis.heatmap(iris_corr, win="heatmap", env="main",
            opts={
                "rownames" : ["x1", "x2", "x3", "x4"],
                "columnnames" : ["x1", "x2", "x3", "x4"],
                "title" : "热力图"
            })

# ==== 可视化图片 =====
img_Image = Image.open("./example.jpg")
img_array = np.array(img_Image.convert("L"), dtype=np.float32)
img_tensor = torch.from_numpy(img_array)
print(img_tensor.shape)

# 这次env自定义
vis.image(img_tensor, win="one image", env="MyPlotEnv",
          opts={
              "title" : "一张图像"
          })

# ==== 可视化文本 =====
text = "hello world"
vis.text(text=text, win="text plot", env="MyPlotEnv",
         opts={
             "title" : "可视化文本"
         })

```

运行上述代码，再
- 在终端中启动服务器
  - python3 -m visdom.server
- 然后根据终端返回的URL，在谷歌浏览器中访问这个URL，就可以看到图像了
- 在Environment中输入不同的env参数可以看到我们在不同环境下绘制的图片。对于分类图集特别有用

注意：
- 如果前端服务器停掉了，那么所有的图片都会丢失，因为此时的图像的数据都是驻留在内存中，而并没有dump到本地磁盘。
- 那么如何保存当前visdom中的可视化结果，并在将来复用呢？点击Manage Views → 点击fork->save

#  pytorch内部机制

[核心开发者全面解读Pytorch内部机制](https://zhuanlan.zhihu.com/p/240938895)

斯坦福大学博士生与 Facebook 人工智能研究所研究工程师 Edward Z. Yang 是 PyTorch 开源项目的核心开发者之一。他在 5 月 14 日的 PyTorch 纽约聚会上做了一个有关 PyTorch 内部机制的演讲，本文是该演讲的长文章版本。
 
选自ezyang博客，作者：Edward Z. Yang，机器之心编译，参与：panda。
- ![](https://pic4.zhimg.com/80/v2-ad42651fcc9d372bed03232338790aef_720w.jpg)
 
大家好！今天我想谈谈 PyTorch 的内部机制。
 
这份演讲是为用过 [PyTorch](https://mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650763179%26idx%3D1%26sn%3Dc41e016ef58f4b4079bb70fbe05081f4%26chksm%3D871aabd5b06d22c38550e6bdf2c645be073537d65e4a686c0345ca70f71ab68d8ff86f5ae35d%26token%3D1612885984%26lang%3Dzh_CN)并且有心为 PyTorch 做贡献但却被 PyTorch 那庞大的 C++ 代码库劝退的人提供的。没必要说谎：PyTorch 代码库有时候确实让人难以招架。
 
本演讲的目的是为你提供一份导航图：为你讲解一个「支持自动微分的张量库」的基本概念结构，并为你提供一些能帮你在代码库中寻路的工具和技巧。我预设你之前已经写过一些 PyTorch，但却可能还没有深入理解机器学习软件库的编写方式。
- ![](https://pic2.zhimg.com/80/v2-bd7a5d2782f171ea8c51f06bae691481_720w.jpg)
 
本演讲分为两部分：
- 在第一部分中，我首先会全面介绍张量库的各种概念。我首先会谈谈你们知道且喜爱的[张量](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650763179%26idx%3D1%26sn%3Dc41e016ef58f4b4079bb70fbe05081f4%26chksm%3D871aabd5b06d22c38550e6bdf2c645be073537d65e4a686c0345ca70f71ab68d8ff86f5ae35d%26token%3D1612885984%26lang%3Dzh_CN)数据类型，并详细讨论这种数据类型究竟能提供什么，这能让我们更好地理解其内部真正的实现方式。
  - 如果你是PyTorch高级用户，可能已经熟悉其中大部分材料了。我们也会谈到「扩展点（extension points）」的三个概念：**布局**（layout）、**设备**（device）和**数据类型**（dtype），这能引导我们思考张量类的扩展的方式。在 PyTorch 纽约聚会的现场演讲中，我略过了有关**自动梯度**（autograd）的幻灯片，但我在这里会进行一些讲解。
- 第二部分会阐述用PyTorch写代码时所涉及的基本细节。如何在 autograd 代码中披荆斩棘、什么代码是真正重要的以及怎样造福他人，我还会介绍 PyTorch 为你写核（kernel）所提供的所有炫酷工具。
 
## 概念
 
### 张量
 
张量是 PyTorch 中的核心数据结构。张量是一种包含某种标量类型（比如浮点数和整型数等）的 n 维数据结构。可以将张量看作是由一些数据构成的，还有一些元数据描述了张量的大小、所包含的元素的类型（dtype）、张量所在的设备（CPU 内存？CUDA 内存？）
- ![](https://pic3.zhimg.com/80/v2-7ee4489394b8e34d19009e40623382f2_720w.jpg)
 
另外还有一个没那么熟悉的元数据：**步幅**（stride）。stride 实际上是 PyTorch 最别致的特征之一，所以值得稍微多讨论它一些。
- ![](https://pic2.zhimg.com/80/v2-d7c311ed86d5171c35d74ec26dbe0d8d_720w.jpg)

张量是一个数学概念。但要在我们的计算机中表示它，我们必须为它们定义某种物理表示方法。最常用的表示方法是在内存中相邻地放置张量的每个元素（这也是术语「contiguous（**邻接**）」的来源），即将每一行写出到内存，如上所示。在上面的案例中，我已经指定该张量包含 32 位的整型数，这样你可以看到每一个整型数都位于一个物理地址中，每个地址与相邻地址相距 4 字节。为了记住张量的实际维度，我们必须将规模大小记为额外的元数据。
 
所以这幅图与步幅有什么关系？
- ![](https://pic3.zhimg.com/80/v2-561af40cdd43ef4557c4136900f46836_720w.jpg)
 
假设我想要读取我的逻辑表示中位置张量 \[0,1\] 的元素。我该如何将这个逻辑位置转译为物理内存中的位置？步幅能让我们做到这一点：要找到一个张量中任意元素的位置，我将每个索引与该维度下各自的步幅相乘，然后将它们全部加到一起。在上图中，我用蓝色表示第一个维度，用红色表示第二个维度，以便你了解该步幅计算中的索引和步幅。进行这个求和后，我得到了 2（零索引的）；实际上，数字 3 正是位于这个邻接数组的起点以下 2 个位置。
 
（后面我还会谈到 TensorAccessor，这是一个处理索引计算的便利类（convenience class）。当你使用 TensorAccessor 时，不会再操作原始指针，这些计算过程已经为你隐藏了起来。）
 
步幅是我们为 PyTorch 用户讲解方法的基本基础。举个例子，假设我想取出一个表示以上张量的第二行的张量：
 
![](https://pic2.zhimg.com/80/v2-8bb5e51f47b14b6571642c7dd8962029_720w.jpg)
 
使用高级的索引支持，我只需写出张量 \[1, :\] 就能得到这一行。重要的是：当我这样做时，不会创建一个新张量；而是会返回一个基于底层数据的不同域段（view）的张量。这意味着，如果我编辑该视角下的这些数据，它就会反映在原始的张量中。
 
在这种情况下，了解如何做到这一点并不算太困难：3 和 4 位于邻接的内存中，我们只需要记录一个说明该（逻辑）张量的数据位于顶部以下 2 个位置的偏移量（offset）。（每个张量都记录一个偏移量，但大多数时候它为零，出现这种情况时我会在我的图表中省略它。）
 
> 演讲时的提问：如果我取张量的一个域段，我该如何释放底层张量的内存？  
> 答案：你必须制作该域段的一个副本，由此断开其与原始物理内存的连接。你能做的其它事情实际上并不多。另外，如果你很久之前写过 Java，取一个字符串的子字符串也有类似的问题，因为默认不会制作副本，所以子字符串会保留（可能非常大的字符串）。很显然，Java 7u6 将其固定了下来。
 
如果我想取第一列，还会更有意思：
- ![](https://pic4.zhimg.com/80/v2-5c729a8c611af9f9060b956c56ee66ff_720w.jpg)
 
当我们查看物理内存时，可以看到该列的元素不是相邻的：两者之间有一个元素的间隙。步幅在这里就大显神威了：我们不再将一个元素与下一个元素之间的步幅指定为 1，而是将其设定为 2，即跳两步。（顺便一提，这就是其被称为「步幅（stride）」的原因：如果我们将索引看作是在布局上行走，步幅就指定了我们每次迈步时向前多少位置。）
 
步幅表示实际上可以让你表示所有类型的张量域段；如果你想了解各种不同的可能做法，请参阅 [https://ezyang.github.io/stride-visualizer/index.html](https://link.zhihu.com/?target=https%3A//ezyang.github.io/stride-visualizer/index.html)
 
我们现在退一步看看，想想我们究竟如何实现这种功能（毕竟这是一个关于内部机制的演讲）。如果我们可以得到张量的域段，这就意味着我们必须解耦张量的概念（你所知道且喜爱的面向用户的概念）以及存储张量的数据的实际物理数据的概念（称为「存储（storage）」）：
- ![](https://pic4.zhimg.com/80/v2-2b8476f6266dff15de357d5df53981c3_720w.jpg)
 
也许会有多个张量共享同一存储。存储会定义张量的 dtype 和物理大小，同时每个张量还会记录大小、步幅和偏移量，这定义的是物理内存的逻辑解释。
 
有一点需要注意：总是会存在一个张量-存储对，即使并不真正需要存储的「简单」情况也是如此（比如，只是用 torch.zeros(2, 2) 划配一个邻接张量时）。
 
> 顺便一提，我们感兴趣的不是这种情况，而是有一个分立的存储概念的情况，只是将一个域段定义为有一个基张量支持的张量。这会更加复杂一些，但也有好处：邻接张量可以实现远远更加直接的表示，而没有存储造成的间接麻烦。这样的变化能让 PyTorch 的内部表示方式更接近 Numpy。
 
上面已经介绍了一些张量的数据布局，但还是有必要谈谈如何实现对张量操作。在最抽象的层面上，当你调用 torch.mm 时，会发生两次调度：
- ![](https://pic2.zhimg.com/80/v2-dd78d289f622b459dab2068c96a6365d_720w.jpg)
-  第一次调度基于**设备类型**和**张量布局**：比如是 CPU 张量还是 [CUDA](https://mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650763179%26idx%3D1%26sn%3Dc41e016ef58f4b4079bb70fbe05081f4%26chksm%3D871aabd5b06d22c38550e6bdf2c645be073537d65e4a686c0345ca70f71ab68d8ff86f5ae35d%26token%3D1612885984%26lang%3Dzh_CN)张量，是有步幅的张量还是稀疏的张量。这个调度是动态的：这是一个虚函数（virtual function）调用（这个虚函数调用究竟发生在何处是本演讲后半部分的主题）。
  - 这里需要做一次调度应该是合理的：CPU 矩阵乘法的实现非常不同于 CUDA 的实现。这里是动态调度的原因是这些核（kernel）可能位于不同的库（比如 libcaffe2.so 或 libcaffe2_gpu.so），这样你就别无选择：如果你想进入一个你没有直接依赖的库，你必须通过动态调度抵达那里。
- 第二次调度是在所涉**dtype**上的调度。这个调度只是一个简单的 switch 语句，针对的是核选择支持的任意 dtype。这里需要调度的原因也很合理：CPU 代码（或 CUDA 代码）是基于 float 实现乘法，这不同于用于 int 的代码。这说明你需要为每种 dtype 都使用不同的核。

如果想理解 PyTorch 中算子的调用方式，这可能就是你头脑中应有的最重要的知识。后面当我们更深入代码时还会回到这里。
- ![](https://pic2.zhimg.com/80/v2-03d0a22840b411e55e2803e61e622549_720w.jpg)
 
花点时间谈谈张量扩展。毕竟，除了密集的 CPU 浮点数张量，还有其它很多类型的张量，比如 XLA 张量、量化张量、MKL-DNN 张量；而对于一个张量库，还有一件需要思考的事情：如何兼顾这些扩展？
- ![](https://pic2.zhimg.com/80/v2-2a547c95739059be38b4dfba62637309_720w.jpg)
 
当前的用于扩展的模型提供了张量的四个扩展点。首先，有三个独立地确定张量类型的配套参数：
*   device（**设备**）：实际存储张量的物理内存，比如在 CPU、英伟达 GPU（cuda）、AMD GPU（hip）或 TPU（xla）上。设备之间各不相同的特性是有各自自己的分配器（allocator），这没法用于其它设备。   
*   layout（**布局**）：对物理内存进行逻辑解读的方式。最常用的布局是有步幅的张量（strided tensor），但稀疏张量的布局不同，其涉及到一对张量，一个用于索引，一个用于数据；MKL-DNN 张量的布局更加奇特，比如 blocked layout，仅用步幅不能表示它。  
*   dtype（**数据类型**）：张量中每个元素实际存储的数据的类型，比如可以是浮点数、整型数或量化的整型数。

如果你想为 PyTorch 张量添加一种扩展，你应该思考扩展这些参数中的哪几种。这些参数的笛卡尔积定义了你可以得到的所有可能的张量。现在，并非所有这些组合都有核（谁为 FPGA 上的稀疏量化张量用核?），但原则上这种组合可能有意义，因此我们至少应该支持表达它。
 
要为张量的功能添加「扩展」，还有最后一种方法，即围绕能实现的目标类型的 PyTorch 张量编写一个 wrapper（包装）类。这可能听起来理所当然，但有时候人们在只需要制作一个 wrapper 类时却跑去扩展那三个参数。wrapper 类的一个突出优点是开发结果可以完全不影响原来的类型（out of tree）。
 
你何时应该编写张量 wrapper，而不是扩展 PyTorch 本身？关键的指标是你是否需要将这个张量传递通过 autograd（自动梯度）反向通过过程。举个例子，这个指标告诉我们稀疏张量应该是一种真正的张量扩展，而不只是一种包含一个索引和值张量的 Python 对象：当在涉及嵌入的网络上执行优化时，我们想要嵌入生成稀疏的梯度。
- ![](https://pic2.zhimg.com/80/v2-0c99ed0b1afed1fb80e2a0b3330f0871_720w.jpg)
我们对扩展的理念也会影响张量本身的数据布局。对于我们的张量结构，我们真正想要的一件事物是固定的布局：我们不想要基本操作（这个说法很常见），比如「一个张量的大小是多少？」来请求虚调度。
 
所以当你查看一个张量的实际布局时（定义为 TensorImpl 结构），会看到所有字段的一个公共前缀——我们认为所有类似「张量」的东西都会有；还有一些字段仅真正适用于有步幅的张量，但它们也很重要，所以我们将其保留在主结构中；然后可以在每个张量的基础上完成有自定义字段的后缀。比如稀疏张量可将其索引和值存储在这个后缀中。
 
### 自动梯度（autograd）
 
我已经说明了张量，但如果 PyTorch 仅有这点把戏，这就只不过是 Numpy 的克隆罢了。PyTorch 的显著特性是其在最初发布时就已提供对张量的自动微分（现在我们还有 TorchScript 等炫酷功能，但那时候就只有这个！）
 
自动微分是做啥？这是负责运行神经网络的机制：
- ![](https://pic2.zhimg.com/80/v2-f5e23bfa1dea5367c0d3fd2693d757f9_720w.jpg)
 
……以及填充实际计算你的网络的梯度时所缺少的代码：
- ![](https://pic4.zhimg.com/80/v2-c4a5fdaa3035cfaa5c4f3c1c3a73a863_720w.jpg)
 
花点时间看看这幅图。其中有很多东西需要解读，我们来看看：
*   首先将你的目光投向红色和蓝色的变量。PyTorch 实现了反向模式自动微分，这意味着我们可以「反向」走过前向计算来有效地计算梯度。查看变量名就能看到这一点：在红色部分的底部，我们计算的是损失（loss）；然后在这个程序的蓝色部分，我们所做的第一件事是计算 grad_loss。loss 根据 next_h2 计算，这样我们可以计算出 grad_next_h2。从技术上讲，我们加了 grad_ 的变量其实并不是梯度，它们实际上左乘了一个向量的雅可比矩阵，但在 PyTorch 中，我们就称之为 grad，基本上所有人都知道这是什么意思。
*   如果代码的结构保持一样，而行为没有保持一样：来自前向的每一行都被替换为一个不同的计算，其代表了前向运算的导数。举个例子，tanh 运算被转译成了 tanh_backward 运算（这两行用图左边一条灰线连接）。前向和反向运算的输入和输出交换：如果前向运算得到 next_h2，反向运算就以 grad_next_h2 为输入。
 
autograd 的意义就在于执行这幅图所描述的计算，但却不用真正生成这个源。PyTorch autograd 并不执行源到源的变换（尽管 PyTorch JIT 确实知道如何执行符号微分（symbolic differentiation））。
- ![](https://pic4.zhimg.com/80/v2-7569e7e29e486ec433ac05561677804b_720w.jpg)
 
要做到这一点，我们需要在张量上执行运算时存储更多元数据。让我们调整一下我们对张量数据结构的图：现在不只是一个指向存储的张量，我们还有一个包装这个张量的变量，而且也存储更多信息（AutogradMeta），这是用户在自己的 PyTorch 脚本中调用 loss.backward() 执行 autograd 时所需的。

这张幻灯片的内容在不久的将来就会过时。Will Feng 在简单融合了 PyTorch 的前端端口之后，正在推动 C++ 中变量和张量的融合：[https://github.com/pytorch/pytorch/issues/13638](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/issues/13638)。
 
我们也必须更新上面关于调度的图：
- ![](https://pic3.zhimg.com/80/v2-988995aebdd2ad936375ac78ebf9e082_720w.jpg)
 
在我们调度到 CPU 或 CUDA 实现之前，还有另一个对变量的调度，其负责打开（unwrap）变量，调用底层实现（绿色），然后再重新将结果包装进变量并为反向过程记录必需的 autograd 元数据。
 
某些实现不会 unwrap；它们只是调用其它变量实现。所以你可能要在变量宇宙中花些时间。但是，一旦你 unwrap 并进入了非变量张量宇宙，你就到达终点了；你再也不用退回变量（除非从你的函数返回）。
 
在我的纽约聚会演讲中，我跳过了以下七页幻灯片。对它们的文本介绍还要等一段时间。
 
- ![](https://pic2.zhimg.com/80/v2-e6c5031d9228be6b8c5027b7541a6941_720w.jpg)
- ![](https://pic4.zhimg.com/80/v2-0d0fe340af795be2ddeaf9fc8a06b637_720w.jpg)
- ![](https://pic4.zhimg.com/80/v2-f4b195ac1878091a9d8d5d36aa527883_720w.jpg)
- ![](https://pic1.zhimg.com/80/v2-9f5021c96a11f5d1e58041af2f6aef30_720w.jpg)
- ![](https://pic4.zhimg.com/80/v2-49c891574145d47790990be81fdc627b_720w.jpg)
- ![](https://pic3.zhimg.com/80/v2-9bf72d5867b3ed5f75c1163a486bcffe_720w.jpg)
- ![](https://pic2.zhimg.com/80/v2-2ae211b8a779315611524f861c73213d_720w.jpg)
 
## 工程开发
 
说够了概念，我们来看看代码。
 
### 找到你的路径
 
PyTorch 有大量文件夹，在 CONTRIBUTING.md 文档中有对它们的非常详细的描述，但实际上你只需知晓 4 个目录：
- ![](https://pic1.zhimg.com/80/v2-3fdf3a5e08851a39b2dc1e5a37e63bf4_720w.jpg)

*   首先，torch/ 包含你最熟悉的东西：你导入和使用的实际的 Python 模块。这些东西是 Python 代码而且易于操作（只需要进行修改然后查看结果即可）。但是，如果太过深入……
*   torch/csrc/：实现了你可能称为 PyTorch 前端的 C++ 代码。用更描述性的术语讲，它实现了在 Python 和 C++ 间转换的绑定代码（binding code）；另外还有一些相当重要的 PyTorch 部分，比如 autograd 引擎和 JIT 编译器。它也包含 C++ 前端代码。
*   aten/：这是「A Tensor Library」的缩写（由 Zachary DeVito 命名），是一个实现张量运算的 C++ 库。如果你检查某些核代码所处的位置，很可能就在 ATen。ATen 本身就分为两个算子区域：「原生」算子（算子的现代的 C++ 实现）和「传统」算子（TH、THC、THNN、THCUNN），这些是遗留的 C 实现。传统的算子是其中糟糕的部分；如果可以，请勿在上面耗费太多时间。
*   c10/：这是「Caffe2」和「A"Ten"」的双关语，包含 PyTorch 的核心抽象，包括张量和存储数据结构的实际实现。
 
找代码需要看很多地方；我们应该简化目录结构，就是这样。如果你想研究算子，你应该在 aten 上花时间。
 
我们看看在实践中是如何分离这些代码的：
- ![](https://pic1.zhimg.com/80/v2-009019cd631544337cad4bb9a4bbffd8_720w.jpg)
 
当你调用一个函数时，比如 torch.add，会发生什么？如果你记得我们的有关调度的讨论，你脑中应该已有了这些基础：
*   我们必须从 Python 国度转换到 C++ 国度（Python 参数解析）。
*   我们处理变量调度（VariableType—Type，顺便一提，和编程语言类型并无特别关联，只是一个用于执行调度的小工具）。
*   我们处理设备类型/布局调度（Type）。
*   我们有实际的核，这要么是一个现代的原生函数，要么是传统的 TH 函数。
 
其中每一步都具体对应于一些代码。让我们开路穿过这片丛林。
 
![](https://pic4.zhimg.com/80/v2-5beac1027d7b80501938675f01d81e23_720w.jpg)
 
我们在 C++ 代码中的起始着陆点是一个 Python 函数的 C 实现，我们已经在 Python 那边见过它，像是 torch.\_C.VariableFunctions.add。THPVariable\_add 就是这样一个实现。
 
对于这些代码，有一点很重要：这些代码是自动生成的。如果你在 GitHub 库中搜索，你没法找到它们，因为你必须实际 build PyTorch 才能看到它们。另外一点也很重要：你不需要真正深入理解这些代码是在做什么，你应该快速浏览它，知道它的功能。
 
我在上面用蓝色标注了最重要的部分：你可以看到这里使用了一个 PythonArgParser 类来从 Python args 和 kwargs 取出 C++ 对象；然后我们调用一个 dispatch_add 函数（红色内联）；这会释放全局解释器锁，然后调用在 C++ 张量自身上的一个普通的旧方法。在其回来的路上，我们将返回的 Tensor 重新包装进 PyObject。
 
（这里幻灯片中有个错误：我应该讲解变量调度代码。我这里还没有修复。某些神奇的事发生了，于是……）
- ![](https://pic4.zhimg.com/80/v2-1ec1bba8299e71dd66d78180e122b773_720w.jpg)
 
当我们在 Tensor 类上调用 add 方法时，还没有虚调度发生。相反，我有一个内联方法，其调用了一个内联方法，其会在「Type」对象上调用一个虚方法。这个方法是真正的虚方法（这就是我说 Type 只是一个让你实现动态调度的「小工具」的原因）。
 
在这个特定案例中，这个虚调用会调度到在一个名为 TypeDefault 的类上的 add 的实现。这刚好是因为我们有一个对所有设备类型（CPU 和 CUDA）都一样的 add 的实现；如果我们刚好有不同的实现，我们可能最终会得到 CPUFloatType::add 这样的结果。正是这种虚方法的实现能让我们最终得到实际的核代码。
 
也希望这张幻灯片很快过时；Roy Li 正在研究使用另一种机制替代 Type 调度，这能让我们更好地在移动端上支持 PyTorch。
 
值得再次强调，一直到我们到达核，所有这些代码都是自动生成的。
- ![](https://pic3.zhimg.com/80/v2-2b2f01593f6a7f655b112883ca882246_720w.jpg)
 
道路蜿蜒曲折，一旦你能基本上把握方向了，我建议你直接跳到核部分。
 
### 编写核（kernel）
 
PyTorch 为有望编写核的人提供了大量有用工具。在这一节我们会了解其中一些。但首先，编写核需要什么？
- ![](https://pic4.zhimg.com/80/v2-05de33363937928c105692c56d4554c7_720w.jpg)
 
我们一般将 PyTorch 中的核看作由以下部分组成：
 
*   首先有一些我们要写的有关核的元数据，这能助力代码生成并让你获取所有与 Python 的捆绑包，同时无需写任何一行代码。
*   一旦你到达了核，你就经过了设备类型/布局调度。你首先需要写的是错误检查，以确保输入的张量有正确的维度。（错误检查真正很重要！不要吝惜它！）
*   接下来，我们一般必须分配我们将要写入输出的结果张量。
*   该到写核的时候了。现在你应该做第二次 dtype 调度，以跳至其所操作的每个 dtype 特定的核。（你不应该过早做这件事，因为那样的话你就会毫无用处地复制在任何情况下看起来都一样的代码。）
*   大多数高性能核都需要某种形式的并行化，这样就能利用多 CPU 系统了。（CUDA 核是「隐式」并行化的，因为它们的编程模型构建于大规模并行化之上。）
*   最后，你需要读取数据并执行你想做的计算！
    
 
在后面的幻灯片中，我将介绍 PyTorch 中能帮你实现这些步骤的工具。
- ![](https://pic2.zhimg.com/80/v2-3a7e22aac7c39d5956b295ad37b9481d_720w.jpg)
 
要充分利用 PyTorch 的代码生成能力，你需要为你的算子写一个模式（schema）。这个模式能提供你的函数的 mypy 风格类型，并控制是否为 Tensor 上的方法或函数生成捆绑包。你还可以告诉模式针对给定的设备-布局组合，应该调用你的算子的哪种实现。
 
有关这种格式的更多信息，请参阅：[https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/README.md)
- ![](https://pic4.zhimg.com/80/v2-7136af67a793fb520f3b3b61549952c3_720w.jpg)
 
你可能也需要为你在 derivatives.yaml 中的操作定义一个导数。
- ![](https://pic1.zhimg.com/80/v2-697b73fb84f4b295c78be40b1fd713b0_720w.jpg)
 
错误检查可以在低层 API 完成，也能通过高层 API 实现。低层 API 只是一个宏 TORCH_CHECK，其接收的是一个布尔值，然后还有任意数量的参数构成错误字符串（error string）以便得出结论看该布尔值是否为真。
 
这个宏有个很好的地方：你可以将字符串与非字符串数据混合起来；每一项都使用它们的 operator<< 实现进行格式化，PyTorch 中大多数重要的数据类型都有 operator<< 实现。
 
高层 API 能让你免于反复编写重复的错误消息。其工作方法是；你首先将每个张量包装为 TensorArg，这包含有关张量来处的信息（比如其参数名称）。然后它提供了一些预先装好的用于检查多种属性的函数；比如 checkDim() 测试的是张量的维度是否是一个固定数值。如果不是，该函数就基于 TensorArg 元数据提供一个用户友好的错误消息。
- ![](https://pic1.zhimg.com/80/v2-9777d783126b775be72ffc9b3ad96c98_720w.jpg)
 
在用 PyTorch 写算子时，有一点很重要：你往往要注册三个算子：abs\_out（其操作的是一个预分配的输出，其实现了 out= keyword 参数）、abs\_（其操作的是 inplace）、abs（这只是一个算子的普通的旧功能版本）。
 
大部分时间，abs\_out 是真正的主力，abs 和 abs\_ 只是围绕 abs_out 的薄弱 wrapper；但有时候也可为每个案例编写专门的实现。
- ![](https://pic4.zhimg.com/80/v2-5a61eb8540dbe8d65cb802860680b22f_720w.jpg)
 
要执行 dtype 调度，你应该使用 AT\_DISPATCH\_ALL_TYPES 宏。这会获取你想要进行调度操作的张量的 dtype，并还会为可从该宏调度的每个 dtype 指定一个 lambda。通常而言，这个 lambda 只是调用一个模板辅助函数。
 
这个宏不只是「执行调度」，它也会决定你的核将支持的 dtype。这样，这个宏实际上就有相当多一些版本，这能让你选取不同的 dtype 子集以生成特定结果。大多数时候，你只需要 AT\_DISPATCH\_ALL_TYPES，但也要关注你可能需要调度其它更多类型的情况。
- ![](https://pic3.zhimg.com/80/v2-ff270ff78311334460fc463e616cf752_720w.jpg)
 
在 CPU 上，你通常需要并行化你的代码。过去，这通常是通过直接在你的代码中添加 OpenMP pragma 来实现。
 
![](https://pic4.zhimg.com/80/v2-0564a9d4e55d03274a23d308b6fdccf7_720w.jpg)
 
某些时候，你必须真正访问数据。PyTorch 为此提供了相当多一些选择。
 
*   如果你只想获取某个特定位置的值，你应该使用 TensorAccessor。张量存取器就像是一个张量，但它将张量的维度和 dtype 硬编码为了模板参数。当你检索一个存取器时，比如 x.accessor();，我们会做一次运行时间测试以确保张量确实是这种格式；但那之后，每次存取都不会被检查。张量存取器能正确地处理步幅，因此你最好使用它们，而不是原始的指针访问（不幸的是，很多传统的核是这样做的）。另外还有 PackedTensorAccessor，这特别适用于通过 CUDA launch 发送存取器，这样你就能从你的 CUDA 核内部获取存取器。（一个值得一提的问题：TensorAccessor 默认是 64 位索引，这比 CUDA 中的 32 位索引要慢得多！）
*   如果你在用很常规的元素存取编写某种算子，比如逐点运算，那么使用远远更高级的抽象要好得多，比如 TensorIterator。这个辅助类能为你自动处理广播和类型提升（type promotion），相当好用。
*   要在 CPU 上获得真正的速度，你可能需要使用向量化的 CPU 指令编写你的核。我们也有用于这方面的辅助函数！Vec256 类表示一种标量向量，并提供了一些能在它们上一次性执行向量化运算的方法。然后 binary\_kernel\_vec 等辅助函数能让你轻松地运行向量化运算，然后结束那些没法用普通的旧指令很好地转换成向量指令的东西。这里的基础设施还能在不同指令集下多次编译你的核，然后在运行时间测试你的 CPU 支持什么指令，再在这些情况中使用最佳的核。
- ![](https://pic3.zhimg.com/80/v2-fc3c0a6d19882db22d0280ffbb2fd29a_720w.jpg)
 
PyTorch 中大量核都仍然是用传统的 TH 风格编写的。（顺便一提，TH 代表 TorcH。这是个很好的缩写词，但很不幸被污染了；如果你看到名称中有 TH，可认为它是传统的。）传统 TH 风格是什么意思呢？
 
*   它是以 C 风格书写的，没有（或很少）使用 C++。
*   其 refcounted 是人工的（使用了对 THTensor_free 的人工调用以降低你使用张量结束时的 refcounts）。
*   其位于 generic/ 目录，这意味着我们实际上要编译这个文件很多次，但要使用不同的 #define scalar_t

这种代码相当疯狂，而且我们讨厌回顾它，所以请不要添加它。如果你想写代码但对核编写了解不多，你能做的一件有用的事情：将某些 TH 函数移植到 ATen。
 
### 工作流程效率
 
![](https://pic1.zhimg.com/80/v2-8bbb39c785b6d86ceeb8652d19e39240_720w.jpg)
 
最后我想谈谈在 PyTorch 上的工作效率。如果 PyTorch 那庞大的 C++ 代码库是阻拦人们为 PyTorch 做贡献的第一只拦路虎，那么你的工作流程的效率就是第二只。如果你想用 Python 习惯开发 C++，那可能会很艰辛：重新编译 PyTorch 需要大量时间，你也需要大量时间才能知道你的修改是否有效。
 
如何高效工作本身可能就值得做一场演讲，但这页幻灯片总结了一些我曾见过某些人抱怨的最常见的反模式：「开发 PyTorch 很困难。」
 
*   如果你编辑一个 header，尤其是被许多源文件包含的 header（尤其当被 CUDA 文件包含时），可以预见会有很长的重新 build 时间。尽量只编辑 cpp 文件，编辑 header 要审慎！
*   我们的 CI 是一种非常好的零设置的测试修改是否有效的方法。但在获得返回信号之前你可能需要等上一两个小时。如果你在进行一种将需要大量实验的改变，那就花点时间设置一个本地开发环境。类似地，如果你在特定的 CI 配置上遇到了困难的 debug 问题，就在本地设置它。你可以将 Docker 镜像下载到本地并运行：[https://github.com/pytorch/ossci-job-dsl](https://link.zhihu.com/?target=https%3A//github.com/pytorch/ossci-job-dsl)
*   贡献指南解释了如何设置 ccache：[https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#use-ccache](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md%23use-ccache) ；强烈建议这个，因为这可以让你在编辑 header 时幸运地避免大量重新编译。当我们在不应该重新编译文件时重新编译时，这也能帮你覆盖我们的 build 系统的漏洞。
*   最后，我们会有大量 C++ 代码。如果你是在一台有 CPU 和 RAM 的强大服务器上 build，那么会有很愉快的体验。特别要说明，我不建议在笔记本电脑上执行 CUDA build。build CUDA 非常非常慢，而笔记本电脑往往性能不足，不足以快速完成。
    
 
### 参与进来！
 
![](https://pic4.zhimg.com/80/v2-0facf85450c2875e3acb821298f5411b_720w.jpg)
 
这就是我们旋风一般的 PyTorch 内核之旅了！其中省略了很多很多东西；但希望这里的描述和解释至少能帮你消化其代码库中相当大一部分。
 
接下来该做什么？你能做出怎样的贡献？我们的问题跟踪器是个开始的好地方：[https://github.com/pytorch/pytorch/issues](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/issues)。
 
从今年开始，我们一直在分类鉴别问题；标注有「triaged」的问题表示至少有一个 PyTorch 开发者研究过它并对该问题进行了初步评估。你可以使用这些标签找到我们认为哪些问题是高优先级的或查看针对特定模块（如 autograd）的问题，也能找到我们认为是小问题的问题。（警告：我们有时是错的！）
 
即使你并不想马上就开始写代码，也仍有很多其它有用的工作值得去做，比如改善文档（我很喜欢合并文档 PR，它们都很赞）、帮助我们重现来自其他用户的 bug 报告以及帮助我们讨论问题跟踪器上的 RFC。没有我们的开源贡献者，PyTorch 不会走到今天



# 结束


