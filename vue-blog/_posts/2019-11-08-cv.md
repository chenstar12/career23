---
layout: post
title:  "图像识别-Image-Recognizer"
date:   2019-11-08 16:52:00
categories: 计算机视觉
tags: 深度学习 计算机视觉 GAN 扫地机器人 自动驾驶 何恺明 CVPR 论文 sota OCR 硕士论文 ocr opencv 数字图像 滤波
excerpt: 图像风格迁移是什么原理？具体如何实施？可以迁移到文本吗？
mathjax: true
permalink: /cv
---

* content
{:toc}


# 说明

- 【2022-8-25】生成人脸：[this-person-does-not-exist.com](https://this-person-does-not-exist.com/en)
- 【2022-8-23】[国产AI作画神器火了，更懂中文，竟然还能做周边](https://mp.weixin.qq.com/s/xh6Q0Pnv9OfP8Je3lDiyZg), “一句话生成画作”这个圈子里，又一个AI工具悄然火起来了,不是你以为的Disco Diffusion、DALL·E，再或者Imagen……而是全圈子都在讲中国话的那种, 文心·一格
  - 操作界面上，Disco Diffusion开放的接口不能说很复杂，但确实有点门槛。它直接在谷歌Colab上运行，需要申请账号后使用（图片生成后保存在云盘），图像分辨率、尺寸需要手动输入，此外还有一些模型上的设置。好处是可更改的参数更多，对于高端玩家来说可操作性更强，只是比较适合专门研究AI算法的人群;相比之下，文心·一格的操作只需三个步骤：输入文字，鼠标选择风格&尺寸，点击生成。
  - 提示词，Disco Diffusion的设置还要更麻烦一些。除了描述画面的内容以外，包括画作类别和参考的艺术家风格也都得用提示词来设置，通常大伙儿会在其他文档中编辑好，再直接粘过来。相比之下文心·一格倒是没有格式要求，输入150字的句子或词组都可以
  - 性能要求上，Disco Diffusion是有GPU使用限制的，每天只能免费跑3小时。抱抱脸（HuggingFace）上部分AI文生图算法的Demo虽然操作简单些，但一旦网速不行，就容易加载不出来; 文心·一格除了使用高峰期以外，基本上都是2分钟就能生成，对使用设备也没有要求。
  - 总体来看，同样是文字生成图片AI，实际相比文心·一格的“真·一句话生成图片”，DALL·E和Disco Diffusion的生成过程都不太轻松。
- 【2022-7-11】AI工具：[解你描述的东西还可以画出来的AI，真就全靠想象](https://www.ixigua.com/7117201327523892488?wid_try=1)
  - 一、[DALL•E2](https://openai.com/dall-e-2/)：openai出品，理解并画出你描述的所有东西，如：一直粉红色大象在撒哈拉沙漠玩扑克
  - 二、[AI智能图片放大](https://bigjpg.com/zh)：还原马赛克图片，就连表情包也能高清重置
  - 三、线稿自动上色：[Style2paints](http://paintstransfer.com/), [GitHub 链接](https://github.com/lllyasviel/style2paints)
    - 用 AI 技术为黑白线稿快速自动上色。在最近推出的 2.0 版中，研究人员使用了完全无监督的生成对抗网络（GAN）训练方法大幅提高了上色的准确性。Style2paints 的作者表示，该工具在精细度、漫画风格转换等方面超越了目前其他所有工具。
- 【2021-11-22】image2text的反向：text2image，NVIDIA Demo; 用深度学习模型(GauGAN)可以将文本转换成图片, demo，[GauGAN AI Art Demo](http://gaugan.org/gaugan2)，只要输入一句简短的描述就可以生成图片了。下图是他们演示的“海浪击打岩石”的效果。
  - [The absurd beauty of hacking Nvidia's GauGAN 2 AI image machine](https://www.zdnet.com/article/the-absurd-beauty-of-hacking-nvidias-gaugan-2-ai-image-machine/)
  - ![](https://p6.toutiaoimg.com/img/tos-cn-i-qvj2lq49k0/50832d2763094d32ab1c2e974d7a625a~tplv-obj:480:272.gif)
- 汇总计算机视觉的应用案例
- 【2021-3-26】[视频大脑：视频内容理解的技术详解和应用](https://www.infoq.cn/article/vhIXoD0CAqmojPHeKP5f/)，[极客时间视频](https://time.geekbang.org/dailylesson/detail/100022917),黄君实 奇虎360 人工智能研究院资深研发科学家
  - ![](https://static001.infoq.cn/resource/image/46/6a/46ccd482ad8a09752bca0a184aaca56a.png)

# sota

【2021-6-22】[CVPR 2021大奖公布！何恺明获最佳论文提名，代码已开源](https://mp.weixin.qq.com/s/sdboE0KmvCV-Zc2R6hs0Tg)

推特上，有学者打趣说，CV论文可以分为这几类：
- 「只想混文凭」
- 「教电脑生成更多猫的照片」
- 「ImageNet上实验结果提升0.1%！」
- 「手握超酷数据集但并不打算公开」
- 「3年过去了，代码仍在赶来的路上」
- 「实验证明还是老baseline性能更牛」
- 「我们的数据集更大！」
- 「研究范围超广，无他，我们有钱」
- 「花钱多，结果好」......

何恺明和Xinlei Chen的论文Exploring Simple Siamese Representation Learning（探索简单的连体表征学习）获得了最佳论文提名。

「连体网络」（Siamese networks）已经成为最近各种无监督视觉表征学习模型中的一种常见结构。这些模型最大限度地提高了一个图像的两个增量之间的相似性，但必须符合某些条件以避免collapse的解决方案。在本文中，我们报告了令人惊讶的经验结果，即简单的连体网络即使不使用以下任何一种情况也能学习有意义的表征。(i) 负样本对，(ii) 大batch，(iii) 动量编码器。我们的实验表明，对于损失和结构来说，collapse的解决方案确实存在，但stop-gradient操作在防止collapse方面发挥了重要作用。我们提供了一个关于stop-gradient含义的假设，并进一步展示了验证该假设的概念验证实验。我们的 「SimSiam 」方法在ImageNet和下游任务中取得了有竞争力的结果。我们希望这个简单的基线能促使人们重新思考连体结构在无监督表征学习中的作用。

代码已开源 https://github.com/facebookresearch/simsiam


## 何恺明编年史

![](https://pic1.zhimg.com/v2-5e022845e2440e673f98a11f99ac6dac_1440w.jpg?source=172ae18b)
- [何恺明：从高考状元到CV领域年轻翘楚，靠“去雾算法”成为“CVPR最佳论文”首位华人得主](https://zhuanlan.zhihu.com/p/55621213)

何恺明履历
- 出生于广州的何恺明是家中独子，父母均在企业里从事管理工作，从小就接触到优良的教学环境。实际上，能从众多学子中脱颖而出，除了教学环境之外，更多的是靠自己的努力。
- 何恺明年少时就被送到少年宫学习绘画，有时一待就是大半天，这也不断使他练就出沉稳的性格。同绘画一样，他对于文化课的钻研也十分耐得住性子，学习成绩优秀而且稳定。在老师的心目中，他是一个“性格比较内向”但是“目标明确”的学生，“从小就立志上清华”。
- 高中时，全国物理竞赛一等奖被保送进清华大学机械工程及其自动化专业，不去，偏要考，结果成了2003年广东理科状元；
- 大学期间，何恺明继续着自己沉稳而优秀的表现，不仅连续3年获得清华奖学金，2007年，还未毕业的他就进入了微软亚洲研究院（MSRA）实习。
- 本科毕业后，他进入香港中文大学攻读研究生，师从AI名人汤晓鸥；
- 2009年，第一篇论文“Single ImageHaze Removalusing Dark Channel Prior”被计算机视觉领域顶级会议CVPR接收并被评为年度最佳论文，CVPR创办25年来华人学者第一次获此殊荣，也使何恺明在CV领域声名鹊起
- 2011年，博士毕业的何恺明正式加入MSRA计算机视觉和深度学习的研究工作。
- 2015年的ImageNet图像识别大赛中，何恺明和他的团队凭借152层深度残差网络ResNet-152，击败谷歌、英特尔、高通等业界团队，荣获第一。目前ResNets也已经成为计算机视觉领域的流行架构，同时也被用于机器翻译、语音合成、语音识别和AlphaGo的研发上。
- 2016年，何恺明凭借ResNets论文再次获得CVPR最佳论文奖，也是目前少有的一人两次获得CVPR最佳论文奖的学者。
- 后来，何恺明和孙剑相继离开MSRA。与孙剑的选择不同，何凯明走得还是那条学院路。他选择了去Facebook，担任其人工智能实验室研究科学家，选择了进一步走学术之路。
- 2017年3月，何恺明和同事公布了其最新的研究Mask R-CNN，提出了一个概念上简单、灵活和通用的用于目标实例分割（object instance segmentation）框架，能够有效地检测图像中的目标，同时还能为每个实例生成一个高质量的分割掩码。同年，凭借《利用焦点损失提升物体检测效果》这篇论文，他一举夺下了另一个计算机视觉顶级会议ICCV最佳论文奖。
- 2018年，何恺明在美国盐湖城召开的CVPR上，获得了PAMI青年研究者奖。几个月前，何恺明等人发表论文称，ImageNet预训练却并非必须。何恺明和其同事使用随机初始化的模型，不借助外部数据就取得了不逊于COCO 2017冠军的结果，再次引发业内关注。


【2022-1-12】[何恺明编年史](https://zhuanlan.zhihu.com/p/415353143)

别人的荣誉都是在某某大厂工作，拿过什么大奖，而何恺明的荣誉是best，best，best ...... kaiming科研嗅觉顶级，每次都能精准的踩在最关键的问题上，提出的方法简洁明了，同时又蕴含着深刻的思考，文章赏心悦目，实验详尽扎实，工作质量说明一切。

何恺明的研究兴趣大致分成这么几个阶段：
- 传统视觉时代：Haze Removal(3篇)、Image Completion(2篇)、Image Warping(3篇)、Binary Encoding(6篇)
- 深度学习时代：Neural Architecture(11篇)、Object Detection(7篇)、Semantic Segmentation(11篇)、Video Understanding(4篇)、Self-Supervised(8篇)

代表作
- 2009 CVPR best paper Single Image Haze Removal Using Dark Channel Prior
  - 利用实验观察到的暗通道先验，巧妙的构造了图像**去雾算法**。现在主流的图像去雾算法还是在Dark Channel Prior的基础上做的改进。
- 2016 CVPR best paper Deep Residual Learning for Image Recognition
  - 通过**残差连接**，可以训练非常深的卷积神经网络。不管是之前的CNN，还是最近的ViT、MLP-Mixer架构，仍然摆脱不了残差连接的影响。
- 2017 ICCV best paper **Mask R-CNN**
  - 在Faster R-CNN的基础上，增加一个实例分割分支，并且将RoI Pooling替换成了RoI Align，使得实例分割精度大幅度提升。虽然最新的实例分割算法层出不穷，但是精度上依然难以超越Mask R-CNN。
  - ![](https://pic1.zhimg.com/80/v2-55b2b7227b553659dd7deea52082bef4_720w.jpg)
- 2017 ICCV best student paper Focal Loss for Dense Object Detection
  - 构建了一个**One-Stage**检测器RetinaNet，同时提出Focal Loss来处理One-Stage的类别不均衡问题，在目标检测任务上首次One-Stage检测器的速度和精度都优于Two-Stage检测器。近些年的One-Stage检测器(如FCOS、ATSS)，仍然以RetinaNet为基础进行改进。
  - ![](https://pic3.zhimg.com/80/v2-7628af32f42bc07197bdc27bc02f9d52_720w.jpg)
- 2020 CVPR Best Paper Nominee Momentum Contrast for Unsupervised Visual Representation Learning
  - 19年末，NLP领域的Transformer进一步应用于Unsupervised representation learning，产生后来影响深远的BERT和GPT系列模型，反观CV领域，ImageNet刷到饱和，似乎遇到了怎么也跨不过的屏障。就在CV领域停滞不前的时候，Kaiming He带着**MoCo**横空出世，横扫了包括PASCAL VOC和COCO在内的7大数据集，至此，CV拉开了Self-Supervised研究新篇章。

# 数字图像处理

## 硕士论文

【2022-01-24】
- 硕士毕业论文：[图像分割配准技术在小鼠舌头三维重建中的应用](https://www.docin.com/p-1966389845.html)
- 计算机工程学报：[基于形态学的小鼠舌头切片图像分割与实现](https://jz.docin.com/p-533278994.html)

## 图像处理内容

包含
- · 基本概念：亮度、对比度、分辨率、饱和度、尖锐化等基础概念
- · 图像灰度变换：线性、分段线性、对数、反对数、幂律(伽马)变换等
- · 图像滤波：线性滤波和非线性滤波、空间滤波和频率域滤波，均值滤波、中值滤波、高斯滤波、逆滤波、维纳滤波等各种图像的基本操作

高级的图像操作:
- · 文本图像的倾斜矫正方法：霍夫变换、透视变换等
- · 图像边缘检测：canny算子、sobel算子、Laplace算子、Scharr滤波器等


## 图像分割

- 【2020-7-17】图像分割（Image Segmentation）是计算机视觉领域中的一项重要基础技术，是图像理解中的重要一环。图像分割是将数字图像细分为多个图像子区域的过程，通过简化或改变图像的表示形式，让图像能够更加容易被理解。
   - 图像分割技术自 60 年代数字图像处理诞生开始便有了研究，随着近年来深度学习研究的逐步深入，图像分割技术也随之有了巨大的发展。
   - 早期的图像分割算法不能很好地分割一些具有抽象语义的目标，比如文字、动物、行人、车辆。这是因为早期的图像分割算法基于简单的像素值或一些低层的特征，如边缘、纹理等，人工设计的一些描述很难准确描述这些语义，这一经典问题被称之为“语义鸿沟”。
   - 第三代图像分割很好地避免了人工设计特征带来的“语义鸿沟”，从最初只能基于像素值以及低层特征进行分割，到现在能够完成一些根据高层语义的分割需求。
   - 参考：[基于深度学习的图像分割在高德的实践](https://yqh.aliyun.com/detail/15920?utm_content=g_1000154176)
   - ![](https://p1-tt-ipv6.byteimg.com/img/pgc-image/9811c9fff31a4fe282dbce591f7642b8~tplv-obj:745:306.image)


## 素描风格化

【2022-1-25】[5个方便好用的Python自动化脚本](https://www.toutiao.com/i7056585992664269344)

自动生成素描草图
- 这个脚本可以把彩色图片转化为铅笔素描草图，对人像、景色都有很好的效果。而且只需几行代码就可以一键生成，适合批量操作，非常的快捷。

第三方库：
- Opencv - 计算机视觉工具，可以实现多元化的图像视频处理，有Python接口

安装

```shell
# 安装opencv的Python库
pip install opencv-python
```

示例

```python
""" Photo Sketching Using Python """
import cv2

img = cv2.imread("elon.jpg")
## Image to Gray Image
gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
## Gray Image to Inverted Gray Image
inverted_gray_image = 255-gray_image
## Blurring The Inverted Gray Image
blurred_inverted_gray_image = cv2.GaussianBlur(inverted_gray_image, (19,19),0)
## Inverting the blurred image
inverted_blurred_image = 255-blurred_inverted_gray_image
### Preparing Photo sketching
sketck = cv2.divide(gray_image, inverted_blurred_image,scale= 256.0)
cv2.imshow("Original Image",img)
cv2.imshow("Pencil Sketch", sketck)
cv2.waitKey(0)
```

素描草图：马斯克
- ![](https://p26.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/8c5a6fdc6274477ca9e3b85990f6942f?from=pc)

## 图片小工具

【2022-10-19】[神奇海螺试验场](https://lab.magiconch.com/)出品
- [电子包浆](https://magiconch.com/patina/)，图片赛博做旧
- [梗图生成器](https://x.magiconch.com/)
- [颜色图片识别](https://magiconch.com/nsfw/)
- 切图：[九宫格](https://v.magiconch.com/sns-image)
- [你画我猜](https://draw.magiconch.com/)在线游戏


## 图像动态化

[Live2D](https://www.live2d.com/en/download/cubism/)

Live2D是一种应用于电子游戏的绘图渲染技术，通过一系列的连续图像和人物建模来生成一种类似二维图像的三维模型。对于以动画风格为主的冒险游戏来说非常有用。该技术由日本Guyzware公司开发，Live2D的前身为TORA系统，衍生技术是OIU系统。
- 知乎：[如何看待live2D这项技术？](https://www.zhihu.com/question/28130936)

<video width="620" height="440" controls="controls" autoplay="autoplay">
  <source src="https://vdn.vzuu.com/SD/fc42fe58-2322-11eb-a20b-9a794694b530.mp4" type="video/mp4" />
</video>


## 图像3D化

- [2017-9-21]自拍照三维重建[3D Face Reconstruction from a Single Image](http://www.cs.nott.ac.uk/~psxasj/3dme/index.php)
- ![demo](https://cdn.vox-cdn.com/thumbor/fXbE0rbXW6WlcmtB1cKBiTsV1b0=/0x0:482x334/1820x1213/filters:focal(203x129:279x205):no_upscale()/cdn.vox-cdn.com/uploads/chorus_image/image/56734861/3d_mark_take_2.0.gif)
- 【2020-7-23】2D照片转3D的效果，代码：[3d-photo-inpainting](https://github.com/vt-vl-lab/3d-photo-inpainting)
- ![](https://p1-tt-ipv6.byteimg.com/img/pgc-image/54a7f500dc92415f91e0766e2f74c45a~tplv-obj:340:424.image?from=post)

- 【2020-11-18】端到端面部表情合成 Speech-Driven Animation [Github代码](https://github.com/DinoMan/speech-driven-animation)
   - ![](https://github.com/DinoMan/speech-driven-animation/raw/master/example.gif)
- 【2021-3-10】面部表情迁移：吴京+甄子丹 [微博示例](https://video.weibo.com/show?fid=1034:4609199536013325)
- 【2020-12-29】[单张图片三维重建](https://blog.csdn.net/zouxy09/article/details/8083553),Andrew Ng介绍他的两个学生用单幅图像去重构这个场景的三维模型。
   - [斯坦福大学](http://ai.stanford.edu/~asaxena/reconstruction3d/)
      - ![](http://ai.stanford.edu/~asaxena/reconstruction3d/Results/mountain_mesh_small.jpg)
   - [康奈尔大学](http://www.cs.cornell.edu/~asaxena/learningdepth/)

## opencv

【2022-10-7】[opencv-python快速入门篇](https://zhuanlan.zhihu.com/p/44255577)

### opencv简介

opencv 是用于快速处理图像处理、计算机视觉问题的工具，支持多种语言进行开发如c++、python、java等

### Python opencv安装

环境：
- 1、 python3
- 2、 numpy
- 3、 opencv-python

```shell
# 安装numpy
pip install numpy
# 安装opencv-python
pip install opencv-python
```

测试：
- 执行 import cv2

### 图像读取

（1）imread函数：读取数字图像

cv2.imread(path_of_image, intflag)
- 参数一： 需要读入图像的完整路径
- 参数二： 标志以什么形式读入图像，可以选择一下方式：
  - · cv2.IMREAD_COLOR： 加载彩色图像。任何图像的透明度都将被忽略。它是默认标志
  - · cv2.IMREAD_GRAYSCALE：以灰度模式加载图像
  - · cv2.IMREAD_UNCHANGED：保留读取图片原有的颜色通道
    - · 1 ：等同于cv2.IMREAD_COLOR
    - · 0 ：等同于cv2.IMREAD_GRAYSCALE
    - · -1 ：等同于cv2.IMREAD_UNCHANGED

### 图像显示

（2）imshow 函数
- imshow函数作用是在窗口中显示图像，窗口自动适合于图像大小，我们也可以通过imutils模块调整显示图像的窗口的大小。
- 函数官方定义：cv2.imshow(windows_name, image)
  - 参数一： 窗口名称(字符串)
  - 参数二： 图像对象，类型是numpy中的ndarray类型，注：这里可以通过imutils模块改变图像显示大小

### 图像写入

（3）imwrite 函数
- imwrite函数检图像保存到本地，官方定义：cv2.imwrite(image_filename, image)
  - 参数一： 保存的图像名称(字符串)
  - 参数二： 图像对象，类型是numpy中的ndarray类型


### 颜色空间

图像颜色主要是由于图像受到外界光照影响随之产生的不同颜色信息，同一个背景物的图像在不同光源照射下产生的不同颜色效果的图像，因此在做图像特征提取和识别过程时，要的是图像的**梯度信息**，也就是图像的本质内容，而**颜色信息**会对梯度信息提取造成一定的干扰，因此会在做图像特征提取和识别前将图像转化为**灰度图**，这样同时也降低了处理的数据量并且增强了处理效果。

图像色彩空间变换函数cv2.cvtColor

函数定义：cv2.cvtColor(input_image, flag)
- 参数一： input_image表示将要变换色彩的图像ndarray对象
- 参数二： 表示图像色彩空间变换的类型，以下介绍常用的两种：
  - · cv2.COLOR_BGR2GRAY： 表示将图像从BGR空间转化成灰度图，最常用
  - · cv2.COLOR_BGR2HSV： 表示将图像从RGB空间转换到HSV空间

如果想查看参数flag的全部类型，请执行以下程序便可查阅，总共有274种空间转换类型：

```python
import cv2
flags = [i for i in dir(cv2) if i.startswith('COLOR_')]
print(flags)
```

### 自定义图像

绘图简单图像
- 对于一个长宽分别为w、h的RGB彩色图像来说，每个像素值是由(B、G、R)的一个tuple组成，opencv-python 中每个像素三个值的顺序是B、G、R，而对于灰度图像来说，每个像素对应的便只是一个整数，如果要把像素缩放到0、1，则灰度图像就是二值图像，0便是黑色，1便是白色

```python
import cv2
#这里图像采用的仍旧是上面那个卡通美女啦
rgb_img = cv2.imread('E:/peking_rw/ocr_project/base_prehandle/img/cartoon.jpg')
print(rgb_img.shape)     #(1200, 1600, 3)
print(rgb_img[0, 0])     #[137 124  38]
print(rgb_img[0, 0, 0])  #137

gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)
print(gray_img.shape)    #(1200, 1600)
print(gray_img[0, 0])    #100
```

彩色图像的高度height = 1200， 宽度w=1600且通道数为3， 像素(0， 0)的值是(137 124 38)，即R=137, G=124, B=38， 对于灰度图像来说便只是单通道的了

因此(0, 0, 0)便是代表一个黑色像素，(255, 255, 255)便是代表一个白色像素。这么想，B=0, G=0, R=0相当于关闭了颜色通道也就相当于无光照进入，所以图像整个是黑的，而(255, 255, 255)即B=255, G=255, R=255， 相当于打开了B、G、R所有通道光线全部进入，因此便是白色。

#### 图像绘制方法

各种绘制方法
- 直线cv2.line、长方形cv2.rectangle、圆cv2.circle、椭圆cv2.ellipse、多边形cv2.polylines等集合图像绘制函数

公共参数：
- · img： 表示需要进行绘制的图像对象ndarray
- · color： 表示绘制几何图形的颜色，采用BGR即上述说的(B、G、R)
- · thickness： 表示绘制几何图形中线的粗细，默认为1，对于圆、椭圆等封闭图像取-1时是填充图形内部
- · lineType ： 表示绘制几何图形线的类型，默认8-connected线是光滑的，当取cv2.LINE_AA时线呈现锯齿状

##### (1) cv2.line函数

直线绘制函数， 函数官方定义为：
- cv2.line(image, starting, ending, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，参数starting、ending分别表示线的起点像素坐标、终点像素坐标

##### (2) cv2.rectangle函数

长方形绘制函数，函数官方定义：
- cv2.rectangle(image, top-left, bottom-right, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，参数top-left、bottom-right分别表示长方形的左上角像素坐标、右下角像素坐标


##### (3) cv2.circle函数
圆形绘制函数，官方定义函数为：
- cv2.circle(image, center, radius, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，参数center、radius分别表示圆的圆心像素坐标、圆的半径长度，圆绘制函数中当参数thickness = -1 时绘制的是实心圆，当thickness >= 0 时绘制的是空心圆


##### (4) cv2.ellipse函数

椭圆绘制函数，官方定义为：
- cv2.circle(image, center, (major-axis-length, minor-axis-length), angle, startAngle, endAngle, color, thickness, lineType)
- 椭圆的参数较多，首先参数image、color、thickness、lineType分别是上述公共定义，椭圆绘制函数中当参数thickness = -1 时绘制的是实心椭圆，当thickness >= 0 时绘制的是空心椭圆，其他参数如下
  - · center： 表示椭圆中心像素坐标
  - · major-axis-length： 表示椭圆的长轴长度
  - · minor-axis-length： 表示椭圆的短轴长度
  - · angle： 表示椭圆在逆时针方向旋转的角度
  - · startAngle： 表示椭圆从主轴向顺时针方向测量的椭圆弧的起始角度
  - · endAngle： 表示椭圆从主轴向顺时针方向测量的椭圆弧的终止时角度


##### (5) cv2.polylines函数

多边形绘制函数，官方定义函数为：
- cv2.polylines(image, \[point-set], flag, color, thickness, lineType)
- 参数image、color、thickness、lineType分别是上述公共定义，其他参数如下：
  - · \[point-set]： 表示多边形点的集合，如果多边形有m个点，则便是一个m*1*2的数组，表示共m个点
  - · flag： 当flag = True 时，则多边形是封闭的，当flag = False 时，则多边形只是从第一个到最后一个点连线组成的图像，没有封闭


#### 图像绘制示例

```python
import cv2
import numpy as np

img = np.ones((512,512,3), np.uint8)
img = 255*img
img = cv2.line(img, (100,100), (400,400),(255, 0, 0), 5)
img = cv2.rectangle(img,(200, 20),(400,120),(0,255,0),3)
img = cv2.circle(img,(100,400), 50, (0,0,255), 2)
img = cv2.circle(img,(250,400), 50, (0,0,255), 0)
img = cv2.ellipse(img,(256,256),(100,50),0,0,180,(0, 255, 255), -1)
pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)
img = cv2.polylines(img,[pts],True,(0, 0, 0), 2)

cv2.imshow('img', img)
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()
```

![](https://pic3.zhimg.com/80/v2-37c3e0653291eafc7d16ce071fdf9db6_1440w.webp)

### 像素操作

#### (1) 对图像取反

```python
reverse_img = 255 - gray_img
```

#### (2) 对图像像素线性变换

```python
for i in range(gray_img.shape[0]):
    for j in range(gray_img.shape[1]):
        random_img[i, j] = gray_img[i, j]*1.2
```

![](https://pic4.zhimg.com/80/v2-8fca4ea068a45033056e89236ae1644b_1440w.webp)

完整代码

```python
import cv2
import imutils
import numpy as np

rgb_img = cv2.imread('E:/peking_rw/ocr_project/base_prehandle/img/cartoon.jpg')
gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)
reverse_img = 255 - gray_img

random_img = np.zeros((gray_img.shape[0], gray_img.shape[1]), dtype=np.uint8)
for i in range(gray_img.shape[0]):
    for j in range(gray_img.shape[1]):
        random_img[i, j] = gray_img[i, j]*1.2
cv2.imshow('reverse_img', imutils.resize(reverse_img, 800))
cv2.imshow('random_img', imutils.resize(random_img, 800))
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()
```

### 窗口销毁

（4）窗口销毁函数
- 当使用imshow函数展示图像时，最后需要在程序中对图像展示窗口进行销毁，否则程序将无法正常终止
- 常用的销毁窗口的函数有下面两个：
  - ① cv2.destroyWindow(windows_name) # 销毁单个特定窗口
    - 参数： 将要销毁的窗口的名字
  - ② cv2.destroyAllWindows() # 销毁全部窗口，无参数

何时销毁窗口？肯定不能图片窗口一出现就将窗口销毁，这样便没法观看窗口，试想有两种方式：
- ① 让窗口停留一段时间然后自动销毁；
- ② 接收指定的命令，如接收指定的键盘敲击然后结束我们想要结束的窗口

以上两种情况都将使用cv2.waitKey函数， 首先产看函数定义：cv2.waitKey(time_of_milliseconds)
- 唯一参数 time_of_milliseconds是整数，可正可负也可是零，含义和操作也不同，分别对应上面说的两种情况
- ① time_of_milliseconds > 0 ：此时time_of_milliseconds表示时间，单位是毫秒，含义表示等待 time_of_milliseconds毫秒后图像将自动销毁
- ② time_of_milliseconds <= 0 ： 此时图像窗口将等待一个键盘敲击，接收到指定的键盘敲击便会进行窗口销毁。我们可以自定义等待敲击的键盘，通过下面的例子进行更好的解释

### opencv 汇总示例

```python
import numpy as np
import cv2

gray_img = cv2.imread('img/cartoon.jpg', 0)  #加载灰度图像
rgb_img = cv2.imread('img/cartoon.jpg', 1)   #加载RGB彩色图像

cv2.imshow('origin image', rgb_img)   #显示原图
cv2.imshow('origin image', imutils.resize(rgb_img, 800))  #利用imutils模块调整显示图像大小
cv2.imshow('gray image', imutils.resize(gray_img, 800))
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()

cv2.imwrite('rgb_img.jpg', rgb_img)   #将图像保存成jpg文件
cv2.imwrite('gray_img.png', gray_img) #将图像保存成png文件

#表示等待10秒后，将销毁所有图像
if cv2.waitKey(10000):
    cv2.destroyAllWindows()
#表示等待10秒，将销毁窗口名称为'origin image'的图像窗口
if cv2.waitKey(10000):
    cv2.destroyWindow('origin image')
#当指定waitKey(0) == 27时，当敲击键盘 Esc 时便销毁所有窗口
if cv2.waitKey(0) == 27:
    cv2.destroyAllWindows()
#当接收到键盘敲击A时，便销毁名称为'origin image'的图像窗口
if cv2.waitKey(-1) == ord('A'):
    cv2.destroyWindow('origin image')
```

### imutils 工具包

imutils 是在OPenCV基础上的一个封装，达到更为简结的调用OPenCV接口的目的，它可以轻松的实现图像的平移，旋转，缩放，骨架化等一系列的操作。

安装方法

```shell
# 在安装前应确认已安装numpy,scipy,matplotlib和opencv
pip install imutils
# pip install NumPy SciPy opencv-python matplotlib imutils
```

图像操作：[参考](https://walkonnet.com/archives/364235)
- 图像平移
  - 相对于原来的cv，使用imutiles可以直接指定平移的像素，不用构造平移矩阵
  - OpenCV中也提供了图像平移的实现，要先计算平移矩阵，然后利用仿射变换实现平移，在imutils中可直接进行图像的平移。
  - translated = imutils.translate(img,x,y)
- 缩放函数：imutils.resize(img,width=100)
- 图像旋转
  - 逆时针旋转 rotated = imutils.rotate(image, 90)
  - 顺时针旋转 rotated_round = imutils.rotate_bound(image, 90)
- 骨架提取（边缘提取）
  - 骨架提取（边缘提取），是指对图片中的物体进行拓扑骨架(topological skeleton)构建的过程。
  - imutils提供的方法是skeletonize()
- 转RGB
  - img = cv.imread("lion.jpeg") 
  - plt.figure() 
  - plt.imshow(imutils.opencv2matplotlib(img))


### 完整示例


```python
import cv2
#pip install imutils
import imutils
import numpy as np

rgb_img = cv2.imread('/Users/wqw/Desktop/二十面体.png')
# 颜色空间转换：rgb → gray
gray_img = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)
# -------------
# 总共有274种空间转换类型：
# flags = [i for i in dir(cv2) if i.startswith('COLOR_')]
# print(flags)
# -----------
cv2.imshow('origin image', imutils.resize(rgb_img, 800))
cv2.imshow('gray image', imutils.resize(gray_img, 800))
cv2.imwrite('rgb_img.jpg', rgb_img)
cv2.imwrite('gray_img.png', gray_img)

# 等待一定时间自动销毁图像窗口
#if cv2.waitKey(10000):
#    cv2.destroyAllWindows()
#if cv2.waitKey(10000):
#    cv2.destroyWindow('origin image')
# 接收特定键盘销毁图像窗口
#if cv2.waitKey(-1) == ord('A'):
#    cv2.destroyWindow('origin image')
if cv2.waitKey(0) == 27: # 按 Esc键销毁所有窗口
    cv2.destroyAllWindows()
```

# OCR

- `光学字符识别`(`OCR`,Optical Character Recognition)是指对文本资料进行扫描，然后对图像文件进行分析处理，获取文字及版面信息的过程。OCR技术非常专业，一般多是印刷、打印行业的从业人员使用，可以快速的将纸质资料转换为电子资料。

## OCR工具


### 简易工具

- 【2021-7-26】免费在线OCR工具 [ocrmaker](http://ocrmaker.com/)
- [UU Tool](https://uutool.cn/ocr/)：截图黏贴图片到网站，提取文本；text转语音
- [城华OCR](https://zhcn.109876543210.com/)，将图片转成各种文档格式，限制次数
- [白描](https://web.baimiaoapp.com/image-to-excel)：提取图片中的文字、数学公式等

### 对比总结

【2022-1-25】kaggle笔记：各类OCR方法对比：[Keras-OCR vs EasyOCR vs PYTESSERACT](https://www.kaggle.com/odins0n/keras-ocr-vs-easyocr-vs-pytesseract)
- ![](https://www.kaggleusercontent.com/kf/72864633/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..bibMciKL4OvFX6946nkcFw.H2h7vNPLD0EzD2z6onWrw4R9VV561rtI-O7dAgl4zRkrjH216E96Cg8_ZO6-4Xny6XZL45qeH7xqBdHs7DWKpxJwl6PSES-c3wCDkH1pZifDsjEiIhboIFocwMjIEWDWNFlTY-gafig2CIc9OMmr8Kj2HyhJ_Xmg88Lbsa25dpCF2XkWG6DDLww1eL9wmXE66SzF7sM1_rsUxLvmAplprAQVNPOo2dVSKaGtD5Q1FOD8NvkeRPVeA-MiFGHe8bCtu0paeoX7aPC1z6WzunEsbpGjAeHOWrHXDtYZMPde_Qc77FVe2Qc91b6W_aAYgFoWuehxHKhOgp-jtcSA8cr_UocTj3chqBQgJKkwFodQdZInVeknz7L1HA9IGJgpWEy8DPZcNjhNuXgoWqpjqJLslljIJa-8N3Dy3qqu5p8Ku54YnzDSak2rMgdn_ThhC5AtDM3_7aB_s6vI4LoeVFxYTJ4JLVyw3v_YqIOe1BG7qD-QN2bqZixhJvtajOYzllcLP21NqMesuo7dHa-favmNVYo6o9zirwLvyYFrW4z0BpdBGkf_nQ_6n452u6GMiaRwmpNgNpD3zVv1BRCNbvMrJyzm5Mb7iqmedml2Yi6NFMxEgyOvb6rclteSyWU8_CMhP0bl3KGxEgeqNSD9Z02teSGWd9Gl8Nb6F9SByo90TtEZPJy7kIpa9Y9VPHwV7JAD.PtUtOX_gh2gJUMvxM9Wyvw/__results___files/__results___14_1.png)

CONCLUSIONS 对比结论
* **Keras-OCR** is image specific OCR tool. If text is inside the image and their fonts and colors are unorganized, Keras-ocr consumes time if used on CPU 
* **EasyOCR** is lightweight model which is giving a good performance for receipt or PDF conversion. It is giving more accurate results with organized texts like pdf files, receipts, bills. EasyOCR also performs well on noisy images 适合发票、pdf格式、噪声图片
* **Pytesseract** is performing well for **high-resolution** images. Certain morphological operations such as dilation, erosion, OTSU binarization can help increase pytesseract performance. It also provides better results on handwritten text as compared to EasyOCR 适合高分辨率图、手写字体
* All these results can be further improved by performing specific image operations.



### Tesseract

- Tesseract 的OCR引擎最先由HP实验室于1985年开始研发，至1995年时已经成为OCR业内最准确的三款识别引擎之一。
- Tesseract 目前已作为开源项目发布在Google Project，其最新版本3.0已经支持中文OCR，并提供了一个命令行工具。

安装：
- pip install pytesseract

调用代码

```python
import cv2                        # OpenCV: Computer vision and image manipulation package
import pytesseract                # python Tesseract: OCR in python
import matplotlib.pyplot as plt   # plotting
import numpy as np                # Numpy for arrays
from PIL import Image             # Pillow: helps us read remote images
import requests                   # Requests: to fetch remote URLs
from io import BytesIO            # Helps read remote images

def get_image(url):
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  return img

img = get_image('https://github.com/jalammar/jalammar.github.io/raw/master/notebooks/cv/label.png')
# OCR结果
print(pytesseract.image_to_string(img))
```

### EasyOCR

- 【2020-8-7】[一个超好用的开源OCR](https://www.toutiao.com/i6858234401206043140/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1596809559&app=news_article&utm_source=mobile_qq&utm_medium=toutiao_android&use_new_style=1&req_id=20200807221239010147083076216022E3&group_id=6858234401206043140)：[EasyOCR](https://github.com/JaidedAI/EasyOCR)，目前能够支持58种语言，其中有中文(简体和繁体)、日语、泰语、韩语等
   - EasyOCR的模型主要分为两个，基于CRAFT的文字检测模型和基于ResNet+LSTM+CTC的识别模型
   - ![](http://p6-tt.byteimg.com/large/pgc-image/2402e44dff954e4985f6762de5b07ce6?from=pc)
   - 第三方基于easyOCR提供了几个demo地址，大家可以试试自己的数据看看效果：
      - [Demo1](https://colab.fan/easyocr)
      - [Demo2](https://hub.docker.com/r/challisa/easyocr)
      - [Demo3](https://easyocrgpu-wook-2.endpoint.ainize.ai/)
      - ![](http://p3-tt.byteimg.com/large/pgc-image/a56400ef928d419c8ef29c64abede5da?from=pc)

### Pix2Text

【2022-9-21】[Pix2Text: 替代 Mathpix 的免费 Python 开源工具](https://www.toutiao.com/article/7145465980930556450)
- [Pix2Text](https://github.com/breezedeus/pix2text) 期望成为 Mathpix 的免费开源 Python 替代工具，完成与 Mathpix 类似的功能。当前 Pix2Text 可识别截屏图片中的数学公式、英文、或者中文文字。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a81b8f2c95794d0596a1d7b803df2a34~noop.image?_iz=58558&from=article.pc_detail&x-expires=1664345981&x-signature=1blBfmn0jnAV0%2FOB5VuIKt1Jd3Q%3D)

Pix2Text首先利用图片分类模型来判断图片类型，然后基于不同的图片类型，把图片交由不同的识别系统进行文字识别：
- 如果图片类型为 **formula** ，表示图片为数学公式，此时调用 LaTeX-OCR 识别图片中的数学公式，返回其Latex表示；
- 如果图片类型为 **english**，表示图片中包含的是英文文字，此时使用 CnOCR (https://github.com/breezedeus/cnocr) 中的英文模型识别其中的英文文字；英文模型对于纯英文的文字截图，识别效果比通用模型好；
- 如果图片类型为 **general**，表示图片中包含的是常见文字，此时使用 CnOCR 中的通用模型识别其中的中或英文文字。


```python
#pip install pix2text -i https://pypi.doubanio.com/simple
from pix2text import Pix2Text

img_fp = './docs/examples/formula.jpg'
p2t = Pix2Text()
out_text = p2t(img_fp)  # 也可以使用 `p2t.recognize(img_fp)` 获得相同的结果
print(out_text)
```


### 中文OCR比赛第一

【2022-1-25】[第一次比赛，拿了世界人工智能大赛 Top1 ！](https://blog.csdn.net/Datawhale/article/details/122613233)，“世界人工智能创新大赛”——手写体 OCR 识别竞赛（任务一），取得了Top1的成绩
- [赛题地址](http://ailab.aiwin.org.cn/competitions/65)
- 背景：银行日常业务中涉及到各类凭证的识别录入，例如身份证录入、支票录入、对账单录入等。以往的录入方式主要是以人工录入为主，效率较低，人力成本较高。近几年来，OCR相关技术以其自动执行、人为干预较少等特点正逐步替代传统的人工录入方式。但OCR技术在实际应用中也存在一些问题，在各类凭证字段的识别中，手写体由于其字体差异性大、字数不固定、语义关联性较低、凭证背景干扰等原因，导致OCR识别率准确率不高，需要大量人工校正，对日常的银行录入业务造成了一定的影响
- 数据集：原始手写体图像共分为三类，分别涉及银行名称、年月日、金额三大类，分别示意如下：
  - ![](https://img-blog.csdnimg.cn/img_convert/4cfda26453767dec3b2d436540d3c6b8.png)
- 相应图片切片中可能混杂有一定量的干扰信息
  - ![](https://img-blog.csdnimg.cn/img_convert/cd77146fdad3c8b41f455b2992a6b784.png)

OCR比赛最常用的模型是 CRNN + CTC，选择代码：Attention_ocr.pytorch-master.zip

模型改进：crnn的卷积部分类似VGG，我对模型的改进主要有一下几个方面：
- 1、加入激活函数Swish。
- 2、加入BatchNorm。
- 3、加入SE注意力机制。
- 4、适当加深模型。

```python
self.cnn = nn.Sequential(
   nn.Conv2d(nc, 64, 3, 1, 1), Swish(), nn.BatchNorm2d(64),
   nn.MaxPool2d(2, 2),  # 64x16x50
   nn.Conv2d(64, 128, 3, 1, 1), Swish(), nn.BatchNorm2d(128),
   nn.MaxPool2d(2, 2),  # 128x8x25
   nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), Swish(),  # 256x8x25
   nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), Swish(),  # 256x8x25
   SELayer(256, 16),
   nn.MaxPool2d((2, 2), (2, 1), (0, 1)),  # 256x4x25
   nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), Swish(),  # 512x4x25
   nn.Conv2d(512, 512, 1), nn.BatchNorm2d(512), Swish(),
   nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), Swish(),  # 512x4x25
   SELayer(512, 16),
   nn.MaxPool2d((2, 2), (2, 1), (0, 1)),  # 512x2x25
   nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), Swish()
)  # 512x1x25
# SE和Swish
class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=True),
            nn.LeakyReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=True),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
```

## 验证码识别

- 验证码是一种区分用户是计算机还是人的公共全自动程序。可以防止：恶意破解密码、刷票、论坛灌水，有效防止某个黑客对某一个特定注册用户用特定程序暴力破解方式进行不断的登陆尝试，实际上用验证码是现在很多网站通行的方式。由于验证码可以由计算机生成并评判，但是必须只有人类才能解答，所以回答出问题的用户就可以被认为是人类。
- 目前的验证码通常的种类及特点如下：
   - （1）最基础的英文验证码：纯粹的英文与数字组合，白色背景，这是最容易实现OCR识别的验证码。
   - （2）字体变形的英文验证码：可以通过简单的机器学习实现对英文与数字的识别，准确率较高。
   - （3）加上扰乱背景线条的验证码：可以通过程序去除干扰线，准确率较高。
   - （4）中文验证码：中文由于字体多样，形状多变，数量组合众多，实现起来难度较大，准确率不高。
   - （5）中文字体变形验证码：准确率更低。
   - （6）中英文混合验证码：非常考验OCR程序的判断能力，基本上识别起来非常有难度。
   - （7）提问式验证码：这是需要OCR结合人工智能才能实现，目前基本上无法识别。
   - （8）GIF动态图验证码：由于GIF图片是动态图，无法定位哪一帧是验证码，所以难度很大。
   - （9）划动式验证码：虽然程序可以模拟人的操作，但是具体拖动到哪个位置很难处理。
   - （10）视频验证码：目前OCR识别还未实现。
   - （11）手机验证码：手机验证码实现自动化是很容易的，但是手机号码不那么容易获得。
   - （12）印象验证码：目前无解。

![](https://pic1.zhimg.com/80/v2-2b9748a5ca5498ba1955eec9a5b79db4_720w.jpg)

- 附录：
   - [利用Tesseract-OCR实现验证码识别](https://zhuanlan.zhihu.com/p/34530032)

- 「Happy Captcha」，一款易于使用的 Java 验证码软件包，旨在花最短的时间，最少的代码量，实现 Web 站点的验证码功能。
   - Captcha缩写含义：Completely Automated Public Turing test to tell Computers and Humans Apart
- 效果图
   - ![](https://pic3.zhimg.com/v2-971f594800cdd101950f916f92cb7b1e_b.webp)

## GAN方法

- 【2018-12-14】[基于GAN的验证码识别工具，0.5秒宣告验证码死刑！](https://baijiahao.baidu.com/s?id=1619803729564462538)
   - 中英两国研究人员联合开发了一套基于GAN的验证码AI识别系统，能在0.5秒之内识别出验证码，从 实际测试结果看，可以说宣布了对验证码的“死刑判决”。
      - ![](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=280512761,907748494&fm=173&app=49&f=JPEG?w=640&h=273&s=0D30E51281D85DC04A55B0CB0000D0B3)
      - [论文地址](http://www.lancaster.ac.uk/staff/wangz3/publications/ccs18.pdf)，博文介绍：[An A.I. cracks the internet’s squiggly letter bot test in 0.5 seconds](https://www.digitaltrends.com/cool-tech/ai-cracks-captcha-05-seconds/)
   - 该系统已在不同的33个验证码系统中进行了成功测试，其中11个来自世界上最受欢迎的一些网站，包括eBay和维基百科等。
   - 这种方法的新颖之处在于：使用生成对抗网络（GAN）来创建训练数据。不需要收集和标记数以百万计的验证码文本数据，只需要500组数据就可以成功学习。而且可以使用这些数据，来生成数百万甚至数十亿的合成训练数据，建立高性能的图像分类器。
   - 结果显示，<font color='red'>该系统比迄今为止所见的任何验证码识别器系统的识别精度都高。</font>
   - ![](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=1299396691,4195542946&fm=173&app=49&f=JPEG?w=640&h=416&s=A498E633795644CA4A6580DA0000C0B3)


# 图像风格化

[图像风格迁移(Neural Style)简史](https://zhuanlan.zhihu.com/p/26746283)

图像风格迁移科技树
- ![](https://pic4.zhimg.com/80/v2-526f16430324d3fbd8c07ff3d1c05c0b_hd.jpg)

## Demo

- 【2021-5-11】[案例解析：用Tensorflow和Pytorch计算骰子值](https://www.toutiao.com/i6622845628902801933/)
  - ![](https://p6-tt.byteimg.com/origin/pgc-image/557db089145144b5a7ae5195cf6d4aef?from=pc)，[github](https://github.com/sugi-chan/2-stage-dice-pipeline)
- 【2020-12-04】AI姿势传递模型，[论文地址](https://arxiv.org/pdf/2012.01158.pdf)，不愿意出节目的码农的年会神器？[将舞蹈化为己用-视频](https://weibo.com/tv/show/1034:4578074625245199?from=old_pc_videoshow)
- 【2020-12-02】【MaskDetection：滴滴开源的口罩检测模型】 by DiDi [GitHub](https://github.com/didi/maskdetection)
- 【2020-12-04】[孪生网络用于图片搜索](https://www.pyimagesearch.com/2020/11/30/siamese-networks-with-keras-tensorflow-and-deep-learning/)《Siamese networks with Keras, TensorFlow, and Deep Learning - PyImageSearch》by Adrian Rosebrock
  - ![](https://www.pyimagesearch.com/wp-content/uploads/2020/11/keras_siamese_networks_header.png)
- 【2022-10-8】北大博士的图像、视频风格化展示：[williamyang1991](https://williamyang1991.github.io/)，还有[文本字体风格化](https://williamyang1991.github.io/project.html#research1)

### fast-style

- 图像风格迁移，[深度学习之风格迁移简介](http://melonteam.com/posts/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/)
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/5489df3b2f1d117cbd275724697edda9ccadb0e92ba6d7c40dfb19c465378e01)
- Fast style transfer check [demo](https://wqw547243068.github.io/demo/fast-style/)

![alt text](https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/images/fast-style.PNG)

### 风格化案例

- 【2019-07-19】[图像风格迁移示例](https://reiinakano.github.io/arbitrary-image-stylization-tfjs)

<iframe src="https://reiinakano.github.io/arbitrary-image-stylization-tfjs" scrolling="yes" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

### Real Time style transfer 实时风格迁移

Real Time style transfer check [demo](https://wqw547243068.github.io/demo/RST/)
- ![alt text](https://raw.githubusercontent.com/zaidalyafeai/zaidalyafeai.github.io/master/images/rst.png)

## 扩散模型

【2022-8-31】苏剑林的[生成扩散模型漫谈](https://kexue.fm/archives/9119)
- 生成模型中，VAE、GAN“如雷贯耳”，还有一些比较小众的选择，如flow模型、VQ-VAE等，颇有人气，尤其是VQ-VAE及其变体VQ-GAN，近期已经逐渐发展到“图像的Tokenizer”的地位，用来直接调用NLP的各种预训练方法。
- 除此之外，还有一个本来更小众的选择——`扩散模型`（Diffusion Models）——正在生成模型领域“异军突起”，当前最先进的两个文本生成图像—— OpenAI 的 `DALL·E 2` 和 Google的`Imagen`，都是基于`扩散模型`来完成的。

生成扩散模型的大火，始于2020年所提出的[DDPM](https://arxiv.org/abs/2006.11239)（Denoising Diffusion Probabilistic Model），虽然也用了“**扩散模型**”这个名字，但事实上除了采样过程的形式有一定的相似之外，DDPM与传统基于`朗之万`方程采样的扩散模型完全不一样，一个新的起点、新的篇章。

### 扩散模型资源

- 【2022-9-20】[扩散模型大全](https://github.com/heejkoo/Awesome-Diffusion-Models)
- hugginface的扩散模型包：[diffusers](https://github.com/huggingface/diffusers/tree/main/examples)，[colab笔记](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=13NnZ4rVioLs), demo: [stable-diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion)

经典论文
- 《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》 2015年 扩散模型起源
- 《Denoising Diffusion Probabilistic Models》 2020年 扩散模型兴起, 对应[pytorch实现](https://github.com/lucidrains/denoising-diffusion-pytorch)
- 《Improved Denoising Diffusion Probabilistic Models》 2021年 第二篇论文的改进, 对应[pytorch实现](https://github.com/openai/improved-diffusion)

技术文章
- [The recent rise of diffusion-based models](https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html) 可以了解到扩散模型近年比较经典的应用
- [Introduction to Diffusion Models for Machine Learning](https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/) 从中可以了解到一个实现扩散模型的库denoising_diffusion_pytorch，博客中有使用案例
- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) 也是扩散模型的一个理论介绍博客，推导挺详细的
- [Diffusion Models as a kind of VAE](https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html) 探究了VAE和扩散模型的联系
- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion) 扩散模型理论和代码实现，代码我进行理解加了注释与理论对应，方便大家理解
- [An introduction to Diffusion Probabilistic Models](https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html) 也是一个介绍性博客，公式也很工整

[扩散模型原理和pytorch代码实现初学资料汇总](https://blog.csdn.net/qq_44941689/article/details/126513283)


### 扩散模型原理

标准的`扩散模型`（diffusion models）涉及到**图像变换**（添加高斯噪声）和**图像反转**。但是扩散模型的生成并不强烈依赖于图像降解的选择。通过实验证明了基于完全确定性的降解（例如模糊、masking 等），也可以轻松训练一个扩散生成模型。
- [项目地址](https://github.com/arpitbansal297/cold-diffusion-models)
- [论文地址](https://arxiv.org/abs/2208.09392)
这个工作成功地质疑了社区对扩散模型的理解：它并非依赖于**梯度郎之万动力学**（gradient Langevin dynamics）或**变分推理**（variational inference）。


准确来说，`DDPM`叫“**渐变模型**”更为准确一些，扩散模型这一名字反而容易造成理解上的误解，传统扩散模型的**能量模型**、**得分匹配**、`朗之万`方程等概念，其实跟DDPM及其后续变体都没什么关系。
- DDPM的数学框架其实在ICML2015的论文《Deep Unsupervised Learning using Nonequilibrium Thermodynamics》就已经完成了，但DDPM是首次将它在高分辨率图像生成上调试出来了，从而引导出了后面的火热。由此可见，一个模型的诞生和流行，往往还需要时间和机遇

### 多模态图像生成

【2022-9-7】[当 AI 邂逅绘画艺术，能迸发出怎样的火花？](https://posts.careerengine.us/p/63183f351bd2e15b54dda53f?from=latest-posts-panel&type=title)

`多模态图像生成`（Multi-Modal Image Generation）旨在利用文本、音频等模态信息作为指导条件，生成具有自然纹理的逼真图像。不像传统的根据噪声生成图像的单模态生成技术，多模态图像生成一直以来就是一件很有挑战的任务，要解决的问题主要包括：
- （1）如何跨越“语义鸿沟”，打破各模态之间固有的隔阂？
- （2）如何生成合乎逻辑的，多样性的，且高分辨率的图像？

近两年，随着 Transformer 在自然语言处理（如 GPT）、计算机视觉（如 ViT）、多模态预训练（如 CLIP）等领域的成功应用，以及以 VAE、GAN 为代表的图像生成技术有逐渐被后起之秀——扩散模型（Diffusion Model）赶超之势，多模态图像生成的发展一发不可收拾。

按照训练方式采用的是 Transformer 自回归还是扩散模型的方式，近两年多模态图像生成重点工作分类如下
- Transformer 自回归：将文本和图像分别转化成 tokens 序列，然后利用生成式的 Transformer 架构从文本序列（和可选图像序列）中预测图像序列，最后使用图像生成技术（VAE、GAN等）对图像序列进行解码，得到最终生成图像。
- 扩散模型：扩散模型（Diffusion Model）是一种图像生成技术，最近一年发展迅速，被喻为 GAN 的终结者。如图所示，扩散模型分为两阶段：
  - （1）加噪：沿着扩散的马尔可夫链过程，逐渐向图像中添加随机噪声；
  - （2）去噪：学习逆扩散过程恢复图像。常见变体有去噪扩散概率模型（DDPM）等。
- ![](https://static.careerengine.us/api/aov2/https%3A_%7C__%7C_mmbiz.qpic.cn_%7C_mmbiz_png_%7C_Z6bicxIx5naJlJ5U7H2h9WvOKicVvP1IMQsP7Beoqq3agsokoH4E75sO33rXmPORQ4djtdEB3IAMBnsk8bugYcKQ_%7C_640%3Fwx_fmt%3Dpng)

采取扩散模型方式的多模态图像生成做法，主要是通过带条件引导的扩散模型学习文本特征到图像特征的映射，并对图像特征进行解码得到最终生成图像


### 文字→视频

【2022-10-8】[图像生成卷腻了，谷歌全面转向文字→视频生成](https://www.toutiao.com/article/7151774108186083843)，挑战分辨率和长度; 文本转图像上卷了大半年之后，Meta、谷歌等科技巨头又将目光投向了一个新的战场：文本转视频。
- 上周，Meta公布了一个能够生成高质量短视频的工具——[Make-A-Video](https://makeavideo.studio/)，利用这款工具生成的视频非常具有想象力。
  - [Introducing Make-A-Video: An AI system that generates videos from text](https://ai.facebook.com/blog/generative-ai-text-to-video/)
  - ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4bc8759f50f747ff99ac4cd92eb2816b~noop.image)
- 谷歌也不甘示弱。刚刚，该公司 CEO Sundar Pichai 亲自安利了他们在这一领域的最新成果：两款文本转视频工具——Imagen Video 与 Phenaki。前者主打视频品质，后者主要挑战视频长度，可以说各有千秋。
  - ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/d85eca195ae541da80c8b7b93d247fa7~noop.image)
  - 生成式建模在最近的文本到图像 AI 系统中取得了重大进展，比如 DALL-E 2、Imagen、Parti、CogView 和 Latent Diffusion。特别地，扩散模型在密度估计、文本到语音、图像到图像、文本到图像和 3D 合成等多种生成式建模任务中取得了巨大成功。
  - 谷歌想要做的是从文本生成视频。以往的视频生成工作集中于具有自回归模型的受限数据集、具有自回归先验的潜变量模型以及近来的非自回归潜变量方法。扩散模型也已经展示出了出色的中等分辨率视频生成能力。
  - 在此基础上，谷歌推出了 [Imagen Video](https://imagen.research.google/video/)，[论文地址](https://imagen.research.google/video/paper.pdf),它是一个基于级联视频扩散模型的文本条件视频生成系统。给出文本提示，Imagen Video 就可以通过一个由 frozen T5 文本编码器、基础视频生成模型、级联时空视频超分辨率模型组成的系统来生成高清视频。

## 应用案例

### 杂货店货架识别

- 【2021-1-23】[Grocery-Product-Detection](https://github.com/sayakpaul/Grocery-Product-Detection)
  - This repository builds a product detection model to recognize products from grocery shelf images. The dataset comes from [here](https://github.com/gulvarol/grocerydataset). Everything from data preparation to model training is done using [Colab Notebooks](https://colab.research.google.com/) so that no setup is required locally. All the relevant commentaries have been included inside the Colab Notebooks.

### 文本生成图片

【2022-8-16】[TikTok 乱拳打死老师傅：硅谷大厂还在发论文，它产品已经上线了](https://www.sohu.com/a/577300364_114819)
- 不少家互联网大厂都在试图测试、开发 AI 文字转图片技术，结果没想到，TikTok 却率先将这项技术应用到了产品里，在 AI 创作潮流中异军突起。
- TikTok 的特效菜单下，最近增加了一个名叫“AI 绿幕” (AI Greenscreen) 的新选项。
- 点击这个选项，然后在屏幕中间的对话框里输入一段文字描述，只用不到5秒的时间，TikTok 就可以根据文字描述生成一张竖版画作，用作短视频的背景：
- ![](https://p3.itc.cn/q_70/images03/20220816/5fdb55b70e054099a88ea6bc5bfeca09.png)
- 生成结果具有非常强的水彩/油画感觉，风格迁移 (style transfer) 的痕迹明显，而且用的颜色也都鲜亮明快，给人一种耳目一新的感受。
- ![](https://p4.itc.cn/q_70/images03/20220816/902b3d3e8e8d45e0aba3be1bdf1694e6.png)

#### Disco Diffusion

Disco Diffsion 存在问题

基于多模态图像生成模型 Disco Diffusion（DD）进行 AI 创作目前存在以下几个问题：
- （1）生成图像质量参差不齐：根据生成任务的难易程度，粗略估算描述内容较难的生成任务良品率 20%～30%，描述内容较容易的生成任务良品率 60%～70%，大多数任务良品率在 30～40% 之间。
- （2）生成速度较慢+内存消耗较大：以迭代 250 steps 生成一张 1280*768 图像为例，需要大约花费 6分钟，以及使用 V100 16G 显存。
- （3）严重依赖专家经验：选取一组合适的描述词需要经过大量文本内容试错及权重设置、画家画风及艺术社区的了解以及文本修饰词的选取等；调整参数需要对 DD 包含的 CLIP 引导次数/饱和度/对比度/噪点/切割次数/内外切/梯度大小/对称/... 等概念深刻了解，同时要有一定的美术功底。众多的参数也意味着需要较强的专家经验才能获得一张还不错的生成图像。

#### Stable Diffusion

Stable Diffusion is a state of the art text-to-image model that generates images from text.

- transformers上的 [Stable Diffusion Demo](https://huggingface.co/spaces/stabilityai/stable-diffusion)
- For faster generation and forthcoming API access you can try [DreamStudio Beta](http://beta.dreamstudio.ai/)
- <iframe src="https://beta.dreamstudio.ai/dream">


#### DALL·E


#### 文心-一格

- 【2022-8-23】[国产AI作画神器火了，更懂中文，竟然还能做周边](https://mp.weixin.qq.com/s/xh6Q0Pnv9OfP8Je3lDiyZg), “一句话生成画作”这个圈子里，又一个AI工具悄然火起来了,不是你以为的Disco Diffusion、DALL·E，再或者Imagen……而是全圈子都在讲中国话的那种, [文心·一格](https://yige.baidu.com/#/)
  - 操作界面上，Disco Diffusion开放的接口不能说很复杂，但确实有点门槛。它直接在谷歌Colab上运行，需要申请账号后使用（图片生成后保存在云盘），图像分辨率、尺寸需要手动输入，此外还有一些模型上的设置。好处是可更改的参数更多，对于高端玩家来说可操作性更强，只是比较适合专门研究AI算法的人群;相比之下，文心·一格的操作只需三个步骤：输入文字，鼠标选择风格&尺寸，点击生成。
  - 提示词，Disco Diffusion的设置还要更麻烦一些。除了描述画面的内容以外，包括画作类别和参考的艺术家风格也都得用提示词来设置，通常大伙儿会在其他文档中编辑好，再直接粘过来。相比之下文心·一格倒是没有格式要求，输入150字的句子或词组都可以
  - 性能要求上，Disco Diffusion是有GPU使用限制的，每天只能免费跑3小时。抱抱脸（HuggingFace）上部分AI文生图算法的Demo虽然操作简单些，但一旦网速不行，就容易加载不出来; 文心·一格除了使用高峰期以外，基本上都是2分钟就能生成，对使用设备也没有要求。
  - 总体来看，同样是文字生成图片AI，实际相比文心·一格的“真·一句话生成图片”，DALL·E和Disco Diffusion的生成过程都不太轻松。

看似“一句话生成图片”不难，其实对AI语义理解和图像生成能力提出了进一步要求。
- 为了能更好地理解文本、提升输出效果，文心·一格还在百度文心的图文生成跨模态模型ERNIE-VilG的基础上，进行了更详细的优化。
- 为了提升图文理解能力，在知识增强的基础上，引入跨模态多视角对比学习；
- 为了降低输入要求同时提升效果，采用基于知识的文本联想能力，让模型学会自己扩展提示词的细节和风格；
- 为了提升图像生成能力，采用渐进式扩散模型训练算法，让模型来选择效果最好的生成网络。


StableDiffusion 图像生成能力一探！Int8量化教程与ONNX导出推理
- CPU下推理StableDiffusion，以及OpenVINO加速的代码，同时，也包含了量化脚本

```shell
#git clone https://github.com/luohao123/gaintmodels
git clone https://huggingface.co/CompVis/stable-diffusion-v1-4
git lfs install
cd stable-diffusion-v1-4
git lfs pull
```

测试StableDiffusion
- 来看看生成的效果，由于模型只能编码英文，我们就以英文作为promopt。
- A green car with appearance of Tesla Model 3 and Porsche 911
- A robot Elon Musk in cyberpunk, driving on a Tesla Model X


#### 自定义图片的text2image

【2022-9-7】[An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://textual-inversion.github.io/)
- [github-textual_inversion](https://github.com/rinongal/textual_inversion)
- 基于潜在扩散模型（Latent Diffusion Models, LDM），允许用户使用自然语言指导 AI 生成包含特定独特概念的图像。
- 例如我想将心爱的宠物猫咪变成一幅独特的画作——抽象派猫猫，只需要提供3-5张照片，然后通过控制自然语言输入，来得到一个我家猫咪的抽象画作。
- 简单介绍下过程：首先，模型会通过学习这些图片，使用一些单词去表示图片。其次，这些单词可以组合成自然语言句子，通过 prompt 形式指导模型进行个性化创作。好处在于，图像的自然语言表示对用户非常友好。用户可以自由修改 prompt 内容以获取他们想要的风格、主题和独一无二的结果。
- We learn to generate **specific concepts**, like personal objects or artistic styles, by describing them using new "words" in the embedding space of pre-trained **text-to-image** models. These can be used in new sentences, just like any other word.
- Our work builds on the publicly available [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion)
- ![](https://textual-inversion.github.io/static/images/editing/teaser.JPG)
- ![](https://textual-inversion.github.io/static/images/training/training.JPG)

#### 视频风格化

【2022-9-7】通过将预训练的语言图像模型（pretrained language-image models）调整为视频识别，以此将对比语言图像预训练方法（contrastive language-image pretraining）扩展到视频领域；
- 为了捕捉视频中帧沿时间维度的远程依赖性，提出了一个跨帧的注意力机制，明确了跨帧的信息交换。此外该模块非常轻量化，可以无缝插入预训练的语言图像模型。
- [项目地址](https://github.com/microsoft/videox)
- [论文地址](https://arxiv.org/abs/2208.02816)


## 资料

- 更多[Demo地址](http://wqw547243068.github.io/demo)

# 风格迁移简介

- [深度学习之风格迁移简介](http://melonteam.com/posts/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/)

`风格迁移`（style transfer）最近两年非常火，可谓是深度学习领域很有创意的研究成果。它主要是通过神经网络，将一幅艺术风格画（style image）和一张普通的照片（content image）巧妙地融合，形成一张非常有意思的图片。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/4a0dbd0ba7031a2b9e0f97d222d5050799764b92b7b135ffba3edfda4fd2feea)

因为新颖而有趣，自然成为了大家研究的焦点。目前已经有许多基于风格迁移的应用诞生了，如移动端风格画应用Prisma，手Q中也集成了不少的风格画滤镜：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/de7624d2c5163daeb833b4a4a4d4bbbf340fbc2a8289763833f8de2608f89b9c)

本文将对风格迁移[^1]的实现原理进行下简单介绍，然后介绍下它的快速版，即fast-style- transfer[^2]。

## 1. 风格迁移开山之作

2015年，Gatys等人发表了文章[^1]《A Neural Algorithm of Artistic Style》，首次使用深度学习进行艺术画风格学习。把风格图像Xs的绘画风格融入到内容图像Xc，得到一幅新的图像Xn。则新的图像Xn：即要保持内容图像Xc的原始图像内容（内容画是一部汽车，融合后应仍是一部汽车，不能变成摩托车），又要保持风格图像Xs的特有风格（比如纹理、色调、笔触等）。

### 1.1 内容损失（Content Loss）

在CNN网络中，一般认为较低层的特征描述了图像的具体视觉特征（即纹理、颜色等），较高层的特征则是较为抽象的图像内容描述。 所以要比较两幅图像的内容相似性，可以比较两幅图像在CNN网络中高层特征的相似性（欧式距离）。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/880c6f8c21936bb4c29a2e6952ce357f7e844e7328c86f2a730f500768e66802)

1.2 风格损失（Style Loss）
而要比较两幅图像的风格相似性，则可以比较它们在CNN网络中较低层特征的相似性。不过值得注意的是，不能像内容相似性计算一样，简单的采用欧式距离度量，因为低层特征包含较多的图像局部特征（即空间信息过于显著），比如两幅风格相似但内容完全不同的图像，若直接计算它们的欧式距离，则可能会产生较大的误差，认为它们风格不相似。论文中使用了Gram矩阵，用于计算不同响应层之间的联系，即在保留低层特征的同时去除图像内容的影响，只比较风格的相似性。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/bc72f7cd6be684f73c7c7e3649dbba4b030bb2607c66370104e043c71b2ac31c)

那么风格的相似性计算可以用如下公式表示：

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/1a7ce05010b913ae2c5f58ef362aa76638199c79293f493856feb80d99703476)

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/eff0adf1fd4b183cb872d79e2a5a70ca66d6d21845a59bbf6faf31012532be3a)

### 1.3 总损失（Total Loss）

这样对两幅图像进行“内容+风格”的相似度评价，可以采用如下的损失函数：
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/3d679c5b0a174e93a56eba66383e6abd57431c24e76805f0fdcf8d7caa3d89ef)

### 1.4 训练过程

文章使用了著名的VGG19网络[3]来进行训练（包含16个卷积层和5个池化层，但实际训练中未使用任何全连接层，并使用平均池化average- pooling替代最大池化max-pooling）。
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/3f981ef8277f3d56dbc0dfb8cb9fb38bbcf6c35914f8bc3e53fda29ac2eed6f6)

内容层和风格层的选择：将`内容图像`和`风格图像`分别输入到VGG19网络中，并将网络各个层的特征图（feature map）进行可视化（重构）。

内容重构五组对比实验：
- 1. conv1_1 (a)
- 2. conv2_1 (b)
- 3. conv3_1 (c)
- 4. conv4_1 (d)
- 5. conv5_1 (e)
风格重构五组对比实验：
- 1. conv1_1 (a)
- 2. conv1_1 and conv2_1 (b) 
- 3. conv1_1, conv2_1 and conv3_1 (c)
- 4. conv1_1, conv2_1, conv3_1 and conv4_1 (d)
- 5. conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 (e)

通过实验发现：对于内容重构，(d)和(e)较好地保留了图像的高阶内容（high-level content）而丢弃了过于细节的像素信息；对于风格重构，(e)则较好地描述了艺术画的风格。如下图红色方框标记：
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/44a2b217d7d007c6110d5248c90ccf0f66c6296f320954668d73c1af6b0d5fa7)

在实际实验中，内容层和风格层选择如下：
- 内容层：conv4_2
- 风格层：conv11, conv2_1, conv3__1_, _conv4_1, conv5_1
- 训练过程：以白噪声图像作为输入(x)到VGG19网络，conv4_2层的响应与原始内容图像计算出内容损失（Content Loss），“conv1_1, conv2_1, conv3_1, conv4_1, conv5_1”这5层的响应分别与风格图像计算出风格损失，然后它们相加得到总的风格损失（Style Loss），最后Content Loss + Style Loss = Total Loss得到总的损失。采用梯度下降的优化方法求解Total Loss函数的最小值，不断更新x，最终得到一幅“合成画”。

### 1.5 总结

每次训练迭代，更新的参数并非VGG19网络本身，而是随机初始化的输入x；
由于输入x是随机初始化的，最终得到的“合成画”会有差异；
每生成一幅“合成画”，都要重新训练一次，速度较慢，难以做到实时。

## 2. 快速风格迁移

2016年Johnson等人提出了一种更为快速的风格迁移方法[2]《[Perceptual losses for real-time style transfer and super- resolution](http://cs.stanford.edu/people/jcjohns/papers/eccv16/JohnsonECCV16.pdf)》。

### 2.1 网络结构
它们设计了一个变换网络（Image Transform Net），并用VGG16网络作为损失网络（Loss Net）。输入图像经由变换网络后，会得到一个输出，此输出与风格图像、内容图像分别输入到VGG16损失网络，类似于[1]的思路，使用VGG16不同层的响应结果计算出内容损失和风格损失，最终求得总损失。然后使用梯度下降的优化方法不断更新变换网络的参数。 
- 内容层：relu3_3
- 风格层：relu12, relu2_2, relu3_3, _relu4_3
其中变换网络（Image Transform Net）的具体结构如下图所示： 

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/bea3e1a19df5198b9a31f7d241361cb129f13e0d0d5f4f4e0d14439d6d0b8126)

### 2.2 跑个实验

Johnson等人将论文的代码实现在[github](https://github.com/jcjohnson/fast-neural-style)上进行了开源，包括了论文的复现版本，以及将“Batch-Normalization ”改进为“Instance Normalization”[[4](https://arxiv.org/pdf/1607.08022.pdf)]的版本。咱们可以按照他的说明，训练一个自己的风格化网络。我这里训练了一个“中国风”网络，运行效果如下： 
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/5489df3b2f1d117cbd275724697edda9ccadb0e92ba6d7c40dfb19c465378e01)
- ![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/c628d678344dddaef81c122d33fcff1cd00d1d7f2b90834410492ae03bf005d4)

### 2.3 总结

网络训练一次即可，不像Gatys等人[1]的方法需要每次重新训练网络；
可以实现实时的风格化滤镜：在Titan X GPU上处理一张512x512的图片可以达到20FPS。下图为fast-style-transfer与Gatys等人[1]方法的运行速度比较，包括了不同的图像大小，以及Gatys方法不同的迭代次数。

![](http://melonteam.com/image/shen_du_xue_xi_zhi_feng_ge_qian_yi_jian_jie/66b64458ff003281762ca3b3da2a7ad3e769b6274259431e2dbd82f9fd5543dd)

3. 参考资料

- Gatys L A, Ecker A S, Bethge M. A neural algorithm of artistic style[J]. arXiv preprint arXiv:1508.06576, 2015.
- Johnson J, Alahi A, Fei-Fei L. Perceptual losses for real-time style transfer and super-resolution[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 694-711.
- Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.
- Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization[J]. arXiv preprint arXiv:1607.08022, 2016.
- [Fast Style Transfer(快速风格转移)](http://closure11.com/fast-style-transfer%E5%BF%AB%E9%80%9F%E9%A3%8E%E6%A0%BC%E8%BD%AC%E7%A7%BB/)
- [图像风格迁移(Neural Style)简史](https://zhuanlan.zhihu.com/p/26746283)

