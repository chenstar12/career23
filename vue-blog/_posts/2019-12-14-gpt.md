---
layout: post
title:  GPT
date:   2019-12-14 16:52:00
categories: 深度学习 自然语言处理
tags: GPT gpt 文本生成
excerpt: 大语言模型之GPT
mathjax: true
permalink: /gpt
---

* content
{:toc}


# GPT模型

- 迭代路线：GPT → GPT-2 → GPT-3

项目[github页面](https://github.com/openai/gpt-3)和论文[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165), 目前没有代码只有生成样本和数据.52页的T5，72页的GPT-3

## 体验

申请账号，调用官方[api](https://beta.openai.com/?app=creative-gen&demo=5)

openai提供的[应用示例集合](https://beta.openai.com/examples)

代码：

```python
import os
import openai

openai.api_key = os.getenv("OPENAI_API_KEY")

# ------- 文本生成 ---------
prompt = """We’re releasing an API for accessing new AI models developed by OpenAI. Unlike most AI systems which are designed for one use-case, the API today provides a general-purpose “text in, text out” interface, allowing users to try it on virtually any English language task. You can now request access in order to integrate the API into your product, develop an entirely new application, or help us explore the strengths and limits of this technology."""

response = openai.Completion.create(model="davinci", prompt=prompt, stop="\n", temperature=0.9, max_tokens=100)

# ------- 其它应用 ---------
response = openai.Completion.create(
  engine="davinci",
  prompt="The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n\nHuman: Hello, who are you?\nAI: I am an AI created by OpenAI. How can I help you today?\nHuman: I'd like to cancel my subscription.\nAI:",
  temperature=0.9,
  max_tokens=150,
  top_p=1,
  frequency_penalty=0.0,
  presence_penalty=0.6,
  stop=["\n", " Human:", " AI:"]
)

print(response)

```

### 个性化聊天

【2021-10-13】[AI 复活「她」！GPT-3 帮美国小哥复刻逝去未婚妻，但又夺走她…](https://www.toutiao.com/i7018474312931885576/),7 月，一名33岁的美国小哥 Joshua Barbeau 在未婚妻去世后，根据她在 Facebook 和 twitter 上的文本在另一名开发人员的帮助下成功在 GPT-3 上微调，能够复刻出未婚妻生前的谈话方式。有很多人觉得 Joshua Barbeau 这个行为很可怕。但他认为，借助 Project December 项目创建出模拟已故之人的聊天机器人，可能会“帮助一些因此抑郁的人解开他们的心结”。但，Project December 的开发作者 Jason Rohrer 却收到了来自 OpenAI 的最后通牒：我们会在 9 月 2 日上午 10 点终止你的 API 访问。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/6a0cdb8a4c414ed08b135927a0af5460?from=pc)
- Jason Rohrer 是一名独立游戏开发者，Project December 是他于去年夏天疫情期间突发奇想的一个灵感：用 GPT-3 API 来开发一款模拟人类的聊天机器人，以电影《Her》中男主角的智能语音助手 Samantha 为原型
- 7月用户量突然激增。在《旧金山纪事报》报道的一篇文章讲述了一位 33 岁的美国男子 Joshua Barbeau 如何利用 Project December 创建出模拟其已故未婚妻 Jessica 的聊天机器人并与之交谈数月。在那之后，成千上万的人开始使用 Rohrer 网站。
- Rohrer 意识到他的网站将达到每月 API 的限制。主动联系 OpenAI 希望能通过支付更多费用以增加配额，以便容纳更多人与他创建的“Samantha”或自己的聊天机器人交流。但与此同时，OpenAI 方面认为 Project December 存在一定隐患：聊天机器人可能会被滥用或对人们造成伤害。
- 因此，双方进行了一场视频会议，可是很明显，效果并不理想。Jason Rohrer 在接受外媒 The Register 的采访时提到，OpenAI 给出了继续使用 GPT-3 API 需满足的 3 个条件：
  - Jason Rohrer 必须禁止人们定制自己的开放式聊天机器人这一功能。
  - Jason Rohrer 需设置内容过滤器以避免用户与“Samantha”谈论敏感话题。
  - Jason Rohrer 必须部署自动化监控工具监视用户的对话，检测他们是否滥用 GPT-3 生成了有害或敏感语句。
- OpenAI 的员工与 Samantha 聊天，并试图了解「她」是否有种族主义倾向，或者会从「她」的训练数据中提供看起来像真实电话号码或电子邮件地址的内容，实验结果表明Samantha很老实，什么也没有说。Samantha 的输出令人感觉很真实，但随着时间的推移，很明显你能感觉到是在与某种自动化系统交谈，谈话的过程中往往会突然丢失对话思路。
  - ![](https://p5.toutiaoimg.com/origin/pgc-image/7add87e26cca475a848d79669be7b2e1?from=pc)
- OpenAI 担心用户会受到 AI 的影响，害怕机器人会让他们自杀或如何给选举投票，可这完全是一种超道德的立场。
- Jason Rohrer 拒绝添加 OpenAI 要求的功能或机制，而是悄悄将原本 Project December 使用的 GPT-3 API 断开。并且替换为功能较差的开源 **GPT-2模型**以及由另一个研究团队开发的大型语言模型 **GPT-J-6B**。不过这两种模型性能显然不比 GPT-3，Samantha的对话能力也受到了影响。

【2022-4-6】[外公去世十年后，我用 AI“复活”了他](https://www.toutiao.com/article/7083406230143681028)

一位程序员，天天跟 AI 和算法打交道，不免开始盘算：现阶段的 AI 技术能不能整合到一起，最终实现一个无论是语言表达还是人形上都极其接近我外公的效果。于是我开始搜索，发现了不少和我相同的愿望，也有人付诸实践。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/200f8a60652f41f5840b558ea0ea938a?from=pc)
- 韩国一位母亲因七岁女儿去世万分痛苦，一个电视团队听闻后耗时八个月制作出了女孩的三维虚拟形象，让母女在 VR 场景中相遇。在我看来，这更偏向动画制作，女孩形象和场景比较“卡通”

Project December 只能造出文字聊天机器人，我想合成一个有具体可感形象的“外公”，最好能写实一些。
- “他有记忆，能和我互动，能开口说话，脸一看就是我外公”，这个大胆的想法越来越清晰，我开始检索可能用得上的 AI 论文。

先做“外公”的大脑
- Project December 之所以能基于种子文本，生成有特定个性的角色，是因为接入了 GPT-3 的 API。GPT-3 是 OpenAI 的商业语言模型，可以简单理解为这个模型给了计算机像“人一样思考的能力”。

GPT-3 甚至能说出一些“高于人类”的话：
- 开始准备要导入 GPT-3 的种子文本，把之前保留的信件扫描成文字，整理好之前同步到云上的聊天短信，还扒下外公之前在视频里说过的话：“这个鱼还是要红烧，八十多块买来清蒸，味道洁洁淡（杭州话，“清淡”的意思），没味道。”“你不要手机一直拍来拍去，去帮你阿弟端菜。”
- 一股脑导入 GPT-3 后，它就能开始模仿外公的语言风格和对话思路……等等，GPT-3 收费。不过，我很快找到了免费开源的 GPT-J，开始了训练。
语言模型训练就是“猜词”的过程。模型利用显卡并行计算，找出一个语料库中每个词句之间的关系，比如出现一个词后，下一个词最有可能是什么。GPT-J 团队开源了预训练模型，已能实现大部分功能，我需要做的就是把种子文本转换成一个个词元，然后将这个外公专有语料库丢给 GPT-J 学习。

一般的深度学习模型需要训练几天几夜，我这次用 GPT-J 学习新语料并不是特别耗时，只需花六个小时。


人类：人生的目的是什么？
- AI：生命是一个美丽的奇迹。它随着时间不断进化，形成一种更大形式的美。从某种意义上来说，人生的目的就是增加宇宙中的这种美。
- 它之所以有这种能力，是因为工程师给这个模型猛喂数据，足足超过 3000 亿个文本。AI 模型在看了这么多文本后，就开始挖掘（也就是找规律）出词与词、句与句之间的关系，然后结合当前语境给出最适合的回答。

用语音驱动人脸
- 让我外公“显形”最直接的就是构建一个三维定制虚拟人像，但这需要采集人体数据点，很显然这条路行不通。
- 结合手头现有的照片、语音和视频等素材，我开始思考：有没有可能只用一段视频加上一串语音，就能生成一个栩栩如生的人脸呢？
- 几经波折，我找到了“Neural Voice Puppetry”这个方案，它是一种“人脸再扮演”（facial reenactment）技术，我只需要给定对话音频，它就能生成一段人脸嘴型与音频同步的动画。
- 论文作者利用卷积神经网络，把人脸外观、脸部情绪渲染和语音三者的关系找出来了，然后再利用这种学到的关系去渲染一帧帧能读出语音的人脸视频。但这个方案唯一的不足是不能指定输出的人物，我们只能选择给定人物，比如奥巴马。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/e424e2a592894b019b3c2d43fde7cc41?from=pc)
实际得到的结果，是一段奥巴马用我外公声音在讲话的视频。我下一步要做的是 AI 换脸。
- 我最终选择用 HeadOn: Real-time Reenactment of Human Portrait Videos这篇论文里提到的技术。相关应用就是现在时兴的虚拟主播：捕捉中之人的表情，驱动二次元人物的脸。
- 提供表情信息的一般是真人，但由于我之前生成的“奥巴马”非常逼真，所以可以直接拿来带动我外公的肖像。
- 就这样，我用了我外公生前的通讯记录和不多的影音资料，整合几个成熟的 AI 技术，就让他“复活”了。
- ![](https://p3.toutiaoimg.com/origin/tos-cn-i-qvj2lq49k0/f2a8dc12e4314d6fb796cf9365337485?from=pc)



## 资料

- 【2021-10-13】[GPT-3 Creative Fiction](https://www.gwern.net/GPT-3) 小说作品创作
- 【2019-2】张俊林：[效果逆天的通用语言模型 GPT 2.0 来了，它告诉了我们什么？](https://www.infoq.cn/article/pW8YaUXjTuhC6d0p*OwX)
- [OpenAI GPT-3 API](https://openai.com/blog/openai-api/)，[Github地址](https://github.com/elyase/awesome-gpt3#awesome-gpt-3)
- ![](https://github.com/elyase/awesome-gpt3/raw/master/screenshot.png)

- Jay Alammar杰作：[怎样向产品解释GPT-3](http://jalammar.github.io/how-gpt3-works-visualizations-animations/)
![](http://jalammar.github.io/images/gpt3/05-gpt3-generate-output-context-window.gif)

- 资料
  - [GPT-3的50种玩法告诉你，它很酷，但是没有通过图灵测试](https://www.toutiao.com/a6855330183403012621/)
  - [最新最全GPT-3模型网络结构详细解析](https://www.toutiao.com/i6858589917883138571/)
  - 知乎：[如何评价1700亿参数的GPT-3？](https://www.zhihu.com/question/398114261)

## 发展历史

- 2018年6月，OpenAI的研究人员使用了一种新颖的组合，将生成式深度学习架构Transformer和无监督预训练（也称为自监督学习）结合起来，得到了GPT模型。
- Transformer的自注意力机制提供了一种通用的方式来对输入的各个部分进行建模，使其依赖于输入的其他部分（需要大量计算）。
- Transformer和无监督预训练的组合不限于GPT系列模型。Google，Facebook和许多大学实验室相继提出了BERT、XLNet等语言模型。
- 到2019年初，OpenAI改进了其基础架构，将参数和数据数量增加10倍来扩展同一模型，即GPT-2。
- 随后，OpenAI推出了SparseTransformer，它是对早期Transformer模型的改进，可以可靠地处理更长的文档。
- 2020年，OpenAI通过其beta API发布了GPT-3，引起了人们的关注。GPT-3不仅扩大了GPT-2上使用的数据量和计算量，而且用SparseTransformer取代了原始Transformer，从而产生了迄今为止具有最佳zero-shot 和 few-shot学习性能的模型。
- GPT-3的few-shot学习能力使得它具备了一些非常有趣的演示功能，包括自动代码生成、“搜索引擎”、写作辅助和创意小说等。

- 【2020-8-10】[京东副总裁何晓冬：GPT-3后，人机对话与交互何去何从？CCF-GAIR 2020](https://www.leiphone.com/news/202008/BC6XqIXF3ifH6uvV.html)
![](https://static.leiphone.com/uploads/new/images/20200810/5f311dc980e89.jpg?imageView2/2/w/740)

【2021-2-6】[GPT发家史](https://mp.weixin.qq.com/s/Y8yHaf7dm5jEQAvP9IvRRA)
- OpenAI 成立之初并非因为文本生成模型而知名，这点和 DeepMind 些许不同，后者专注强化学习一百年。 OpenAI 一开始两条线是**强化学习**和**生成模型**（集中 GAN），而 GPT 开始也没受到太大关注，而是在探索中 OpenAI 发现了其可能性，便开始大力投入，到现在基本上一大半项目都与其相关。所以，现今大家提起 OpenAI 相信都是马上想起 GPT，再或者和马一龙（Elon Musk）早期有一腿，又多少人还能想起强化学习和GAN呢。
- OpenAI 早期成员，除 Pieter Abbeel 等做强化学习的，就是一众做偏图像生成的，比如
- GAN 提出者 Ian Goodfellow 最早也是入职 OpenAI
- 同期入职的还有一个叫 Alec Radford 发明 DCGAN 的精神小伙。大家记住这个名字，因为他对 GPT 的发展应该说至关重要。
- 所以可以看出最早 OpenAI 就是群做强化学习和图像生成的人，没啥做 NLP 的，自然也难料想自己居然是通过 NLP 来一战成名。

GPT系列：
- 2018年6月 `GPT-1`：大量数据（约5GB文本）上无监督训练，然后针对具体任务在小的有监督数据集上做微调；关键词：“scalable, task-agnostic system”；8个GPU上训练一个月；预训练模型（1.1亿参数）可[下载](https://github.com/openai/finetune-transformer-lm)；
- 2019年2月 `GPT-2`：大量数据（约40GB文本）上无监督训练，然后针对具体任务在小的有监督数据集上做微调，尝试在一些任务上不微调（即使结果离SOTA还远）；关键词“without task-specific training”；据说在256个Google Cloud TPU v3上训练，256刀每小时，训练时长未知[2]；预训练模型（15亿参数）最终公开可[下载](https://github.com/openai/gpt-2-output-dataset)；[openai model](https://openai.com/blog/better-language-models/​openai.com/blog/better-language-models/)
- 2020年5月 `GPT-3`：大量数据（499B tokens）上无监督训练，不微调就超越SOTA；关键词“zero-shot, one-shot, few-shot”；训练据说话费1200万刀；1750亿参数，将会开放付费API


![](http://files.cn-healthcare.com/upload/20201117/wximg/41331605568278419)

[白描网页版](https://web.baimiaoapp.com/)

| 时间| 机构| 模型名称| 模型规模| 数据规模 | 计算时间|
|---|---|---|---|---|---|
|2018.6 | OpenAI | GPT | 110M | 4GB| 3天 |
|2018.10 | Google | BERT | 330M | 16GB | 50天 |
| 2019.2 | OpenAI | GPT-2 | 1.5B | 40GB | 200天 |
| 2019.7 | Facebook | RoBERTa | 330M | 160GB | 3年 |
| 2019.10 | Google| T5| 11B| 800GB| 66年|
| 2020.6| OpenAl| GPT-3| 175B| 2TB| 355年|
| 2021| 预计 | 预计|~1000B| ~10TB| ～1000年|

【202-7-14】[人工智能GPT3](https://zhuanlan.zhihu.com/p/159414219)

2019 年初，OpenAI 发布了通用语言模型 GPT-2，能够生成连贯的文本段落，在许多语言建模基准上取得了 SOTA 性能。这一基于 Transformer 的大型语言模型共包含 15 亿参数、在一个 800 万网页数据集上训练而成。GPT-2 是对 GPT 模型的直接扩展，在超出 10 倍的数据量上进行训练，参数量也多出了 10 倍。

OpenAI在最近， 新提出的 GPT-3 在网络媒体上引起啦的热议。因为它的参数量要比 2 月份刚刚推出的、全球最大深度学习模型 Turing NLP 大上十倍，而且不仅可以更好地答题、翻译、写文章，还带有一些数学计算的能力。
- [NLP各种语言模型参数对比](https://pic2.zhimg.com/80/v2-ddabb5228a36ec649adfad9a1589d838_720w.jpg?source=1940ef5c)
  - ![](https://pic2.zhimg.com/80/v2-ddabb5228a36ec649adfad9a1589d838_720w.jpg?source=1940ef5c)
  - 最早的ELMO模型有94M，然后2018年7月GPT出世，模型参数有110M，接着BERT-Large有340M；后来GPT-2出世已经把参数弄到1.5b了；再后来随着Turing  NLG的出现直接将参数提升到17b，成为当时最大的模型；最后GPT-3出现了，直接将参数增加到175b，参数量基本上是第二名Turing  NLG的十倍。参考：[数据拾光者](https://www.zhihu.com/question/398114261/answer/1647770083)
- `GPT-2` （参数15 亿）、`Megatron-BERT`（80 亿参数）、`Turing NLG`（170 亿参数），而`GPT-3`直接1700亿个参数。GPT-3不需要fine-tune，就能具有非常好的效果


GPT-3 在许多 NLP 数据集上均具有出色的性能，包括翻译、问答和文本填空任务，这还包括一些需要即时推理或领域适应的任务，例如给一句话中的单词替换成同义词，或执行 3 位数的数学运算。新闻生成，GPT-3生成的新闻我们很难将机器写的和人类写的区分。

GPT-3 是一种具有1,750亿个参数的自然语言深度学习模型，足足是 GPT-2 的 **116倍** 。该模型经过了将近0.5万亿个单词的预训练，并且在不进行微调的情况下，可以在多个NLP基准上达到最先进的性能。

GPT-3 最令人惊讶的还是**模型体量**，它用的最大数据集在处理前容量达到了 **45TB**。根据 OpenAI 的算力统计单位 petaflops/s-days，训练 AlphaGoZero 需要 1800-2000pfs-day，而 OpenAI 刚刚提出的 GPT-3 用了 3640pfs-day。
- Google的T5论文的一页实验烧了几百万美元，当时看起来已经是壕无人性了，但背靠MS的OpenAI的GPT-3需要的GPU算力是BERT的近2000倍，训练成本保守估计一千万美元，以至于训练出了一个bug也无能无力，论文只能拿出一部分篇幅研究了这个bug会有多大影响
- 当下入坑DL建议：<font color='red'>穷搞理论，富搞预训练。</font>
- 31个作者，72页论文，320万token（一个batch），1700亿参数，暴力出奇迹，few-shot干翻SOTA，finetune都省了（当然也tune不动），有钱真好。- 计算量（flops）是BERT的两千多倍，训练一个BERT 1.2万美元, GPT-3训练下来大约花了**1200万刀**。难怪出了bug也不敢retrain，**地主家也没余粮**了。
- ![](https://pica.zhimg.com/80/v2-601de22700b3f16299cad6596b7c46e9_720w.jpg?source=1940ef5c)
- 参考：[Jsgfery](https://www.zhihu.com/question/398114261/answer/1253374136)


研究者们希望 GPT-3 能够成为更通用化的 NLP 模型，解决当前 BERT 等模型的两个不足之处：对领域内**有标记**数据的过分依赖，以及对于领域数据分布的过拟合。GPT-3 致力于能够使用**更少**的特定领域，不做 fine-tuning 解决问题。

GPT-3依旧延续自己的**单向**语言模型训练方式，只不过这次把模型尺寸增大到了1750亿，并且使用45TB数据进行训练。同时，GPT-3主要聚焦于更通用的NLP模型，解决当前BERT类模型的两个缺点：
- 对领域内有标签数据的过分依赖：虽然有了预训练+精调的两段式框架，但还是少不了一定量的领域标注数据，否则很难取得不错的效果，而标注数据的成本又是很高的。
- 对于领域数据分布的过拟合：在精调阶段，因为领域数据有限，模型只能拟合训练数据分布，如果数据较少的话就可能造成过拟合，致使模型的泛华能力下降，更加无法应用到其他领域。

因此GPT-3的主要目标是用更少的领域数据、且不经过精调步骤去解决问题。GPT-3一定程度上证明了**大力真的可以出奇迹**，无需fine-tuning就能在下游任务中“大显神威”。

预训练好的GPT-3探索了不同输入形式下的推理效果：
- ![](https://pic1.zhimg.com/80/v2-da41862b5628280989f1add7ad7aa2d4_720w.jpg)
- Zero-shot、One-shot、Few-shot都是完全不需要精调的，因为GPT-3是单向transformer，在预测新的token时会对之前的examples进行编码。
- 实验证明Few-shot下GPT-3有很好的表现: 量变引起的质变
  - ![](https://pic1.zhimg.com/80/v2-77f44d864f988f74bdc9c3f29fc043c0_720w.jpg)

传入文本作为输入，GPT输出，模型在训练期间扫描大量文本“学到”的东西产生的，3000亿个文本token的数据集用于生成模型的训练样本，训练是将模型暴露于大量文本的过程。现在看到的所有实验都来自该受过训练的模型。据估计，这需要花费355年的GPU时间，花费460万美元
- ![](https://pic1.zhimg.com/80/v2-675873e6eb879d499511e4d3113180a4_720w.jpg)

GPT3为2048个token。这就是它的“上下文窗口”。这意味着它有2048条轨道，沿着这些轨道处理token。

NLP可以说是实现AGI的最大难题，NLP的突破需要一个效果很好且通用的模型，GPT-3依凭借巨大的参数与算力已经极力接近这样的性质，在许多任务上（如翻译、QA和文本填空任务）拥有出色的性能甚至取得了SOTA。然而，GPT-3还是存在一些局限，论文作者给出了未来有前景的方向：建立GPT-3尺度的双向模型。使双向模型能在少样本、零样本学习上工作。

其它评论：
- GPT-3参数量再大，还是没有逃过任何一个普通两层全连接神经网络的缺点：
  - 灾难性遗忘
  - 独立同分布假设
- 1700亿参数的堆叠就会是智能的本质吗？**大一点的猴子，但还是猴子，不是人**。只是在量变并没有质变。
- 人工智能该到了谈信仰的时候了，上一次这样争论的内容是联结主义和符号主义。Judea Pearl的结构因果模型才是真正可以称得上智能的东西。GPT-3呢？仍然处于 Association 阶段，只是在寻找数据之间的相关性，并没有从因果的角度显式地给出文本之间可解释的内在逻辑。它做不到训练集分布外的延拓，做不到因果推断，更何谈智能。总而言之，GPT-3更像是深度学习在现有算力下的一次巅峰验证，只是一个顺应时代的产物，但绝不是我们对智能最终的解决方案。
- GPT-3不具备人类的感知思维，它的生成表现只是大数据训练的结果，无法超越数据本身，也无法拥有人类自成长型的广泛组合性推理的能力，所以，我们不如说它学会的是“统计层面的复制粘贴能力”。[知乎](https://www.zhihu.com/question/398114261/answer/1376204327)


## 模型结构

GPT(“Generative Pre-Training”)也叫**生成式**预训练模型，之所以说它超强但不秀的原因是作为NLP中极有价值的工作，比BERT出现的早，但是名声却远远不如BERT那么响亮。

GPT是典型的预训练+微调的两阶段模型。
- **预训练**阶段就是用海量的文本数据通过无监督学习的方式来获取语言学知识
- **微调**就是用下游任务的训练数据来获得特定任务的模型。

GPT预训练模型结构主要有两个重要的点：
- 一个是使用**Transformer**作为特征抽取器
- 另一个是使用**单向**的语言模型。

GPT与BERT关系
- ![](https://pic2.zhimg.com/80/v2-c5295b8541bce75b8468e42f639235a6_720w.jpg?source=1940ef5c)

- 原始GPT网络结构
  - ![](https://p6-tt.byteimg.com/origin/pgc-image/f3fcfe5dd66149a59d4adb1c82b5a812?from=pc)
- 常见文本生成
  - 并非所有英雄都穿 -> **斗篷**
- GPT生成
  - 并非所有英雄都披着斗篷 -> **但**
  - 并非所有英雄都披着斗篷 ，但-> **全部**
  - 并非所有英雄都披着斗篷，但全部 -> **恶棍**
  - 并非所有英雄都披着斗篷，但全部恶棍 -> **做**
- 说明
  - 1. 输入序列固定在2048个字（对于GPT-3）以内。将短序列作为输入时，只需用“空”值填充。
  - 2. GPT输出不仅是一次预测（概率），而是一系列预测（长度2048）（每个可能单词的概率）。序列中每个“next”位置都是一个预测。但是在生成文  时，通常只查看序列中最后一个单词的预测。
  - 3. 为了提高效率，GPT-3实际上使用字节级（byte-level）字节对编码（[BPE](https://huggingface.co/transformers  tokenizer_summary.html)）进行Token化。
  - 4. 对当前Token在序列中的位置进行编码，将Token的位置（标量i，在[0-2047]中）传递给12288个正弦函数，每个函数的频率都不同

![](https://p6-tt.byteimg.com/origin/pgc-image/f900defa52ba43f89260c42eaaee237a?from=pc)

## GPT-2

【2021-10-21】[图解GPT-2完整版](https://mp.weixin.qq.com/s?__biz=MzIyNjM2MzQyNg==&mid=2247539832&idx=1&sn=907c887c260a110cf5f0375cde6e6f9b&chksm=e8738d35df04042355802506243989770ebaa25ab4df3ff28e62d4ff2d862fda3b10c4a16968&mpshare=1&scene=23&srcid=1020ygRzRzn95VkxxlIb0njd&sharer_sharetime=1634742907130&sharer_shareid=b8d409494a5439418f4a89712efcd92a#rd)，[英文原文](http://jalammar.github.io/illustrated-gpt2/)
- GPT-2 不是一个特别新颖的架构，而是一种与 Transformer 解码器非常类似的架构。不过 GPT-2 是一个巨大的、基于 Transformer 的语言模型，它是在一个巨大的数据集上训练的。
- GPT-2 基本上就是键盘应用程序中预测下一个词的功能，但 GPT-2 比你手机上的键盘 app 更大更复杂。GPT-2 是在一个 40 GB 的名为 WebText 的数据集上训练的，OpenAI 的研究人员从互联网上爬取了这个数据集，作为研究工作的一部分。从存储空间大小方面来比较，我使用的键盘应用程序 SwiftKey，占用了 78 MB 的空间。而最小的 GPT-2 变种，需要 500 MB 的空间来存储它的所有参数。最大的 GPT-2 模型变种是其大小的 13 倍，因此占用的空间可能超过 6.5 GB。
- ![](http://p9.itc.cn/q_70/images03/20201111/dfb14796eddd4a4eac1f436e8d0041ec.png)
- ![](http://p8.itc.cn/q_70/images03/20201111/6bfcefd3a0d14eb1be6fac226a89c756.png)
- GPT-2体验：AllenAI [GPT-2 Explorer](https://gpt2.apps.allenai.org/)。它使用 GPT-2 来显示下一个单词的 10 种预测（包括每种预测的分数）。你可以选择一个单词，然后就能看到下一个单词的预测列表，从而生成一篇文章。

模型结构
- BERT 是使用 Transformer 的 Encoder 模块构建，而GPT-2 用 Transformer 的 Decoder 模块构建。
- 一个重要差异是，GPT-2 和传统的语言模型一样，一次输出一个 token
- GPT-2和后来的一些模型如 TransformerXL 和 XLNet，本质上都是自回归的模型。但 BERT 不是自回归模型。这是一种权衡。去掉了自回归后，BERT 能够整合左右两边的上下文，从而获得更好的结果。XLNet 重新使用了 自回归，同时也找到一种方法能够结合两边的上下文。
- ![](http://p2.itc.cn/q_70/images03/20201111/b01d3ba72549484ea085877e173e8da5.gif)
- 更多资料见原文

## GPT3工作原理

- [How GPT3 Works - Visualizations and Animations](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)，汉化版：[图解GPT3的工作原理](https://zhuanlan.zhihu.com/p/344695943)


GPT3进行微调后，会更加惊人。微调实际上会更新模型的权重，以使模型在某些任务上表现更好

<iframe src="https://vdn1.vzuu.com/SD/8741ab12-57a8-11eb-ad57-02310f44807a.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  height="600" width="100%"> </iframe>



## 被玩high的GPT-3


- GitHub项目中的50种玩法，感兴趣的同学们可以继续探索。

- 1、生成应用和布局
    - 根据描述生成HTML布局和代码
    - 根据描述创建UI设计
    - 根据描述生成React代码创建待办事项清单应用
    - 仅基于React变量名称生成component
    - 根据颜色名称或表情符号生成色阶
    - 根据描述创建网站
- 2、搜索和数据分析
    - 问题解答和搜索引擎
    - 扩充表中的信息
    - 根据描述创建图表
    - 根据描述生成代码并转换为电子表格
    - 根据描述生成图表和代码
- 3、程序生成与分析
    - 根据描述生成shell命令
    - 阅读代码并回答相关问题
    - 根据描述生成LaTeX表达式
    - 根据问题描述生成SQL代码_1
    - 根据问题描述生成SQL代码_2
    - 编码面试
    - 生成Python代码回答自然语言问题
    - 生成特定数据库的SQL代码
    - 根据描述生成机器学习代码
- 4、文本生成
    - 语言翻译
    - 将日常语言转换为法律语言
    - 自动生成请求
    - 根据关键词写完整的回复邮件
    - 简化法律语言
    - 翻译中文非文学诗歌
    - 将句子改写得更礼貌
    - 总结名著思想
    - 以大五人格（外向性、开放性、宜人性、尽责性、神经质）控制GPT-3的语言风格
- 5、内容创作
    - 营销内容创作
    - 生成模因，模仿创作
    - 撰写Google广告
    - 生成图片说明
    - 根据描述生成食谱
    - 根据“如何有效召开董事会会议”写“如何招募董事会成员”
    - 生成莎士比亚风格的诗歌
    - 生成科学问题并回答
    - 生成历史问题并回答
    - 文本补全和风格化重写
- 6、一般推理
    - 物理问题
    - 数学问题
    - 医学问题
    - 无意义的问题
    - 推理问题
    - 多步骤处理问题
    - 通过图片确定食品成分和健康性
    - 日常用语翻译成正式表达
- 7、其他
    - GPT-3下棋
    - 使用自然语言设计交互式语音应答流
    - 通过临床症状对患者进行诊断

应用案例：
- 1、根据描述生成HTML布局和代码：根据输入的自然语言描述生成HTML网页布局，以及相应代码。
    - ![](https://p1-tt.byteimg.com/origin/pgc-image/S6FOh7mE73PNC1?from=pc)
- 2、根据描述创建UI设计：输入文字描述，就可以生成相应的UI界面，跟上一个类似，不过界面更适应手机操作系统
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOhi7Ffa0ki6?from=pc)
- 3、扩充表中的信息
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOhiu5j0rUax?from=pc)
- 4、根据描述生成图表和Python代码
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOiGE9VukEYd?from=pc)
- 5、根据描述生成LaTeX表达式
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOiGyIe28RJk?from=pc)
- 6、根据问题描述生成SQL代码
    - ![](https://p1-tt.byteimg.com/origin/pgc-image/S6FOiHNAhX0Ebi?from=pc)
- 7、根据描述生成机器学习代码：GPT-3还能写自己同类的代码，比AutoML还AutoML
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOiHu6Sat7pt?from=pc)
- 8、编码面试
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOiKUAtRoQdm?from=pc)
- 9、将日常语言转换为法律语言
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj84Ij3iIs1?from=pc)
- 10、根据关键词写完整的回复邮件
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj8RDFq98C0?from=pc)
- 11、将句子改写得更礼貌
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj8v4Ngc0XH?from=pc)
- 12、总结名著思想
    - ![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOj97FXhmLjI?from=pc)
- 13、生成科学问题并回答
    - ![](https://p3-tt.byteimg.com/origin/pgc-image/S6FOjqo2eoEbhh?from=pc)
- 14、推理问题


## 思考

- OpenAI的创始人Sam Altman也认为GPT-3被过度炒作，在推特上表示：“ GPT-3的炒作实在太多了。它仍然存在严重的缺陷，有时还会犯非常愚蠢的错误。”

问题
- GPT-3还是一个依赖算力和大数据的怪兽。GPT-3的训练需要花费355GPU年和460万美元，数据集包含3000亿个文本token，存储量高达45TB，参数数量更是达到1750亿，而GPT-2的参数数量是15亿。
- 最近的流行也不能忽视心理学效应的影响
- 但是，GPT-3的few-shot 学习能力不是通用的，尽管该模型在复杂任务和模式的学习上给人留下了深刻的印象，但它仍然可能会失败。例如，即使看过10,000个示例，也解决不了反写字符串那样简单的任务。
- 即使是OpenAI，也曾指出GPT-3存在缺陷，GPT-3的原始论文就提供了一些证据，证明GPT-3无法执行复杂的逻辑推理。
- GPT3的宽度为2048个token，这是它理解上下文的极限，而人类可以记住多本书的知识，并将其关联起来，在这方面，GPT-3还差得远。
- GPT-3的生成结果表现出的灵活性是大数据训练的结果，它无法超越数据本身，也就无法拥有组合性推理能力，不如说，它学到的是“统计层面的复制粘贴能力”。

![](https://p6-tt.byteimg.com/origin/pgc-image/S6FOjrg9iyHuW9?from=pc)

- 【2020-8-15】[强大如 GPT-3，1750 亿参数也搞不定中国话](https://www.infoq.cn/article/l7bhKDEolj06Y9dEwJ6O)
    - 魏晨：GPT-3 模型从看上去更加接近“通用人工智能”(AGI) ，可以动态学习，处理多种不同的任务，只需少量的标注数据。

重点：
1. GPT-3 参数庞大（约 1750 亿参数），能力较之前确实有所提升，但是宣传效果有夸张成分；
2. 受参数大小影响，GPT-3 并不是一款性价比很高的模型，训练成本较高；
3. 中文 GPT-3 的实践尚未出现；
4.GPT-3 确实可以通过文字输入生成代码，但是仅限于比较简单的情况；
5. 离 AI 真正替代程序员工作， 还有较长的路要走 。

## 中文GPT

### 好玩儿的案例

【2021-10-14】[爆肝100天，我开发了一个会写作文的人工智能【17亿参数、2亿数据、1万行代码】](https://www.bilibili.com/video/BV1pr4y1w7uM) EssayKiller
- 一个基于OCR、NLP领域模型所构建的生成式文本创作AI框架，目前第一版finetune模型针对高考作文（主要是议论文），可以有效生成符合人类认知的文章，多数文章经过测试可以达到正常高中生及格作文水平。视频中有部分细节为了方便非AI专业的观众理解，以及为了更好的节目效果，做的略有不严谨。由于要控制时长我没有展开讲，业内大佬们见谅。技术上的问题欢迎[Github](https://github.com/EssayKillerBrain/EssayKiller_V2/tree/2.0)

<iframe src="//player.bilibili.com/player.html?aid=755124609&bvid=BV1pr4y1w7uM&cid=249390460&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

### CPM 清华智源

【2020-11-17】[中文版GPT-3来了？智源研究院发布清源 CPM —— 以中文为核心的大规模预训练模型](https://www.cn-healthcare.com/articlewm/20201117/content-1163510.html)
- ![](http://files.cn-healthcare.com/upload/20201117/wximg/38391605568279885)
- ![](http://files.cn-healthcare.com/upload/20201117/wximg/4751605568279966)
- ![](http://files.cn-healthcare.com/upload/20201117/wximg/94871605568280187)
- [CPM清华大学演示使用过程小说语句生成](https://www.bilibili.com/video/BV1VA411s77D/)
- <iframe src="//player.bilibili.com/player.html?aid=330632724&bvid=BV1VA411s77D&cid=268856252&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width='800' height='600'> </iframe>

北京智源人工智能研究院和清华大学研究团队合作开展大规模预训练模型，并发布[清源CPM](https://cpm.baai.ac.cn/) (Chinese Pretrained Models) 研究计划，旨在推动中文自然语言处理的研究与应用。2020 年 11 月中旬，CPM 开放第一阶段的26 亿参数规模的中文语言模型 (CPM-LM) 和217亿参数规模的结构化知识表示模型 (CPM-KM) 下载，以及相应的系统演示。

清源 CPM 大规模预训练模型具有以下特点： 
1. 学习能力强：能够在多种自然语言处理任务上，进行**零次**学习或**少次**学习达到较好的效果。
2. 语料丰富**多样**：收集大量丰富多样的中文语料，包括百科、小说、对话、问答、新闻等类型。
3. 行文自然流畅：基于给定上文，模型可以续写出一致性高、可读性强的文本，达到现有中文生成模型的领先效果。
4. 模型规模大：本次发布的 CPM-LM 的参数规模为 26 亿，预训练中文数据规模100 GB，使用了 64 块 V100 GPU 训练时间约为 3 周。CPM-KG 的参数规模分别为217亿，预训练结构化知识图谱为 WikiData 全量数据，包含近 1300 个关系、8500万实体、4.8 亿个事实三元组，使用了 8 块 V100 GPU 训练时间约为 2 周。

资料
- [清源CPM主页](https://cpm.baai.ac.cn/)
- 清源CPM [Github](https://github.com/TsinghuaAI/)
- 预训练模型必读[论文列表](https://github.com/thunlp/PLMpapers)
- [清源 CPM-中文GPT3-我魔改出了一个TF版本](https://zhuanlan.zhihu.com/p/297152907)


### PLUG——阿里巴巴达摩院

[PLUG测试地址](https://nlp.aliyun.com/portal#/BigText_chinese)
- ![](https://pic1.zhimg.com/80/v2-9abea76b517e3ab3f4e24dbeddf4ced8_720w.jpg)

【2021-4-19】[达摩院用128张GPU烧出“中文版GPT-3”，我试了下，这文风不是开往幼儿园的车…](https://zhuanlan.zhihu.com/p/365999690)

PLUG，Pre-training for Language Understanding and Generation，顾名思义，就是集语言理解（NLU）和生成（NLG）能力于一身。要实现这一点，据团队介绍，这一模型是达摩院此前提出的两种自研模型——NLU语言模型StructBERT、NLG语言模型PALM的融合。

此外，跟GPT-3的单向建模方式不同的是，它采用了编码器-解码器（encoder-decoder）的双向建模方式。整个训练过程分为两个阶段。
- 第一阶段，以达摩院自研的语言理解模型——StructBERT作为编码器。简单来说，它是在句子级别和词级别两个层次的训练目标中，加强对语言结构信息的建模，从而提高模型的语法学习能力。这也使得PLUG具有输入文本双向理解能力，能够生成和输入更相关的内容。这个过程共训练了300B tokens训练数据。
- 第二阶段，将这个编码器用于生成模型的初始化，并外挂一个6层、8192个隐藏层节点数的解码器，共计训练了100B tokens的训练数据。
- ![](https://pic2.zhimg.com/80/v2-ce80eff0eaf1d9e3d1aec364a1a3904d_720w.jpg)

PLUG还能为目标任务做针对性优化。GPT-3并没有利用**微调**和**梯度更新**，而是通过指定任务、展示少量演示，来与模型文本进行交互，完成各种任务。因此在面对新任务时候，不需要重新收集大量的带标签数据。但不可避免的，生成的效果不足。比如，**犯低级错误**就是GPT-3被人诟病比较多的一点。而PLUG的能力更加全面，既可以实现与GPT-3类似的**零样本**生成功能，也可以利用下游训练数据微调（finetune）模型，提升特定任务的生成质量。

当然，效果实现的关键，还少不了算力和数据。PLUG负责人表示，原本计划用128张A100训练120天炼成，不过由于阿里云、算法优化等达摩院多方力量的参与，以及加速手段的有效利用，成功将日程缩短到三分之一。最后，只烧了35天就达到了这样的效果。前面也提到，PLUG的参数量达到了270亿，中文训练数据量也达到了1T以上。在语言理解任务上，PLUG以80.614分刷新了CLUE分类任务榜单记录。而在语言生成任务上，据团队介绍，其多项应用数据较业内最优水平提升了8%以上。
- ![](https://pic4.zhimg.com/80/v2-804a587190c5cc17c24cb453b96ec3e3_720w.jpg)

耗时3个月、270亿参数规模、一发布就给体验端口

去年，阿里达摩院发布了自研深度语言模型体系，包括6大自研模型。
- **通用**语言模型StructBERT
- **多模态**语言模型StructVBERT
- **多语言**模型VECO
- **生成式**语言模型PALM……
他们一直在致力于陆陆续续将模型开源出来。


### 彩云小梦

[彩云小梦](https://if.caiyunai.com/dream/#/)
- ![](https://pic2.zhimg.com/80/v2-acb86090e26d23f3462b7ff43afef379_720w.jpg?source=1940ef5c)

总结：
- 小梦熟悉小说写作的各种套路，它有着不错的脑洞，能够一定程度上理解前文的脉络，并且不失时机地运用它知道的写作手法。
- 不过，它的缺点也是明显的，依然是缺少常识。这导致它在遣词造句上，会写出不符合人类习惯的奇怪句子。
- 不过小梦显然是值得期待的。甚至现在的网文作者，已经可以把小梦当作工具，在一些特定的场景里，帮助作者寻找情节的突破口。小梦写得还不够好，但它肯定看过的文章比任何人都多，未来可期。


# 结束