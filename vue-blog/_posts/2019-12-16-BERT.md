---
layout: post
title:  BERT语言模型
date:   2019-12-16 16:52:00
categories: 深度学习 自然语言处理
tags: NLP Transformer BERT Attention ALBERT 可视化 gpu 迁移学习 sentence
excerpt: 预训练语言模型及BERT知识点汇总
mathjax: true
permalink: /bert
---

* content
{:toc}


# BERT

- 2018年11月底，谷歌发布了基于双向 `Transformer` 的大规模预训练语言模型 `BERT`，该预训练模型能高效抽取文本信息并应用于各种 NLP 任务，该研究凭借预训练模型刷新了 11 项 NLP 任务的当前最优性能记录。
- 技术博主 Jay Alammar 近日发文[illustrated-bert](https://jalammar.github.io/illustrated-bert/)，通过图解方式生动地讲解了 BERT 的架构和方法基础。
- 2018 年是机器学习模型处理文本（更准确地说是自然语言处理，简称 NLP）的一个转折点。
- ![](https://image.jiqizhixin.com/uploads/editor/5442b9f1-17ca-49b3-a259-e5fb52107534/1544732034404.png)
- ![](https://image.jiqizhixin.com/uploads/editor/87b820e3-dc5c-4f9f-97f6-6a01360156b7/1544732034725.png)

下游任务
- ![](https://image.jiqizhixin.com/uploads/editor/41afd366-28b8-4aa1-8464-5f10d253cb48/1544732037865.png)

## BERT介绍

语言模型的预训练用在下游任务的策略主要有两种：
- 基于**特征**（feature-base）：也就是**词向量**，预训练模型训练好后输出的词向量直接应用在下游模型中。
  - 如ELMo用了一个**双向LSTM**，一个负责用前几个词预测下一个词，另一个相反，用后面几个词来预测前一个词，一个从左看到右，一个从右看到左，能够很好地捕捉到上下文的信息，不过只能输出一个词向量，需要针对不同的下游任务构建新的模型。
- 基于**微调**（fine-tuning）：先以**自监督**的形式预训练好一个很大的模型，然后根据下游任务接一个**输出层**，不需要再重新去设计模型架构
  - 如OpenAI-GPT，但是GPT用的是一个**单向transformer**，训练时用前面几个词来预测后面一个词，只能从左往右看，不能够很好的捕捉到上下文的信息。
ELMo虽然用了两个单向的LSTM来构成一个双向的架构，能够捕捉到上下文信息，但是只能输出词向量，下游任务的模型还是要自己重新构建，而GPT虽然是基于微调，直接接个输出层就能用了，但是是单向的模型，只能基于上文预测下文，没有办法很好的捕捉到整个句子的信息。

**BERT**（Bidirectional Encoder Representations from Transformers）把这两个模型的思想融合了起来
- 首先，用基于**微调**的策略，在下游有监督任务里面只需要换个输出层就行
- 其次，训练时用了一个transformer的encoder来基于**双向**的上下文来表示**词元**

ELMo、GPT和BERT的区别
- ![](https://pic2.zhimg.com/80/v2-59c61eca79848c14285c302564cfe3d9_720w.jpg)

BERT很好的融合了ELMo和GPT的优点，论文中提到在11种自然语言处理任务中（文本分类、自然语言推断、问答、文本标记）都取得了SOTA的成绩。

## BERT结构

BERT结构
- ![](https://pic1.zhimg.com/80/v2-b80e8506c78fbf64fe80b8203abff7e8_1440w.jpg)

### 输入格式

Bert 的输入格式如下：

```
tokens     : [CLS] Sentence 1 [SEP] Sentence 2 [SEP]
input_ids  :  101     3322     102     9987     102
segment_ids:   0        0       0        1       1
```

文本特征
- **单句**或者**句子对**(或叫上下句)经过 Word Piece Tokenize 后变成 tokens，然后通过一个**词表**(vocab)被映射成 input_ids。
- 上下句信息会被编码成 segment_ids，连同 input_ids 一起进入 Bert
- 经过多层 transformer 被转换成 Vector Representation，文本信息也就被编码到向量中了。

注意
- tokens 总长度的**限制**(Seq Length)：
  - 一方面是**硬性限制**，即不能超过 512；
  - 另一方面是**资源限制**，GPU 显存消耗会随着 Seq Length 的增加而显著增加。
- Google 给出的在单个 Titan X GPU(12G RAM) 上 Bert 的资源消耗情况。
  - ![](https://pic2.zhimg.com/80/v2-ebd0eb2aab87ca8357953bdce3f608f5_1440w.jpg)

真实的数据集中，每条数据的文本长度是不相同的，一般呈现**长尾分布**，需要简单分析后确定一个 Seq Length。
- 以 Google QUEST Q&A Labeling 比赛为例，文本特征包括 question_title、question_body 和 answer，tokenize 后分布
- ![](https://pic3.zhimg.com/80/v2-7f5802f65d1e427daebdbc61d84b75c2_1440w.jpg)
- ![](https://pic1.zhimg.com/80/v2-983e93c94c883f2d0f6770b60e60e300_1440w.jpg)
- 使用 question_title + question_body + answer 的方式，则有75%左右的样本满足512的长度限制，另外**25%**需要做truncate截断处理；
- 使用 question_title + question_body 和 answer 分开编码的方式，则有92%左右的样本满足512的长度限制，不过这时候需要使用两个 Bert；

常用的截断的策略有三种：
- pre-truncate
- post-truncate
- middle-truncate (head + tail)：最优

研究对比了三种策略的效果，head + tail 最优，这也符合直觉，一般文章或段落的**开头结尾**往往会包含重要信息，截掉中间部分信息丢失最少。
- 当然，这也不是绝对的，建议三种方法都尝试，比较后挑选，或者都保留，后期做融合。
- 另外，也可以根据具体的场景**自定义**截断策略，比如在上面的例子中，可以通过设定 question_title 不超过 50，question_body 不超过 200，answer 不超过 200 等。

### 特殊符号

在 Bert 输入端，通过添加 special tokens

Google QUEST Q&A Labeling 比赛举例，比如对于每条问答数据，有一个类别信息，类别可以是 culture、science 等，可以设计两个 special tokens [CAT=CULTURE] 和 [CAT=SCIENCE] 加入到文本中：

```
tokens     : [CLS] [CAT=CULTURE] question [SEP] answer [SEP]
input_ids  :  101       1          3322    102   9987   102
segment_ids:   0        0           0       0      1     1
```

当然，新增的 special tokens 需要相应地在词表中添加，实现方法也很简单，具体可以查看相关框架的文档。

### 输出

在 Bert 输出端
- 直接做 embedding
- 另一种方式是直接在 Bert 的输出端对 meta 特征进行 embedding，然后与文本特征的 Vector Representation 进行融合。

实现方法也非常简单，下面是一个简单的代码示意：

```python
emb = nn.Embedding(10, 32)    # 初始化一个 Embedding 层
meta_vector = emb(cat)    # 将类别编码成 vector
logits = torch.cat([txt_vector, meta_vector], dim=-1)    # 文本向量和类别向量融合
两种方式都可以尝试
```

### 向量表示 Vector Representation

文本特征经过 Encoder 编码后变成了啥？
- 以12层的 bert-base 为例，得到了 transformer 12层的 hidden states，每一层的维度为 B x L x H (B 表示 batch size，L 表示 Seq Length，H 表示 Hidden dim)。一般认为最后一层的第一个 token (也即 \[CLS]) 对应的向量可以作为整个句子(或句子对)的向量表示，也即，包含了从文本中提取的有效信息。但在比赛中可以看到各种花式操作，并且都得到了明显的效果提升，比如：
- 取最后一层所有 token 对应的向量做融合；
- 取所有层的第一个 token 对应的向量做融合；
- 取最后四层的所有 token 对应的向量，加权重(可学习)融合；
- ...

![](https://pic3.zhimg.com/80/v2-0a675ad3cd5c24a6f3338b01f2554bc2_1440w.jpg)

### 分类器 Classifier

一般为一层或多层的全连接网络，最终把特征的向量表示映射到 label (target) 维度上。这部分比较灵活，可以做很多尝试，比如使用不同的激活函数、Batch Normalization、dropout、使用不同的损失函数甚至根据比赛 metric 自定义损失函数等。

广泛使用的小技巧：

（1）**Multi-Sample Dropout**

使用连续的dropout，可以加快模型收敛，增加泛化能力，详细见论文，代码实现如下：

```python
dropouts = nn.ModuleList([
    nn.Dropout(0.5) for _ in range(5)
])

for j, dropout in enumerate(dropouts):
    if j == 0:
        logit = self.fc(dropout(h))
    else:
        logit += self.fc(dropout(h))
```

（2）**辅助任务**

多任务学习在 NLP 领域也很常见，通过设计一些与目标任务相关的辅助任务，有时也可以获得不错的提升。在 Jigsaw Unintended Bias in Toxicity Classification 比赛中，1st place solution 就采用了辅助任务的方法。
- ![](https://pic4.zhimg.com/v2-47e66fc3817e1d2ceb5dfed653b3f323_r.jpg)

图中 Target 是比赛要预测的目标，is male mentioned、is christian mentioned 等是一些**辅助目标**，通过这些辅助目标提供的监督信号，也可以对模型的训练和最终效果提供帮助。

### 采样

当一种神经网络框架很强大时(比如 transformer 结构)，对网络结构的小的改进(比如在 classifier 中增加几层全连接层、改变一下激活函数、使用dropout等)收益都是非常小的，这时候大的提升点往往是在数据层面。

### 文本扩增

比较常用且有效的方法是基于翻译的，叫做 back translation （**回译法**）。原理简单，是将文本翻译成另一种语言，然后再翻译回来，比如：

```shell
# 原始文本
Take care not to believe your own bullshit, see On Bullshit.
# EN -> DE -> EN
Be careful not to trust your own nonsense, see On Bullshit.
# EN -> FR -> EN
Be careful not to believe your own bullshit, see On Bullshit.
# EN -> ES -> EN
Be careful not to believe in your own shit, see Bullshit.
```

因为 Google 翻译已经做到了很高的准确率，所以 back translation 基本不会出现太大的语义改变，不过需要有办法搞到 Google 翻译的 api，目前似乎没有免费可用的了。

## BERT要点

Bert细节：
- 在输入上，Bert的输入是两个segment，其中每个segment可以包含多个句子，两个segment用\[SEP]拼接起来。
- 模型结构上，使用 Transformer，这点跟Roberta是一致的。
- 学习目标上，使用两个目标：
  1. Masked Language Model(`MLM`) **掩码语言模型**: 其中**15%**的token要被Mask，在这15%里，有80%被替换成\[Mask]标记，有10%被随机替换成其他token，有10%保持不变。
  1. Next Sentence Prediction(`NSP`) **下一句预测**: 判断segment对中第二个是不是第一个的后续。随机采样出**50%**是和50%不是。
- Optimizations 算法优化:
  - Adam, beta1=0.9, beta2=0.999, epsilon=1e-6, L2 weight decay=0.01
  - learning rate, 前10000步会增长到1e-4, 之后再**线性下降**。
  - dropout=0.1
  - GELU激活函数
  - 训练步数：1M
  - mini-batch: 256
  - 输入长度: 512
- Data
  - BookCorpus + English Wiki = 16GB


## BERT进阶思考

### BERT学到了什么

ACL 2019最新收录的论文：[What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document) [理解BERT每一层都学到了什么](https://zhuanlan.zhihu.com/p/74515580)， [代码](https://github.com/ganeshjawahar/interpret_bert)
- Frege早在1965年的组合原则里谈到，复杂表达式的意义由其子表达式的意义以及意义如何组合的规则共同决定。
- 本文思路与分析卷积神经网络每层学习到的表征类似，主要是探索了BERT的每一层到底捕捉到了什么样的信息表征。作者通过一系列的实验证明BERT学习到了一些**结构化**的语言信息，比如
  - BERT的**低层**网络就学习到了**短语**级别的**信息表征**
  - BERT的**中层**网络就学习到了丰富的**语言学**特征
  - BERT的**高层**网络则学习到了丰富的**语义**信息特征。
- [图示](https://pic2.zhimg.com/80/v2-602f7d353a057e56327a631e396934b1_720w.jpg)
  - ![](https://pic2.zhimg.com/80/v2-602f7d353a057e56327a631e396934b1_720w.jpg)

### BERT可视化

【2022-7-22】[最全深度学习训练过程可视化工具](https://mp.weixin.qq.com/s/QQgKfYi-m9psUOJUZUxZ0Q)
- 深度学习训练过程一直处于黑匣子状态，有很多同学问我具体怎么解释？其实很多还是无法可解释，但是通过可视化，具体可以知道深度学习在训练过程到底学习了哪些特征？到底对该目标的哪些特征感兴趣？
  - 1.深度学习网络结构画图工具，[地址](https://cbovar.github.io/ConvNetDraw/)
  - 2.caffe可视化工具,输入：caffe配置文件 输出：网络结构; [地址](http://ethereon.github.io/netscope/#/editor)
  - 3.深度学习可视化工具Visual DL; Visual DL是百度开发的，基于echar和PaddlePaddle，支持PaddlePaddle，PyTorch和MXNet等主流框架。ps：这个是我最喜欢的，毕竟echar的渲染能力不错哈哈哈，可惜不支持caffe和tensorflow。[地址](https://github.com/PaddlePaddle/VisualDL)
  - 4.结构可视化工具PlotNeuralNet, 萨尔大学计算机科学专业的一个学生开发。[地址](https://github.com/HarisIqbal88/PlotNeuralNet)
  - [CNN Explainer](https://poloclub.github.io/cnn-explainer/) [github](https://github.com/poloclub/cnn-explainer), 交互可视化CNN类神经网络，使用 TensorFlow.js 加载预训练模型进行可视化效果，交互方面则使用 Svelte 作为框架并使用 D3.js 进行可视化。最终的成品即使对于完全不懂的新手来说，也没有使用门槛。
  - [VisualDL](https://github.com/PaddlePaddle/VisualDL), a visualization analysis tool of PaddlePaddle；类似tensorboard
    - ![](https://user-images.githubusercontent.com/48054808/103188111-1b32ac00-4902-11eb-914e-c2368bdb8373.gif)
- NLP可视化
  - [NLPReViz](https://nlpreviz.github.io/): An Interactive Tool for Natural Language Processing on Clinical Text
  - [NLPVis](https://github.com/shusenl/nlpvis) web系统，visualize the attention of neural network based natural language models.
    - ![](https://github.com/shusenl/nlpvis/raw/master/teaser.png?raw=true)
  - [Visualizing BERT](https://home.ttic.edu/~kgimpel/viz-bert/viz-bert.html)
  - kaggle上外国人分享的[Visualizing BERT embeddings with t-SNE](https://www.kaggle.com/wqw547243068/visualizing-bert-embeddings-with-t-sne/edit)
  - SHAP
    - ![](https://www.nowhere.co.jp/blog/wp-content/uploads/2022/07/SHAP-for-translation.png)
  - [BertViz](https://github1s.com/jessevig/bertviz)是BERT可视化工具包，支持[transformers](https://github.com/huggingface/transformers) 库的大部分模型 (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)，继承于[Tensor2Tensor visualization tool](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization)

#### BERTViz

```shell
# pip安装
pip install bertviz
# 源码安装
git clone https://github.com/jessevig/bertviz.git
cd bertviz
python setup.py develop
```

功能：
- headview：指定层的注意力层可视化
  - [interactive Colab Notebook](https://colab.research.google.com/drive/1PEHWRHrvxQvYr9NFRC-E_fr3xDq1htCj)
  - ![](https://upload-images.jianshu.io/upload_images/22279029-4a9d09dd11e565a3.png)
- modelview：整个模型的注意力可视化
  - [interactive Colab Notebook](https://colab.research.google.com/drive/1c73DtKNdl66B0_HF7QXuPenraDp0jHRS)
  - ![](https://upload-images.jianshu.io/upload_images/22279029-59e2434e45d87166.png)
- neuralview：QKV神经元可视化, 及如何计算注意力
  - [interactive Colab Notebook](https://colab.research.google.com/drive/1m37iotFeubMrp9qIf9yscXEL1zhxTN2b)
  - ![](https://upload-images.jianshu.io/upload_images/22279029-7088fd1e5fef41e5.png)


```python
from transformers import AutoTokenizer, AutoModel
from bertviz import model_view

model_version = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_version)
model = AutoModel.from_pretrained(model_version, output_attentions=True)

inputs = tokenizer.encode("The cat sat on the mat", return_tensors='pt')
outputs = model(inputs)
attention = outputs[-1]  # Output includes attention weights when output_attentions=True
tokens = tokenizer.convert_ids_to_tokens(inputs[0]) 

# head可视化
from bertviz import head_view
head_view(attention, tokens)
head_view(attention, tokens, layer=2, heads=[3,5])

# model可视化
model_view(attention, tokens)
model_view(attention, tokens, display_mode="light") # 背景设置
model_view(attention, tokens, include_layers=[5, 6]) # 只显示5-6层

# neural可视化
# Import specialized versions of models (that return query/key vectors)
from bertviz.transformers_neuron_view import BertModel, BertTokenizer

from bertviz.neuron_view import show

model = BertModel.from_pretrained(model_version, output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)
model_type = 'bert'
show(model, model_type, tokenizer, sentence_a, sentence_b, layer=2, head=0)

```

jupyter notebook版本：

```python
# bert可视化  pip install bertviz
from transformers import BertTokenizer, BertModel
#from bertv_master.bertviz import head_view
from bertviz import head_view


# 在 jupyter notebook 显示visualzation 
def call_html():
  import IPython
  display(IPython.core.display.HTML('''<script src="/static/components/requirejs/require.js"></script><script>// <![CDATA[
requirejs.config({ paths: { base: '/static/base', "d3": "https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min", jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min', }, });
// ]]></script>'''))

# 記得我们是使用中文 BERT
model_version = 'bert-base-chinese'
model = BertModel.from_pretrained(model_version, cache_dir="./transformers/", output_attentions=True)
tokenizer = BertTokenizer.from_pretrained(model_version)
 
# 情境 1 的句子
sentence_a = "老爸叫小宏去买酱油，"
sentence_b = "回来慢了就骂他。"
 
# 得到tokens后输入BERT模型获取注意力权重(attention)
inputs = tokenizer.encode_plus(sentence_a,sentence_b,return_tensors='pt', add_special_tokens=True)
token_type_ids = inputs['token_type_ids']
input_ids = inputs['input_ids']
attention = model(input_ids, token_type_ids=token_type_ids)[-1]
input_id_list = input_ids[0].tolist() # Batch index 0
tokens = tokenizer.convert_ids_to_tokens(input_id_list)
call_html()

# 用BertViz可视化
head_view(attention, tokens)
```


#### SHAP

SHapley Additive exPlanations 

![](https://www.nowhere.co.jp/blog/wp-content/uploads/2022/07/SHAP-for-translation.png)

All the code and outputs are provided at the end of this blood. Please refer to it while reading and feel free to run it in [Google Colab notebook](https://colab.research.google.com/gist/Cheto01/1285f09b81d043856b6ba2b13f2ec942/shap-example.ipynb)


（1）Explaining a transformers-based model —— 分类模型

SHAP or ECCO have the potential to provide substantial syntactic information captured from an NLP model’s attention.

SHAP can be used to visualize and interpret what a complex transformers-based model sees when applied to a classification task.

```python
#import the necessary libraries
import pandas as pd
import shap
import sklearn

import transformers
import datasets
import torch
import numpy as np
import scipy as sp

# load a BERT sentiment analysis model
tokenizer = transformers.DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")
model = transformers.DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased-finetuned-sst-2-english"
).cuda()

# define a prediction function
def f(x):
    tv = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=500, truncation=True) for v in x]).cuda()
    outputs = model(tv)[0].detach().cpu().numpy()
    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T
    val = sp.special.logit(scores[:,1]) # use one vs rest logit units
    return val

# build an explainer using a token masker
explainer = shap.Explainer(f, tokenizer)

# explain the model's predictions on IMDB reviews
imdb_train = datasets.load_dataset("imdb")["train"]
shap_values = explainer(imdb_train[:30], fixed_context=1, batch_size=2)

# plot a sentence's explanation
shap.plots.text(shap_values[27])
```

This visualization uses the probability scores of the model’s prediction. By taking into account the underlying patterns inside the layers, it uses the neuron activations values, more precisely the non-negative matrix factorization.


（2）NLP Model for Translation

Imagining what happens inside a sequence-to-sequence model could be described easier than a classifier. Our intuition for a translation could even show us more precisely which token or words have been mistranslated.

By hovering over the translated text, you will notice that those neuron activations carry embeddings scores. The non-latent embedding scores can be translated into embedding space where word similarity can easily be visualized. For example, in the embedding space, for “father”, “brother” will be what “sister” is for “mother”. This can be visualized even through a sentiment classification where each word is represented by its embedding score.  

```python
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import shap
import torch

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-fr")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-fr").cuda()

s=["In my family, we are six: my father, my mother, my elder sister, my younger brother and sister"]

explainer = shap.Explainer(model,tokenizer)

shap_values = explainer(s)
```


### 语义匹配

#### sentence BERT

【2020-3-25】[Sentence-Bert论文笔记](https://zhuanlan.zhihu.com/p/113133510)
- 论文: [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)，代码 [sentence-transformers](https://github.com/UKPLab/sentence-transformers)

Bert模型已经在NLP各大任务中都展现出了强者的姿态。在**语义相似度计算**（semantic textual similarity）任务上也不例外，但由于bert模型规定，在计算语义相似度时，需要将两个句子同时进入模型，进行信息交互，这造成大量的计算开销。
- 例如，有1w个句子，我们想要找出最相似的句子对，需要计算（10000*9999/2）次，需要大约**65小时**。
Bert模型的构造使得它既不适合**语义相似度**搜索，也不适合**非监督**任务，比如聚类。
- 问答系统任务中，往往会人为配置一些常用并且描述清晰的问题及其对应的回答，即“**标准问**”。当用户进行提问时，常常将用户的问题与所有配置好的标准问进行相似度计算，找出与用户问题最相似的标准问，并返回其答案给用户，这样就完成了一次问答操作。
- 如果使用bert模型，那么每次用户问题都需要与标准问库计算一遍。在实时交互的系统中，是不可能上线的。
Sentence-BERT网络结构可以解决bert模型的不足。简单通俗地讲，借鉴**孪生网络**模型的框架，将不同的句子输入到两个bert模型中（但这两个bert模型是参数共享的，也可以理解为是同一个bert模型），获取到每个句子的句子表征向量；而最终获得的**句子表征向量**，可以用于**语义相似度计算**，也可以用于无监督**聚类**任务。
- 对于同样的10000个句子，我们想要找出最相似的句子对，只需要计算10000次，需要大约5秒就可计算完全。从65小时到5秒钟，这真是恐怖的差距。

Sentence-BERT网络结构
- 文中定义了三种通过bert模型求句子向量的策略，分别是**CLS向量**，**平均**池化和**最大值**池化。
- （1）**CLS向量**策略，就是将bert模型中，开始标记【cls】向量，作为整句话的句向量。
- （2）**平均池化**策略，就是将句子通过bert模型得到的句子中所有的字向量进行求均值操作，最终将均值向量作为整句话的句向量。
- （3）**最大值池化**策略，就是将句子通过bert模型得到的句子中所有的字向量进行求最大值操作，最终将最大值向量作为整句话的句向量。
对bert模型进行微调时，设置了三个目标函数，用于不同任务的训练优化，假设两个待比较的向量分别为 u 和 v 
- （1）Classification Objective Function：u、v以及\|u-v\|拼接起来，加softmax
  - ![](https://www.zhihu.com/equation?tex=o+%3D+softmax%28W_%7Bt%7D%28u%2Cv%2C%7Cu-v%7C%29%29)
  - ![](https://pic1.zhimg.com/80/v2-75a858c4c766a4a3158fa3970badc628_720w.jpg)
- （2）Regression Objective Function：目标函数是，直接对两句话的句子向量 u 和 v 计算余弦相似度
  - ![](https://pic3.zhimg.com/80/v2-1aeacdd9a1d3ee03655f08304d00f726_720w.jpg)
- （3）Triplet Objective Function: 将原来的两个输入，变成三个句子输入。
  - 给定一个锚定句 a ，一个肯定句 p 和一个否定句 n ，模型通过使 a-p 的距离小于 a-n 的距离，来优化模型。使其目标函数o最小
  - ![](https://www.zhihu.com/equation?tex=max%28%7C%7Cs_%7Ba%7D-s_%7Bp%7D%7C%7C-%7C%7Cs_%7Ba%7D-s_%7Bn%7D%7C%7C%2B%5Cepsilon%2C+0%29)

大量实验比较三种求句子向量策略的好坏，认为**平均池化**策略最优，并且在多个数据集上进行了效果验证。虽然效果没有bert输入两句话的效果好，但是比其他方法还是要好的，并且速度很快。

### BERT降维

#### BERT-flow

BERT-flow来自论文《[On the Sentence Embeddings from Pre-trained Language Models](https://arxiv.org/abs/2011.05864)》，EMNLP 2020，主要是用flow模型校正了BERT出来的句向量的分布，从而使得计算出来的cos相似度更为合理一些。

用句向量做相似度计算、索引时，常常用到余弦相似度，根据数值排序，然而，不是所有向量都适合用余弦相似度。
- 余弦相似度：两个向量x,y的内积的几何意义就是“各自的模长乘以它们的夹角余弦”，即两个向量的内积并除以各自的模长
- 假设：向量的“夹角余弦”本身是具有鲜明的几何意义的，但上式右端只是坐标的运算，坐标依赖于所选取的坐标基，基底不同，内积对应的坐标公式就不一样，从而余弦值的坐标公式也不一样。即余弦公式只在“**标准正交基**”下成立，如果这组基是标准正交基，每个分量都是独立的、均匀的，基向量集表现出“**各项同性**”。

BERT向量用余弦值来比较相似度，表现不好，原因可能就是句向量所属的坐标系并非**标准正交基**。不满足各项同性时，需要通过一些方法转换，比如：BERT-flow用了flow模型。

flow模型是一个向量变换模型，它可以将输入数据的分布转化为**标准正态分布**，而显然标准正态分布是各向同性的

flow模型本身很弱，BERT-flow里边使用的flow模型更弱，所以flow模型不大可能在BERT-flow中发挥至关重要的作用。反过来想，那就是也许我们可以找到更简单直接的方法达到BERT-flow的效果。


#### BERT-whitening

[你可能不需要BERT-flow：一个线性变换媲美BERT-flow](https://kexue.fm/archives/8069)
- [BERT-whitening](https://github.com/bojone/BERT-whitening)：通过简单的向量**白化**来改善句向量质量，可以媲美甚至超过BERT-flow的效果, 一个线性变换，可以轻松套到任意的句向量模型中
  - 《[Whitening Sentence Representations for Better Semantics and Faster Retrieval](https://arxiv.org/abs/2103.15316)》
将句向量的**均值**变换为0、**协方差矩阵**变换为单位阵, 相当于传统数据挖掘中的**白化**操作（Whitening），所以该方法笔者称之为**BERT-whitening**

协方差矩阵Σ是一个半正定对称矩阵，半正定对称矩阵都具有如下形式的SVD分解Σ=UΛU⊤，其中U是一个正交矩阵，而Λ是一个对角阵，并且对角线元素都是正的，因此直接让$\boldsymbol{W}^{-1}=\sqrt{\boldsymbol{\Lambda}} \boldsymbol{U}^{\top}$就可以完成求解：$\boldsymbol{W}=\boldsymbol{U} \sqrt{\mathbf{\Lambda}^{-1}}$

```python
def compute_kernel_bias(vecs):
    """计算kernel和bias
    vecs.shape = [num_samples, embedding_size]，
    最后的变换：y = (x + bias).dot(kernel)
    """
    mu = vecs.mean(axis=0, keepdims=True)
    cov = np.cov(vecs.T)
    u, s, vh = np.linalg.svd(cov)
    W = np.dot(u, np.diag(1 / np.sqrt(s)))
    return W, -mu
```

[Github链接](https://github.com/bojone/BERT-whitening)

使用一个简单线性变换的BERT-whitening取得了跟BERT-flow媲美的结果。除了STS-B之外，笔者的同事在中文业务数据内做了类似的比较，结果都表明BERT-flow带来的提升跟BERT-whitening是相近的,

仿照PCA降维，效果更好

```python
def compute_kernel_bias(vecs, n_components=256):
    """计算kernel和bias
    vecs.shape = [num_samples, embedding_size]，
    最后的变换：y = (x + bias).dot(kernel)
    """
    mu = vecs.mean(axis=0, keepdims=True)
    cov = np.cov(vecs.T)
    u, s, vh = np.linalg.svd(cov)
    W = np.dot(u, np.diag(1 / np.sqrt(s)))
    return W[:, :n_components], -mu
```

将base版本的768维只保留前256维，效果有所提升，并且由于降维，向量检索速度肯定也能大大加快；类似地，将large版的1024维只保留前384维，那么降维的同时也提升了效果。

这个结果表明：无监督训练出来的句向量其实是“**通用型**”的，对于特定领域内的应用，里边有很多特征是冗余的，剔除这些冗余特征，往往能达到提速又提效的效果。

而flow模型是可逆的、不降维的，这在某些场景下是好处，但在不少场景下也是缺点，因为它无法剔除冗余维度，限制了性能，比如GAN的研究表明，通过一个256维的高斯向量就可以随机生成1024×1024的人脸图，这表明这些人脸图其实只是构成了一个相当低维的流形，但是如果用flow模型来做，因为要保证可逆性，就得强行用1024×1024×3那么多维的高斯向量来随机生成，计算成本大大增加，而且效果还上不去。


#### SimBERT

[鱼与熊掌兼得：融合检索和生成的SimBERT模型](https://kexue.fm/archives/7427)

追一科技开放了一个名为[SimBERT](https://github.com/ZhuiyiTechnology/pretrained-models#simbert-base)的模型权重，它是以Google开源的BERT模型为基础，基于微软的[UniLM](https://arxiv.org/abs/1905.03197)思想设计了融检索与生成于一体的任务，来进一步微调后得到的模型，所以它同时具备相似问**生成**和相似句**检索**能力。不过当时除了放出一个权重文件和示例脚本之外，未对模型原理和训练过程做进一步说明

`UniLM`是一个融合NLU和NLG能力的Transformer模型，由微软在去年5月份提出来的，今年2月份则升级到了v2版本。我们之前的文章《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》就简单介绍过UniLM，并且已经集成到了bert4keras中。

UniLM的核心是通过特殊的Attention Mask来赋予模型具有Seq2Seq的能力。假如输入是“你想吃啥”，目标句子是“白切鸡”，那UNILM将这两个句子拼成一个：[ CLS] 你 想 吃 啥 [ SEP] 白 切 鸡 [ SEP]，然后接如图的Attention Mask
- ![](https://kexue.fm/usr/uploads/2019/09/1625339461.png)
- [ CLS] 你 想 吃 啥 [ SEP]这几个token之间是双向的Attention，而白 切 鸡 [ SEP]这几个token则是单向Attention，从而允许递归地预测白 切 鸡 [ SEP]这几个token，所以它具备文本生成能力。
- ![](https://kexue.fm/usr/uploads/2019/09/1879768703.png)


[SimBERT](https://github.com/ZhuiyiTechnology/simbert)属于有监督训练，训练语料是自行收集到的相似句对，通过一句来预测另一句的相似句生成任务来构建Seq2Seq部分，然后前面也提到过[ CLS]的向量事实上就代表着输入的句向量，所以可以同时用它来训练一个检索任务
- ![](https://kexue.fm/usr/uploads/2020/05/2840550561.png)
- 假设SENT_a和SENT_b是一组相似句，那么在同一个batch中，把[ CLS] SENT_a [ SEP] SENT_b [ SEP] 和 [ CLS] SENT_b [ SEP] SENT_a [ SEP]都加入训练，做一个相似句的生成任务，这是Seq2Seq部分。
关键就是“[ CLS]的向量事实上就代表着输入的句向量”，所以可以用它来做一些NLU相关的事情。最后的loss是Seq2Seq和相似句分类两部分loss之和。

实施：
- 数据来源是爬取百度知道推荐的相似问，然后经过简单算法过滤。如果本身有很多问句，也可以通过常见的检索算法检索出一些相似句，作为训练数据用。总而言之，训练数据没有特别严格要求，理论上有一定的相似性都可以。
- 至于训练硬件，开源的模型是在一张TITAN RTX（22G显存，batch_size=128）上训练了4天左右，显存和时间其实也没有硬性要求，视实际情况而定，如果显存没那么大，那么适当降低batch_size即可，如果语料本身不是很多，那么训练时间也不用那么长（大概是能完整遍历几遍数据集即可）。

#### SimCSE对比学习

中文任务还是SOTA吗？我们给SimCSE补充了一些实验

苏剑林构思的“BERT-whitening”的方法一度成为了语义相似度的新SOTA，然而不久之后，Arxiv上出现了至少有两篇结果明显优于BERT-whitening的新论文。
- 第一篇是《[Generating Datasets with Pretrained Language Models](https://arxiv.org/pdf/2104.07540.pdf)》，这篇借助模板从GPT2_XL中无监督地构造了数据对来训练**相似度**模型，虽然有一定启发而且效果还可以，但是复现的成本和变数都太大。
- 另一篇则是《[SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2104.08821)》，它提出的`SimCSE`在英文数据上显著超过了BERT-flow和BERT-whitening，并且方法特别简单～

https://github.com/bojone/SimCSE

`SimCSE`可以看成是`SimBERT`的**简化版**：
- 1、SimCSE去掉了SimBERT的生成部分，仅保留检索模型；
- 2、由于SimCSE没有标签数据，所以把每个句子自身视为相似句传入。
即：(自己,自己) 作为**正例**、(自己,别人) 作为**负例**来训练对比学习模型，这里的“自己”并非完全一样，而是采用一些数据扩增手段，让正例的两个样本有所差异，但是在NLP中如何做数据扩增本身又是一个难搞的问题，SimCSE则提出了一个极为简单的方案：**直接把Dropout当作数据扩增**！

实验结果
- 英文语料：SimCSE**明显优于**BERT-flow和BERT-whitening
- 中文语料：除了PAWSX这个“异类”外，SimCSE相比BERT-whitening确实有压倒性优势，有些任务还能好10个点以上，在BQ上SimCSE还比有监督训练过的SimBERT要好，而且像SimBERT这种已经经过监督训练的模型还能获得进一步的提升，这些都说明确实强大

### BERT真的有理解能力吗

【2019-07-12】
- ACL 2019，[论文1：Probing Neural Network Comprehension of Natural Language Arguments](https://arxiv.org/abs/1907.07355), 台湾成功大学计算机科学与信息工程系智能知识管理实验室，一作 Timothy Niven，ARCT一个数据集
  - [台湾小哥一篇论文把 BERT 拉下神坛](https://zhuanlan.zhihu.com/p/74652696)
  - [把BERT拉下神坛！ACL论文只靠一个“Not”，就把AI阅读理解骤降到盲猜水平](https://bbs.huaweicloud.com/blogs/107657)
  - We are surprised to find that BERT's peak performance of 77% on the **Argument Reasoning Comprehension Task** reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an **adversarial dataset** on which <font color='red'>all models achieve random accuracy</font>. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.
- ACL 2019, [论文2：Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference](https://arxiv.org/abs/1902.01007)，只不过，数据集不一样了。这篇论文里BERT是在**多类型语言推理数据集** (MNLI) 上训练的，而测试集则是研究团队自制的HANS数据集，一些简单的句子变换，就能让AI做出错误的判断。
  - A machine learning system can score well on a given test set by relying on heuristics that are effective for **frequent example types** but break down in more challenging cases. We study this issue within **natural language inference** (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a **controlled evaluation set** called `HANS` (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area
- 论文3：What does BERT Learn from Multiple-Choice Reading Comprehension Datasets? 在数据集中添加干扰文本，结果显示BERT的表现非常差
- 研究认为，包括BERT在内，许多模型的成功都是建立在**虚假的线索**上。团队用了去年诞生的`观点推理理解任务` (ARCT) 考验了BERT。结果发现，只要做个**对抗数据集**，准确率就从77%降到53%，几乎等同于随机猜
  - 对抗并不是把o变成0、把I变成1的小伎俩
  - BERT是依靠数据集里 “**虚假的统计学线索** (Spurious Statistical Cues) ”来推理的，BERT是利用了一些线索词来判断，特别是“Not”这个词, 它并不能真正分析出句子之间的逻辑关系。
  - `观点推理理解任务` (ARCT) ，是Habernal和小伙伴们提出的阅读理解任务，考察的是语言模型的**推理能力**，中选了NAACL 2018。一个`观点`，包含`前提` (Premise) ，和`主张` (Claim) 。除此之外，观点又有它的`原因` (Reasoning) ，以及它的`佐证` (Warrant) 。
- 这篇ACL论文打击了以BERT为首的众多阅读理解模型

Reddit评论区补充说：
> - 每隔几个月就会听到有关NLP的新进展，更新、更好的模型层出不穷。但当有人实际用数据集测试时，会发现这些模型并没有真正学习到什么。优化模型的竞赛该放缓脚步了，我们更应该仔细研究研究数据集，看看它们是否真的有意义。
> - 并不否认BERT和其他新模型的价值，但是并不相信一些Benchmark。


# BERT变种

## 改进总结

- 【2022-5-26】[BERT模型的优化改进方法](https://zhuanlan.zhihu.com/p/514849987), 论文《BERT模型的主要优化改进方法研究综述》的阅读笔记，对 BERT主要优化改进方法进行了研究梳理。

基础回顾
- BERT是由Google AI于2018年10月提出的一种基于深度学习的语言表示模型。BERT 发布时，在11种不同的NLP测试任务中取得最佳效果，NLP领域近期重要的研究成果。
- ![](https://pic4.zhimg.com/80/v2-c1d76b60d55e808946c73c54556753b3_1440w.jpg)
- BERT主要的模型结构是**Transformer编码器**。Transformer是由 Ashish 等于2017年提出的，用于Google机器翻译，包含编码器（Encoder）和解码器（Decoder）两部分。
BERT 模型使用两个预训练目标来完成文本内容特征的学习。
- **掩藏语言模型**（Masked Language Model，MLM）通过将单词掩盖，从而学习其上下文内容特征来预测被掩盖的单词
- **相邻句预测**（Next Sentence Predication，NSP）通过学习句子间关系特征，预测两个句子的位置是否相邻

### BERT家族

- `BERT`：MLM 和 NSP任务
  - 基于 Transformer Encoder来构建的预训练语言模型，它是通过 Masked Lanauge Model(MLM) 和 Next Sentence Prediction(NSP) 两个任务在大规模语料上训练得到的
  - 开源的 Bert 模型分为 base 和 large，它们的差异在模型大小上。大模型有更大的参数量，性能也有会几个百分点的提升，当然需要消耗更多的算力
- `BERT-WWM`：mask策略由token 级别升级为词级别
- `Roberta`：BERT优化版，更多数据+迭代步数+去除NSP+动态mask
- `XLNet`：模型结构和训练方式上与BERT差别较大
  - Bert 的 MLM 在预训练时有 MASK 标签，但在推理时却没有，导致训练和推理出现不一致；并且 MLM 不属于 Autoregressive LM，不能做**生成类**任务。
  - XLNet 采用 PML(Permutation Language Model) 避免了 MASK 标签的使用，且属于 Autoregressive LM，可以做生成任务。
  - Bert 使用的 Transformer 结构对文本的长度**有限制**，为更好地处理长文本，XLNet 采用升级版的 Transformer-XL。
- `Albert`：BERT简化版，更少的数据，得到更好的结果（70% 参数量的削减，模型性能损失<3% ）；两个方面减少模型的参数量：
  - 对 Vocabulary Embedding 进行矩阵分解，将原来的矩阵V x E分解成两个矩阵V x H和H x E（H << E）。
  - 跨层参数共享，每层的 attention map 有相似的pattern，可以考虑共享。

这些模型的性能在不同的数据集上有差异，需要试了才知道哪个表现更好，但总体而言 XLNet 和 Roberta 会比 Bert 效果略好，large 会比 base 略好，更多情况下，它们会被一起使用，最后做 ensemble。

### 分支1：改进预训练

自然语言的特点在于丰富多变，很多研究者针对更丰富多变的文本表达形式，在这两个训练目标的基础上进一步完善和改进，提升了模型的文本特征学习能力。

#### 1.1 改进掩藏语言模型 —— 全词覆盖（wwm/ERNIE/SpanBERT）

在BERT模型中，对文本的预处理都按照**最小单位**进行了切分。例如对于英文文本的预处理采用了Google的 wordpiece 方法以解决其未登录词的问题。
- MLM中掩盖的对象多数情况下为**词根**（subword），并不是完整的词；
- 对于中文则直接按**字**切分，直接对单个字进行掩盖。这种掩盖策略导致了模型对于词语信息学习的不完整。
  - 汉语的词语几乎可以用无穷无尽来形容。但中文常用的汉字也就**4000**多个，BERT训练很容易实现，所以最初BERT对中文任务是以**汉字**为单位实现训练的。
  - 问题：既然是以汉字为单位训练的，其训练出的就是孤零零的汉字向量，而在现代汉语中，单个汉字是远不足以表达像词语或者短语那样丰富的语义的，这就造成了BERT在很多中文任务上表现并不理想的情况。
针对这一不足，大部分研究者改进了MLM的掩盖策略。

BERT-WWM 模型中，提出了**全词覆盖**的方式。
- 2019年7月，哈工大发布的 BERT-Chinese-wwm 利用中文分词，将组成一个完整词语的所有单字同时掩盖。
- 2019年4月，ERNIE 扩展了中文全词掩盖策略，扩展到对于中文分词、短语及命名实体的全词掩盖。
  - 百度版：全词mask，**隐式**输入知识信息，mask训练阶段将知识实体全部mask
  - 清华版：知识图谱，**显式**加入知识信息，在输入阶段
- SpanBERT 采用了几何分布来随机采样被掩盖的短语片段，通过Span边界词向量来预测掩盖词

（1）BERT-WWM

BERT-WWM是 2019年7月 推出的，它是由哈工大讯飞联合实验室针对中文做出的改进，WWM 的意思是Whole Word Masking，其实就是ERNIE模型中的**短语级别**遮盖（phrase-level masking）训练。

BERT-WWM属于BERT-base模型结构，由12层Transformers构成。
- 训练第一阶段（最大长度为128）采用的 batch size为2560，训练了100K步。
- 训练第二阶段（最大长度为512）采用的 batch size为384，训练了100K步。

后续又出了一个 BERT-WWM-ext ，它是BERT-WWM的一个**升级版**，相比于BERT-WWM的改进是增加了训练数据集同时也增加了训练步数，性能也得到了一定程度的提升。它也属于BERT-base模型结构，由12层Transformers构成。
- 训练第一阶段（最大长度为128）采用的batch size为2560，训练了1000K步。
- 训练第二阶段（最大长度为512）采用的batch size为384，训练了400K步。

为什么要单独说这个呢？因为提出BERT-WWM的论文中，作者团队还同时训练出了：BERT-WWM、BERT-WWM-ext、RoBERTa-WWM-ext和RoBERTa-WWM-ext-large模型，并且对比了这些模型的性能，其中RoBERTa-WWM-ext-large的强悍性能也让很多人对它青睐有加，即便是模型发布一年之久的今天它也难逢敌手。这些模型的发布为NLP从业者、学习者和爱好者提供了极大的方便，使用起来也很简单，在此感谢作者们。

（2）ERNIE

ERNIE的改进思路：给模型加上**词语**信息，即百度宣称的让模型学到“知识”

那要怎样加入知识信息呢？有两种方法：
- 第一种是**显式**的方法，即通过某种手段，在**输入**阶段，就给模型输入知识实体。（清华版）
- 第二种是**隐式**的方法，即在masked**训练**阶段，将包含有某一个知识实体的汉字全都Mask掉，让模型来猜测这个词语，从而学习到知识实体。（百度版）
百度的ERNIE选取的是第二种方案，其实清华大学也推出了一个同名的ERNIE模型，采取的就是第一种方法，但貌似表现效果和名气都不如百度的ERNIE，故在此不多谈。百度是国内搜索引擎常年的霸主，其对语义理解和知识图谱的研究可谓是炉火纯青，所以百度将知识图谱引入了BERT中，形成了ERNIE。

实际上，英文任务也可以通过ERNIE的方式来改进，掩盖整个英文短语或者实体，让BERT学习到知识
- ![](https://pic4.zhimg.com/80/v2-3477503af72ce5ad3dd9c3b233447837_1440w.jpg)

ERNIR的mask是由三个阶段学习组成
- 第一个阶段，采用的是 BERT模式的 word piece（词根） 级别的 mask (basic-level masking)
- 其次，加入**短语**级别的mask(phrase-level masking)
- 最后，再加入**实体**级别的mask(entity-level masking)。

![](https://pic1.zhimg.com/80/v2-39571e3fc839399476c5fb3995b32944_1440w.png)

在这个基础上，借助百度在中文的社区的强大能力，中文的ERNIR还用了各种混杂数据集(Heterogeneous Data)。

ERNIE 2.0
- 百度在之前的模型上做了新的改进，这篇论文主要是走**多任务**的思想，引入了多大7个任务来预训练模型，并且采用的是逐次增加任务的方式来预训练，具体的任务如下面图中所示，图中红框、蓝框、绿框里面的就是七种任务的名称：
- ![](https://pic3.zhimg.com/80/v2-63bc4e5877bd7b1944483286aa883522_1440w.jpg)
- 因为不同的任务输入不同，因此作者还引入了Task Embedding，来区分不同的任务。真就万物皆可Embedding呗。
- 训练的方法是先训练任务1，保存模型，然后加载刚保存的模型，再同时训练任务1和任务2，依次类推，到最后同时训练7个任务。

效果：
- 2.0在效果上比1.0版本全面提升，其中，在阅读理解的任务上提升非常大。

#### 1.2 引入降噪自编码器 —— BART

MLM 将原文中的词用\[MASK]标记**随机**替换，这本身是对文本进行了破坏，相当于在文本中添加了噪声，然后通过训练语言模型来还原文本，消除噪声。

DAE 是一种具有降噪功能的**自编码器**，旨在将含有噪声的输入数据还原为干净的原始数据。对于语言模型来说，就是在原始语言中加入噪声数据，再通过模型学习进行噪声的去除以恢复原始文本。

BART引入了**降噪自编码器**，丰富了文本的破坏方式。例如随机掩盖（同 MLM 一致）某些词、随机删掉某些词或片段、打乱文档顺序（含旋转）等，将文本输入到编码器中后，利用一个解码器生成破坏之前的原始文档。
- ![](https://pic3.zhimg.com/80/v2-27d3a08c1e6cff3d55f201ac7adc97ee_1440w.jpg)


#### 1.3 引入替代词检测 —— ELECTRA

MLM 对文本中的\[MASK]标记的词进行预测，以试图恢复原始文本。其预测结果可能完全正确，也可能预测出一个不属于原文本中的词。

ELECTRA引入了**替代词**检测，来预测一个由语言模型生成的句子中哪些词是原本句子中的词，哪些词是语言模型生成的且不属于原句子中的词。
- ELECTRA 使用一个小型的 MLM 模型作为**生成器**（Generator），来对包含\[MASK]的句子进行预测。
- 另外训练一个基于二分类的判别器（Discriminator）来对生成器生成的句子进行判断。

![](https://pic3.zhimg.com/80/v2-91eed3257d2330631a8506ca5b1d17be_1440w.jpg)

#### 1.4 改进相邻句预测

在大多数应用场景下，模型仅需要针对单个句子完成建模，舍弃NSP训练目标来优化模型对于单个句子的特征学习能力。
- 删除NSP：NSP仅仅考虑了两个句子是否相邻，而没有兼顾到句子在整个段落、篇章中的位置信息。
- 改进NSP：通过预测句子之间的顺序关系，从而学习其位置信息。

### 分支2：融合融合外部知识

当下知识图谱的相关研究已经取得了极大的进展，大量的外部知识库都可以应用到 NLP 的相关研究中。
 
#### 2.1 嵌入实体关系知识
 
实体关系三元组是知识图谱的最基本的结构，也是外部知识最直接和结构化的表达。
- `K-BERT`从BERT模型输入层入手，将实体关系的三元组显式地嵌入到输入层中。
- ![](https://pic1.zhimg.com/80/v2-4f699efff5260493029a417af59c51ac_1440w.jpg)
 
#### 2.2 特征向量拼接知识

BERT可以将任意文本表示为特征向量的形式，因此可以考虑采用**向量拼接**的方式在 BERT 模型中融合外部知识。
- `SemBERT`利用语义角色标注工具，获取文本中的语义角色向量表示，与原始BERT文本表示融合。
 
#### 2.3 训练目标融合知识
 
在知识图谱技术中，大量丰富的外部知识被用来直接进行模型训练，形成了多种训练任务。
- `ERNIE`以DAE的方式在BERT中引入了**实体对齐**训练目标，WKLM通过随机替换维基百科文本中的实体，让模型预测正误，从而在预训练过程中嵌入知识。

### 分支3：改进Transformer
 
由于Transformer结构自身的限制，BERT等一系列采用 Transformer 的模型所能处理的最大文本长度为 512个token。
 
#### 3.1 改进 Encoder MASK矩阵
 
BERT 作为一种双向编码的语言模型，其“双向”主要体现在 Transformer结构的 MASK 矩阵中。Transformer 基于自注意力机制（Self-Attention），利用MASK 矩阵提供一种“注意”机制，即 MASK 矩阵决定了文本中哪些词可以互相“看见”。
 
`UniLM`通过对输入数据中的两个句子设计不同的 MASK 矩阵来完成生成模型的学习。对于第一个句子，采用跟 BERT 中的 Transformer-Encoder 一致的结构，每个词都能够“注意”到其“上文”和“下文”信息。
 
对于第二个句子，其中的每个词只能“注意”到第一句话中的所有词和当前句子的“上文”信息。利用这种巧妙的设计，模型输入的第一句话和第二句话形成了经典的“Seq2Seq”的模式，从而将 BERT 成功用于语言生成任务。
 
#### 3.2 Encoder + Decoder语言生成
 
BART模型同样采用Encoder+Decoder 的结构，借助DAE语言模型的训练方式，能够很好地预测和生成被“噪声”破坏的文本，从而也得到具有文本生成能力的预训练语言模型。
 
### 分支4：量化与压缩

#### 4.1 模型蒸馏
 
对 BERT 蒸馏的研究主要存在于以下几个方面：
*   在预训练阶段还是微调阶段使用蒸馏
*   学生模型的选择
*   蒸馏的位置
 
- `DistilBERT`在预训练阶段蒸馏，其学生模型具有与BERT结构，但层数减半。
- `TinyBERT`为BERT的嵌入层、输出层、Transformer中的隐藏层、注意力矩阵都设计了损失函数，来学习 BERT 中大量的语言知识。
 
![](https://pic3.zhimg.com/80/v2-50248129fc9e5102fb355d979742b65a_1440w.jpg)
 
#### 4.2 模型剪枝
 
剪枝（Pruning）是指去掉模型中不太重要的权重或组件，以提升推理速度。用于 BERT 的剪枝方法主要有权重修剪和结构修剪。

## BERT fine-tune

- 【2021-8-25】[中文语料的 Bert 微调 Bert Chinese Finetune](https://kuhungio.me/2019/bert-chinese-finetune/)

[Bert](https://github.com/google-research/bert) 的文档本身对 finetune 进行了较为详细的描述，但对于不熟悉官方标准数据集的工程师来说，有一定的上手难度。随着 [Bert as service](https://github.com/hanxiao/bert-as-service) 代码的开源，使用 Bert 分类或阅读理解的副产物–词空间，成为一个更具实用价值的方向。

### 预训练模型

- 下载 [BERT-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip): Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters

### 数据准备
- train.tsv 训练集
- dev.tsv 验证集

数据格式
- 第一列为 label，第二列为具体内容，tab 分隔。因模型本身在字符级别做处理，因而无需分词。

```
fashion	衬衫和它一起穿,让你减龄十岁!越活越年轻!太美了!...
houseliving	95㎡简约美式小三居,过精美别致、悠然自得的小日子! 屋主的客...
game	赛季末用他们两天上一段，7.20最强LOL上分英雄推荐！ 各位小伙...
```

bert 的 finetune 主要存在两类应用场景：分类和阅读理解。

### 分类任务

因分类较为容易获得样本，以下以分类为例，做模型微调：

修改 run_classifier.py
- 集成抽象类DataProcessor，实现里面的几种方法：
  - get_train_examples：获取训练集数据
  - get_dev_examples：获取验证集数据
  - get_test_examples：获取测试集数据
  - get_labels：获取分类标签集合

BERT数据处理过程：<font color='blue'>原始文本 → 分词 → 加特殊标记 → 映射为id → 合并</font>
- 原始文本
- **分词**：中英文不同
  - 英文：词性/词干还原
  - 中文：分词，bert默认以单字切割
- 加**特殊标记**
  - 根据任务不同，加\[CLS],\[SEP],\[PAD]等
- **映射为id**
  - 将以上所有字符映射为id编号序列
  - 注意：encoder和decoder对应不同的字典
- 句子**向量化**
  - 根据语料长度，设置高于分词后最大长度的阈值max_len，作为句子长度维度

格式：
- (a) For sequence pairs: **句子对**
  - sentence: is this jackson ville ? || no it is not .
  - tokens:   \[CLS] is this jack ##son ##ville ? \[SEP] no it is not . \[SEP]
  - type_ids: 0     0  0    0    0   0  0 0     1  1  1  1   1 1
-  (b) For single sequences: **单句形式**
  - sentence: the dog is hairy .
  - tokens:   \[CLS] the dog is hairy . \[SEP]
  - type_ids:   0     0   0   0  0     0  0
- 输出格式：[guid, text_a, text_b, label]
  - 后两个字段可选
- 输入模型
  - input_ids：句子id向量，max_len维
  - input_mask：句子掩码模板，0，1标记，0表示空白填充
  - segment_ids：两个句子分隔位置
  - label_id：分类目标对应的id

```python
class DemoProcessor(DataProcessor):
    """任务相关的数据集，处理类"""
    def __init__(self):
        self.labels = set() # label集合
    
    def get_train_examples(self, data_dir):
        """读取训练集"""
        # _read_csv只是简单按照tab分隔成list
        # _create_examples将以上list转成标准样本格式
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")
    def get_dev_examples(self, data_dir):
        """读取验证集"""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "dev.tsv")), "dev")
    def get_test_examples(self, data_dir):
        """读取测试集"""
        return self._create_examples(
          self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")
    def get_labels(self):
        """获取目标值集合"""
        #return list(self.labels)
        return ["fashion", "houseliving","game"] # 根据 label 自定义
    
    def _create_examples(self, lines, set_type):
        """从训练集/验证集中读取样本"""
        examples = []
        for (i, line) in enumerate(lines):
            # 格式：[label text]
            # Only the test set has a header 测试集有header表头信息
            if set_type == "test" and i == 0:
                continue
            guid = "%s-%s" % (set_type, i) # 样本唯一id
            if set_type == "test":
                text_a = tokenization.convert_to_unicode(line[0])
                label = "0" # 测试集给默认label
            else: # 将所有字符转成unicode
                text_a = tokenization.convert_to_unicode(line[1])
                label = tokenization.convert_to_unicode(line[0])
                examples.append(
                  InputExample(guid=guid, text_a=text_a, text_b=None, label=label))
        # 输出格式：[guid, text_a, text_b, label]
        return examples
```

添加 DemoProcessor

```python
  processors = {
      "cola": ColaProcessor,
      "mnli": MnliProcessor,
      "mrpc": MrpcProcessor,
      "xnli": XnliProcessor,
      "demo": DemoProcessor,
  }
```

启动训练

```python
export BERT_Chinese_DIR=/path/to/bert/chinese_L-12_H-768_A-12
export Demo_DIR=/path/to/DemoDate
python run_classifier.py \
  --task_name=demo \
  --do_train=true \
  --do_eval=true \
  --data_dir=$Demo_DIR \
  --vocab_file=$BERT_Chinese_DIR/vocab.txt \
  --bert_config_file=$BERT_Chinese_DIR/bert_config.json \
  --init_checkpoint=$BERT_Chinese_DIR/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/Demo_output/
```

若一切顺利，将会有以下输出:

```shell
***** Eval results *****
  eval_accuracy = xx
  eval_loss = xx
  global_step = xx
  loss = xx
```

最终，微调后的模型保存在output_dir指向的文件夹中。

### 总结

Bert 预训练后的 finetune，是一种很高效的方式，节省时间，同时提高模型在垂直语料的表现。finetune 过程，实际上不难。较大的难点在于数据准备和 pipeline 的设计。从商业角度讲，应着重考虑 finetune 之后，模型有效性的证明，以及在业务场景中的应用。如果评估指标和业务场景都已缕清，那么不妨一试。

[Github 地址](https://github.com/kuhung/bert_finetune)

## MLM改进

- 基于MLM，做各种改进尝试

MLM，全称“Masked Language Model”，可以翻译为“掩码语言模型”，实际上就是一个完形填空任务，随机 Mask 掉文本中的某些字词，然后要模型去预测被 Mask 的字词。其中被 Mask 掉的部分，可以是直接随机选择的 Token，也可以是随机选择连续的能组成一整个词的 Token，后者称为 `WWM`（Whole Word Masking）。

论文 BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model指出 MLM 可以作为一般的生成模型用，论文 Spelling Error Correction with Soft-Masked BERT 则将 MLM 用于文本纠错。

### 结合人工模板

GPT-3 的论文叫做 Language Models are Few-Shot Learners [1]，标题里边已经没有 G、P、T 几个单词了，只不过它跟开始的 GPT 是一脉相承的，因此还是以 GPT 称呼它。顾名思义，GPT-3 主打的是 **Few-Shot Learning**，也就是**小样本学习**。此外，GPT-3 的另一个特点就是大，最大的版本多达 1750 亿参数，是 BERT Base 的一千多倍。

正因如此，前些天 Arxiv 上的一篇论文 It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners [2] 便引起了笔者的注意，意译过来就是“谁说一定要大的？小模型也可以做小样本学习”。

[必须要GPT-3吗？不，BERT的MLM模型也能小样本学习](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247512167&idx=1&sn=cc7695d92362e3b18a6e8969fb14dc27&chksm=96ea6fe7a19de6f1be86b965e268df1b9c6320810cf32b6d64ddd3d238bf9088be41fb36adfe#rd)中，我们介绍了一种名为**Pattern-Exploiting Training**（`PET`） 的方法，它通过人工构建的模版与BERT的MLM模型结合，能够起到非常好的零样本、小样本乃至半监督学习效果，而且该思路比较优雅漂亮，因为它将预训练任务和下游任务统一起来了。

将任务转成完形填空
- MLM 的一个精彩应用：用于小样本学习或半监督学习，某些场景下甚至能做到零样本学习。

![](https://pic1.zhimg.com/80/v2-886f03f3e90c8e65f98329f4375b1408_720w.jpg)

一些简单的推理任务也可以做这样的转换，常见的是给定两个句子，判断这两个句子是否相容，比如“我去了北京”跟“我去了上海”就是矛盾的，“我去了北京”跟“我在天安门广场”是相容的，常见的做法就是将两个句子拼接起来输入到模型做，作为一个二分类任务。如果要转换为完形填空，那该怎么构造呢？一种比较自然的构建方式是：

>- 我去了北京？______，我去了上海。
>- 我去了北京？______，我在天安门广场。

其中空位之处的候选词为 { 是的，不是 }

给输入的文本增加一个前缀或者后缀描述，并且 Mask 掉某些 Token，转换为完形填空问题，这样的转换在原论文中称为 Pattern，这个转换要尽可能与原来的句子组成一句自然的话，不能过于生硬，因为预训练的 MLM 模型就是在自然语言上进行的。


### 模板自动生成（PET）

【2021-6-17】[P-tuning：自动构建模版，释放语言模型潜能](https://zhuanlan.zhihu.com/p/364141928)

然而，人工构建这样的模版有时候也是比较困难的，而且不同的模版效果差别也很大，如果能够通过少量样本来自动构建模版，也是非常有价值的。

最近Arxiv上的论文《[GPT Understands, Too](https://arxiv.org/abs/2103.10385)》提出了名为**P-tuning**的方法，成功地实现了模版的**自动构建**。不仅如此，借助P-tuning，GPT在SuperGLUE上的成绩首次超过了同等级别的BERT模型，这颠覆了一直以来“GPT不擅长NLU”的结论，也是该论文命名的缘由。


P-tuning重新审视了关于模版的定义，放弃了“模版由自然语言构成”这一常规要求，从而将模版的构建转化为连续参数优化问题，虽然简单，但却有效

模版的反思
- 首先，我们来想一下“什么是模版”。直观来看，模版就是由自然语言构成的前缀/后缀，通过这些模版我们使得下游任务跟预训练任务一致，这样才能更加充分地利用原始预训练模型，起到更好的零样本、小样本学习效果。
- 等等，我们真的在乎模版是不是“自然语言”构成的吗？
- 并不是。本质上来说，我们并不关心模版长什么样，我们只需要知道模版由哪些token组成，该插入到哪里，插入后能不能完成我们的下游任务，输出的候选空间是什么。模版是不是自然语言组成的，对我们根本没影响，“自然语言”的要求，只是为了更好地实现“一致性”，但不是必须的。于是，P-tuning考虑了如下形式的模版：

![](https://pic1.zhimg.com/80/v2-a8313077087b511186ccb280a2e08a20_720w.jpg)

▲ P-tuning直接使用[unused*]的token来构建模版，不关心模版的自然语言性

这里的[u1]～[u6]，代表BERT词表里边的 [unused1]～[unused6]，也就是用几个从未见过的token来构成模板，这里的token数目是一个超参数，放在前面还是后面也可以调整。接着，为了让“模版”发挥作用，我们用标注数据来求出这个模板。

根据标注数据量的多少，优化思路又分两种情况讨论。
- 第一种，标注数据**比较少**。这种情况下，我们固定整个模型的权重，只优化[unused1]～[unused6]这几个token的Embedding，换句话说，其实我们就是要学6个新的Embedding，使得它起到了模版的作用。这样一来，因为模型权重几乎都被固定住了，训练起来很快，而且因为要学习的参数很少，因此哪怕标注样本很少，也能把模版学出来，不容易过拟合。
- 第二种，标注数据**很充足**。这时候如果还按照第一种的方案来，就会出现欠拟合的情况，因为只有6个token的可优化参数实在是太少了。因此，我们可以放开所有权重微调，原论文在SuperGLUE上的实验就是这样做的。读者可能会想：这样跟直接加个全连接微调有什么区别？原论文的结果是这样做效果更好，可能还是因为跟预训练任务更一致了吧。

原作者在SuperGLUE上的实验结果，显示出如果配合P-tuning，那么：
- 1）GPT、BERT的效果相比直接finetune都有所提升；
- 2）GPT的效果还能超过了BERT。

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bc%7Ccc%7D++%5Chline++%26+%5Ctext%7B%E9%AA%8C%E8%AF%81%E9%9B%86%7D+%26+%5Ctext%7B%E6%B5%8B%E8%AF%95%E9%9B%86%7D+%5C%5C++%5Chline++%5Ctext%7B%E5%B0%8F%E6%A0%B7%E6%9C%AC%E7%9B%B4%E6%8E%A5%E5%BE%AE%E8%B0%83%7D+%26+88.93%5C%25+%26+89.34%5C%25+%5C%5C++%5Ctext%7BVAT%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%7D+%26+89.83%5C%25+%26+90.37%5C%25+%5C%5C++%5Chline++%5Ctext%7BPET%E9%9B%B6%E6%A0%B7%E6%9C%AC%7D+%26+85.17%5C%25+%26+84.27%5C%25+%5C%5C++%5Ctext%7BPET%E6%97%A0%E7%9B%91%E7%9D%A3%7D+%26+88.05%5C%25+%26+87.53%5C%25+%5C%5C++%5Ctext%7BPET%E5%B0%8F%E6%A0%B7%E6%9C%AC%7D+%26+89.29%5C%25+%26+89.18%5C%25+%5C%5C++%5Ctext%7BPET%E5%8D%8A%E7%9B%91%E7%9D%A3%7D+%26+90.09%5C%25+%26+89.76%5C%25+%5C%5C++%5Chline++%5Ctext%7BBERT+%2B+P-tuning%7D+%26+89.81%5C%25+%26+89.75%5C%25+%5C%5C++%5Ctext%7BGPT+%2B+P-tuning%7D+%26+89.30%5C%25+%26+88.51%5C%25+%5C%5C++%5Chline++%5Cend%7Barray%7D%5C%5C)

其中“小样本”只用到了“少量标注样本”，“无监督”则用到了“大量无标注样本”，“半监督”则用到了“少量标注样本+大量无标注样本”，“P-tuning”都是小样本，PET的几个任务报告的是最优的人工模版的结果，其实还有更差的人工模版。从小样本角度来看，P-tuning确实取得了最优的小样本学习效果；从模版构建的角度来看，P-tuning确实也比人工构建的模版要好得多；从模型角度看，P-tuning确实可以将GPT的分类性能发挥到跟BERT相近，从而揭示了GPT也有很强的NLU能力的事实。

完整[代码](https://github.com/bojone/P-tuning), 原论文也开源了[代码](https://github.com/THUDM/P-tuning)

## ALBERT

【2019-9-28】谷歌Lab发布了一个新的预训练模型"ALBERT"全面在SQuAD 2.0、GLUE、RACE等任务上超越了BERT、XLNet、RoBERTa再次刷新了排行榜
- [ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)
- ![](https://pic4.zhimg.com/80/v2-237353aaeb0f6a2a8fe24bcd585cc65b_720w.jpg)

Albert 的目的是想对 Bert 瘦身，希望用更简单的模型，更少的数据，得到更好的结果。它主要从以下两个方面减少模型的参数量：
- 对 Vocabulary Embedding 进行**矩阵分解**，将原来的矩阵V x E分解成两个矩阵 V x H 和 H x E（H << E）。
- **跨层参数共享**，每层的 attention map 有相似的pattern，可以考虑共享。

效果是 70% 参数量的削减，模型性能损失 < 3%，但有很多文章指出 Albert 的计算量并没有减少太多，并且如果达到超过 Bert 性能，需要使用更大的模型以及相应的算力。

【2020-5-20】[BERT的youxiu变体：ALBERT论文图解介绍](https://zhuanlan.zhihu.com/p/142416395)

ALBERT(一个Lite BERT)(Lan等人，2019)主要解决了更高的**内存消耗**和BERT**训练速度慢**的问题。 

ALBERT引入了两种参数减少技术。 
- 首先是嵌入分解，它将嵌入矩阵分解为两个小的矩阵。 
- 其次是跨层参数共享，跨ALBERT的每一层共享transformer权重，这将显着减少参数。 
此外，他们还提出了**句序预测**(SOP)任务来代替传统的**NSP**预训练任务。

ALBERT作为BERT的一个变体，在保持性能的基础上，大大减少了模型的参数，使得实用变得更加方便，是经典的BERT变体之一。
 
考虑下面给出的句子。作为人类，当我们遇到“**apple**”这个词时，我们可以：
*   把“apple”这个词和我们对“apple”这个水果的表征联系起来
*   根据上下文将“apple”与水果联系在一起，而不是与公司联系在一起
*   理解“_he ate an apple_”
*   在字符级，单词级和句子级理解它
 
![](https://pic2.zhimg.com/v2-e7dda49cb43c1d2f5e4bbe757b0a5a19_b.jpg)
 
NLP最新发展的基本前提是赋予机器学习这些表示的能力。  
2018年，谷歌发布了BERT，试图基于一些新的想法来学习这个表示：
 
### 回顾BERT

**1\. 掩码语言建模**
 
语言建模包括预测单词的上下文，作为一种学习表示的方法。传统上，这包括到当给出前面的单词时预测句子中的下一个单词。
 
![](https://pic2.zhimg.com/v2-e394582d3b192c7b9e6ce6a23a5ea67d_b.jpg)
 
相反，BERT使用了一个**掩码语言模型**目标，在这个模型中，我们在文档中随机地对单词进行掩码，并试图根据周围的上下文来预测它们。
 
![](https://pic3.zhimg.com/v2-cceaeae52d02d52a4a2a27dac0662d9a_b.jpg)
 
 **2\. 下一个句子预测**  
 
“下一个句子预测”的目的是检测两个句子是否连贯。
 
![](https://pic2.zhimg.com/v2-a66f48660c3d0d00c8c06011f0897fc1_b.jpg)
 
为此，训练数据中的连续句被用作一个正样本。对于负样本，取一个句子，然后在另一个文档中随机抽取一个句子放在它的旁边。在这个任务中，BERT模型被训练来识别两个句子是否可以同时出现。
 
**3\. Transformer结构**
 
为了解决上述两项任务，BERT使用了多层Transformer模块作为编码器。单词向量被传递到各个层，以捕获其含义，并为基本模型生成大小为768的向量。
 
![](https://pic2.zhimg.com/v2-2797503b2f0e9275bd2d0466e47e66a5_b.jpg)
 
Jay Alammar有一篇非常好的文章：[http://jalammar.github.io/bert/](https://link.zhihu.com/?target=http%3A//jalammar.github.io/bert/)，更深入地阐述了Transformer的内部机制。
 
### BERT的问题
 
BERT发布后，在排行榜上产生了许多NLP任务的最新成果。但是，模型非常大，导致了一些问题。“ALBERT”论文将这些问题分为两类：
 
1、**内存限制和通信开销**：
 
考虑一个包含一个输入节点、两个隐藏节点和一个输出节点的简单神经网络。即使是这样一个简单的神经网络，由于每个节点的权重和偏差，也会有7个参数需要学习。
 
![](https://pic1.zhimg.com/v2-955ed3bb42debfc88c5d35ae147d4b70_b.jpg)

BERT-large模型是一个复杂的模型，它有24个隐含层，在前馈网络和注意头中有很多节点，所以有3.4亿个参数。如果你想在BERT的基础上进行改进，你需要大量的计算资源的需求来从零开始进行训练并在其上进行迭代
 
![](https://pic3.zhimg.com/v2-c30e166f19a018432a2354ac329b2eaa_b.jpg)
 
这些计算需求主要涉及gpu和TPUs，但是这些设备有内存限制。所以，模型的大小是有限制的。  
 
分布式训练是解决这个问题的一种流行方法。我们以BERT-large上的数据并行性为例，其中训练数据被分到两台机器上。模型在两台机器上对数据块进行训练。如图所示，你可以注意到在梯度同步过程中要传输的大量参数，这会减慢训练过程。同样的瓶颈也适用于模型的并行性，即我们在不同的机器上存储模型的不同部分。
 
![](https://pic2.zhimg.com/v2-2ecac204965c9acbdaec48a3c4f413e9_b.jpg)
 
2、**模型退化**  
 
最近在NLP研究社区的趋势是使用越来越大的模型，以获得在排行榜上的最先进的性能。ALBERT 的研究表明，这可能会导致收益退化。
 
在论文中，作者做了一个有趣的实验。
 
> 如果更大的模型可以带来更好的性能，为什么不将最大的BERT模型(BERT-large)的隐含层单元增加一倍，从1024个单元增加到2048个单元呢?
 
他们称之为“BERT-xlarge”。令人惊讶的是，无论是在语言建模任务还是在阅读理解测试(RACE)中，这个更大的模型的表现都不如BERT-large模型。
 
![](https://pic1.zhimg.com/v2-32e5c234b4128584c752d11cb3751a48_b.jpg)
 
从原文给出的图中，我们可以看到性能是如何下降的。BERT-xlarge的性能比BERT-large差，尽管它更大并且有更多的参数。  
 
![](https://pic3.zhimg.com/v2-1a9c618bd33e5fd93ce6d98ef1f50f66_b.jpg)
 
### 从BERT到ALBERT

一个有趣的现象：
- 当我们让一个模型的参数变多的时候，一开始模型效果是提高的趋势，但一旦复杂到了一定的程度，接着再去增加参数反而会让效果降低，这个现象叫作“model degratation"。

albert要解决的问题：
1. 让模型的参数更少 
2. 使用更少的内存 
3. 提升模型的效果

ALBERT提出了三种优化策略，做到了比BERT模型小很多的模型，但效果反而超越了BERT， XLNet。
- **低秩分解** `Factorized Embedding Parameterization`. 针对于Vocabulary Embedding。在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。通过对Embedding 部分降维来达到降低参数的作用。在最初的BERT中，以Base为例，Embedding层的维度与隐层的维度一样都是768，而词的分布式表示，往往并不需要这么高的维度，如Word2Vec时代就多采用50或300这样的维度。为了解决这些问题他们提出了一个基于factorization的方法。他们没有直接把one-hot映射到hidden layer, 而是先把one-hot映射到低维空间之后，再映射到hidden layer。这其实类似于做了矩阵的分解。
  - ![](https://www.zhihu.com/equation?tex=+O%28V+%5Ctimes+H%29+%5Cto+O%28V+%5Ctimes+E+%2B+E+%5Ctimes+H%29+)
  - V：词表大小；H：隐层维度；E：词向量维度
  - 以 BERT-Base 为例，Base中的Hidden size 为768， 词表大小为3w，此时的参数量为：768 * 3w = 23040000。 如果将 Embedding 的维度改为 128，那么此时Embedding层的参数量为： 128 * 3w + 128 * 768 = 3938304。二者的差为19101696，大约为19M。我们看到，其实Embedding参数量从原来的23M变为了现在的4M，似乎变化特别大，然而当我们放到全局来看的话，BERT-Base的参数量在110M，降低19M也不能产生什么革命性的变化。因此，可以说Embedding层的因式分解其实并不是降低参数量的主要手段。
- **层间参数共享** `Cross-layer parameter sharing`. Zhenzhong博士提出每一层的layer可以共享参数，这样一来参数的个数不会以层数的增加而增加。所以最后得出来的模型相比BERT-large小18倍以上。
  - 本质上就是对参数共享机制在Transformer内的探讨。Transformer两大主要的组件：FFN与多头注意力机制。
- **SOP替代NSP** `Inter-sentence coherence loss`. 在BERT的训练中提出了next sentence prediction loss, 也就是给定两个sentence segments, 然后让BERT去预测它俩之间的先后顺序，但在ALBERT文章里提出这种是有问题的，其实也说明这种训练方式用处不是很大。 所以他们做出了改进，他们使用的是setence-order prediction loss (SOP)，其实是基于主题的关联去预测是否两个句子调换了顺序。

模型压缩有很多手段，包括剪枝，参数共享，低秩分解，网络结构设计，知识蒸馏等。ALBERT 也没能逃出这一框架，它其实是一个相当工程化的思想
 
ALBERT在BERT 的基础上提出了一些新颖的想法来解决这些问题：
 
1、**跨层参数共享**
 
BERT-large模型有24层，而它的基础版本有12层。随着层数的增加，参数的数量呈指数增长。
 
![](https://pic4.zhimg.com/v2-da47ac3ebd17d165d957b4959294118f_b.jpg)
 
为了解决这个问题，ALBERT使用了跨层参数共享的概念。为了说明这一点，让我们看一下12层的BERT-base模型的例子。我们只学习第一个块的参数，并在剩下的11个层中重用该块，而不是为12个层中每个层都学习不同的参数。  
 
![](https://pic1.zhimg.com/v2-e5457046cbaeb14334cc8d9de72f128c_b.jpg)
 
我们可以只共享feed-forward层的参数，只共享注意力参数，也可以共享整个块的参数。论文对整个块的参数进行了共享。  
 
与BERT-base的1.1亿个参数相比，ALBERT模型只有3100万个参数，而使用相同的层数和768个隐藏单元。当嵌入尺寸为128时，对精度的影响很小。精度的主要下降是由于feed-forward层的参数共享。共享注意力参数的影响是最小的。
 
![](https://pic3.zhimg.com/v2-5e3a3fe7c409ca694d28ab4f68e16356_b.jpg)
 
跨层参数策略对性能的影响
 
2、**句子顺序预测 (SOP)**
 
BERT引入了一个叫做“**下一个句子预测**”的二分类损失。这是专门为提高使用句子对，如“自然语言推断”的下游任务的性能而创建的。基本流程为：
*   从训练语料库中取出两个连续的段落作为正样本
*   从不同的文档中随机创建一对段落作为负样本

![](https://pic2.zhimg.com/v2-90f2e0b33156326e46387959ce388c29_b.jpg)
 
像ROBERTA和XLNET这样的论文已经阐明了NSP的无效性，并且发现它对下游任务的影响是不可靠的。在取消NSP任务之后，多个任务的性能都得到了提高。  
 
因此，ALBERT提出了另一个任务**“句子顺序预测”**。关键思想是:
*   从同一个文档中取两个连续的段落作为一个正样本
*   交换这两个段落的顺序，并使用它作为一个负样本
 
![](https://pic3.zhimg.com/v2-c7b91c17a1b42283817b17a2d566a6d6_b.jpg)
 
这使得模型能学习到更细粒度的关于段落级的一致性的区别。
 
ALBERT推测NSP是无效的，因为与掩码语言建模相比，它并不是一项困难的任务。在单个任务中，它混合了主题预测和连贯性预测。主题预测部分很容易学习，因为它与掩码语言建模的损失有重叠。因此，即使NSP没有学习连贯性预测，它也会给出更高的分数。
 
SOP提高了下游多句编码任务(SQUAD 1.1, 2.0, MNLI, SST-2, RACE)的性能。
 
![](https://pic3.zhimg.com/v2-c9ce732b00678780cb2ae1fc9e219612_b.jpg)

在这里我们可以看到，在SOP任务上，一个经过NSP训练的模型给出的分数只比随机基线略好一点，但是经过SOP训练的模型可以非常有效地解决NSP任务。这就证明SOP能带来更好的学习表现。
 
3、**嵌入参数分解**
 
在BERT中，使用的embeddings(word piece embeddings)大小被链接到transformer块的隐藏层大小。Word piece embeddings使用了大小为30,000的词汇表的独热编码表示。这些被直接投射到隐藏层的隐藏空间。
 
假设我们有一个大小为30K的词汇表，大小为E=768的word-piece embedding和大小为H=768的隐含层。如果我们增加了块中的隐藏单元尺寸，那么我们还需要为每个嵌入添加一个新的维度。这个问题在XLNET和ROBERTA中也很普遍。

![](https://pic4.zhimg.com/v2-262f2f051d69dda14c1e437524f84e43_b.jpg)
 
ALBERT通过将大的词汇表嵌入矩阵分解成两个小的矩阵来解决这个问题。这将隐藏层的大小与词汇表嵌入的大小分开。这允许我们在不显著增加词汇表嵌入的参数大小的情况下增加隐藏的大小。
 
![](https://pic1.zhimg.com/v2-e9fb494e0067ae3770d09c7299e6aa94_b.jpg)
 
我们将独热编码向量投影到E=100的低维嵌入空间，然后将这个嵌入空间投影到隐含层空间H=768。
 
### 结果
 
*   比BERT-large模型缩小了18x的参数
*   训练加速1.7x
*   在GLUE, RACE和SQUAD得到SOTA结果：
  *   RACE：89.4%\[提升45.3%\]
  *   GLUE Benchmark：89.4
  *   SQUAD2.0 f1 score：92.2

ALBERT与BERT模型之间参数情况

![](https://pic3.zhimg.com/80/v2-7f6261989fc1b9b8d2ce05d4249911ce_720w.jpg)

在benchmark上的效果
![](https://pic1.zhimg.com/80/v2-5e320cd88fbc16d4038eebfaf586a9f4_720w.jpg)
![](https://pic4.zhimg.com/80/v2-969d2eefa07339b637d6333817d13a0f_720w.jpg)


**总结**
 
ALBERT标志着构建语言模型的重要一步，该模型不仅达到了SOTA，而且对现实世界的应用也是可行的。
 
- 英文原文：[https://amitness.com/2020/02/al](https://amitness.com/2020/02/albert-visual-summary/)
- albert的[中文预训练模型](https://github.com/brightmart/albert_zh)


## RoBERTa —— 数据量+训练方式

【2022-5-9】[Roberta: Bert调优](https://zhuanlan.zhihu.com/p/260693956)

Roberta，是Robustly Optimized BERT Approach的简称。
- Robustly用词很赞，既有“鲁棒的”，又有”体力的”。Roberta是一片实验为基础的论文，有点体力活的意思，但是结果又非常的鲁棒可信赖。

RoBERTa 是BERT的成功变种之一，主要有4个简单有效的变化：
- 1）去除NSP任务；
- 2）大语料与更长的训练步数：batch size更大，数据更多；
- 3）更长的训练句子；
- 4）Masking策略——静态与动态：**动态**改变 [ MASK ] 模式。复制多份数据

Bert 的优化版，模型结构与 Bert 完全一样，只是在数据量和训练方法上做了改进。简单说就是更大的数据量，更好的训练方式，训练得更久一些。
- 相比原生 Bert 的16G训练数据，RoBerta 训练数据量达到了161G；
- 去除了 NSP 任务，研究表明 NSP 任务太过简单，不仅不能提升反倒有损模型性能；
- MLM 换成 Dynamic Masking LM；
- 更大的 Batch size 以及其他超参数的调优；

RoBERTa 在 BERT 的基础上取得了令人印象深刻的结果。而且，RoBERTa 已经指出，**NSP 任务对于 BERT 的训练来说相对没用**。

结论：
- NSP不是必须的loss
- Mask的方式虽不是最优但是已接近。
- 增大batch size和增大训练数据能带来较大的提升。

由于Roberta出色的性能，现在很多应用都是基于Roberta而不是原始的Bert去微调了。

（1）动态 mask
- Bert中是在训练数据中静态的标上Mask标记，然后在训练中是不变的，这种方式就是**静态**的。
- Roberta尝试了一种动态的方式，说是动态，其实也是用静态的方式实现的，把数据复制10份，每一份中采用不同的Mask。这样就有了10种不同的Mask数据。
- 从结果中，可以看到动态mask能带来微小的提升。

（2）NSP任务

Bert的模型输入中是由两个segment组成的，因而就有两个问题：
- 两个segment是不是必要？
- 为什么是segment而不是单个的句子？

因此设置了四个实验：
- Segment-Pair + NSP
- Sentence-Pair + NSP: 只用了sentence以后，输入的长度会变少，为了使得每一步训练见到的token数类似，在这里会增大batch size
- Full-Sentence: 每一个样本都是从一个文档中连续sample出来的，如果跨过文档边界，就添加一个[SEP]的标记，没有NSP损失。
- Doc-Sentence: 类似于Full-Sentence，但是不会跨过文档边界。
从实验结果中可以看到，改用Sentence-Pair会带来一个较大的损失。猜测是因为这样无法捕捉long-term的依赖。

另外，Full-Sentence和Doc-Sentence能够带来微小的提升，说明NSP不是必须的。
- 这点跟Bert中的消融实验结论相反，但是请注意它们的输入还是不同的，原始Bert中的输入是Segment-Pair，有50%/50%的采样，而Full/Doc-Sentence中则是从文章中连续sample来的句子。

因为Doc-Sentence会导致不同的batch_size（因为要保证每个batch见到的token数类似），所以在Roberta中，使用Full-Sentence模式。

（3）Large-Batch

现在越来越多的实验表明增大batch_size会使得收敛更快，最后的效果更好。原始的Bert中，batch_size=256，同时训练1M steps。

在Roberta中，实验了两个设置：
- batch_size=2k, 训练125k steps。
- batch_size=8k, 训练31k steps。
从结果中看，batch_size=2k时结果最好。

## XLNet

XLNet 对 Bert 做了较大的改动，二者在**模型结构**和**训练方式**上都有不小的差异。
- Bert 的 MLM 在预训练时有 MASK 标签，但在使用时却没有，导致训练和使用时出现不一致；并且 MLM 不属于 Autoregressive LM，不能做生成类任务。
- XLNet 采用 PML(Permutation Language Model) 避免了 MASK 标签的使用，且属于 Autoregressive LM，可以做生成任务。

Bert 使用的 Transformer 结构对文本的**长度有限制**，为更好地处理长文本，XLNet 采用升级版的 Transformer-XL。

要点
- 全排列语言模型
- transformer-XL
- 跟多的数据

Yang等人(2019年)认为，现有的基于自编码的预训练语言模型(例如BERT)会遭受**预训练**和**微调**阶段的差异，因为masking符号\[MASK]永远不会出现在微调阶段。 

为了缓解这个问题，他们提出了XLNet，它基于Transformer-XL(Dai等人，2019)。 XLNet主要通过两种方式进行修改。 
- 首先是所有排列上的输入**因式分解**的最大化期望似然，在此将它们称为**排列语言模型**(PLM)。 
- 其次是将**自编码**语言模型更改为**自回归**模型，这与传统的统计语言模型相似

## ERNIE（融合知识）

思路：
- 百度版：**全词**mask
- 清华版：**知识图谱**融入

ERNIE(通过知识整合增强表示)(Sun等人，2019a)旨在优化BERT的masking过程，其中包括**实体**级masking和**短语**级masking。 与在输入中选择随机单词不同，实体级mask将mask通常由多个单词组成的命名实体。短语级mask是mask连续的单词，类似于N-gram mask策略

## NeZha 挪吒（华为）

[华为开源预训练语言模型「哪吒」：编码、掩码升级，提升多项中文 NLP 任务性能](https://www.leiphone.com/category/yanxishe/YmSMHZUOCekn9Cyr.html)

【2019-12-5】华为诺亚方舟实验室语音语义团队与海思、云BU等团队合作，共同研究大规模预训练模型的训练技术，发布了自己的中文预训练语言模型NEZHA(NEural ContextualiZed Representation for CHinese LAnguage Understanding，中文：哪吒)。

- NEZHA [论文地址](https://arxiv.org/pdf/1909.00204.pdf)
- 关于知识蒸馏模型 TinyBERT 详细解读，可参考[往期内容](https://mp.weixin.qq.com/s/f2vxlhaGW1wnu8UYrvh-tA)
- Github [开源地址](https://github.com/huawei-noah/Pretrained-Language-Model)（包含 NEZHA 与 TinyBERT )  

NEZHA是基于预训练语言模型BERT的改进模型，BERT通过使用大量无监督文本进行预训练，其包含两个预训练任务：Masked Language Modeling（MLM）和 Next Sentence Prediction （NSP），分别预测句子里被Mask的字（在构造训练数据时，句子里的部分字被Mask）和判断训练句对里面是不是真实的上下句。

三头六臂 NEZHA（哪吒）
- **函数式**相对位置编码
- 全词覆盖的实现

### 函数式相对位置编码

位置编码有**函数式**和**参数式**两种
- 函数式通过定义函数直接计算就可以了。
- 参数式中位置编码涉及两个概念，一个是距离；二是维度。其中，Word Embedding 一般有几百维，每一维各有一个值，一个位置编码的值正是通过位置和维度两个参数来确定。

NEZHA 预训练模型则采用了**函数式**相对位置编码，其输出与注意力得分的计算涉及到他们相对位置的正弦函数，这一灵感正是来源于 Transformer 的绝对位置编码，而相对位置编码则解决了在 Transformer 中，每个词之间因为互不知道相隔的距离引发的一系列资源占用问题。

Transformer 最早只考虑了**绝对位置编码**，而且是函数式的；后来 BERT 的提出就使用了参数式，而参数式训练则会受收到句子长度的影响，BERT 起初训练的句子最长为 512，如果只训练到 128 长度的句子，在 128~520 之间的位置参数就无法获得，所以必须要训练更长的语料来确定这一部分的参数。

而在 NEZHA 模型中，距离和维度都是由正弦函数导出的，并且在模型训练期间是固定的。也就是说，位置编码的每个维度对应一个正弦，不同维度的正弦函数具有不同的波长，而选择固定正弦函数，则可以使该模型具有更强的扩展性；即当它遇到比训练中序列长度更长的序列时，依然可以发挥作用。

### 全词覆盖

现在的神经网络模型无论是在语言模型还是机器翻译任务中，都会用到一个词表；而在 Softmax 时，每个词都要尝试比较一下。每次运算时，所有词要都在词表中对比一遍，往往一个词表会包含几万个词，而机器翻译则经常达到六七万个词，因此，词表是语言模型运算中较大的瓶颈。

而 NEZHA 预训练模型，则采用了全词覆盖（WWM）策略，当一个汉字被覆盖时，属于同一个汉字的其他汉字都被一起覆盖。该策略被证明比 BERT 中的随机覆盖训练（即每个符号或汉字都被随机屏蔽）更有效。
- ![](https://static.leiphone.com/uploads/new/images/20191205/5de8d5448fe36.jpg?imageView2/2/w/740)

### 混合精度训练及 LAMB 优化器

在 NEZHA 模型的预训练中，研究者采用了混合精度训练技术。该技术可以使训练速度提高 2-3 倍，同时也减少了模型的空间消耗，从而可以利用较大的批量。

传统的深度神经网络训练使用 FP32（即单精度浮点格式）来表示训练中涉及的所有变量（包括模型参数和梯度）；而混合精度训练在训练中采用了多精度。具体而言，它重点保证模型中权重的单精度副本（称为主权重），即在每次训练迭代中，将主权值舍入 FP16（即半精度浮点格式），并使用 FP16 格式存储的权值、激活和梯度执行向前和向后传递；最后将梯度转换为 FP32 格式，并使用 FP32 梯度更新主权重。


## MacBERT —— RoBERTa改进

[MacBERT: 中文自然语言预训练模型](https://zhuanlan.zhihu.com/p/333202482)

MacBERT在多个方面对RoBERTa进行了改进，尤其是采用 MLM 作为校正(Mac)的masked策略。用**相似单词**mask，减轻了预训练和微调阶段两者之间的差距，这已被证明对下游任务是有效的

MacBERT与BERT共享相同的预训练任务，但有一些修改。 对于MLM任务，我们执行以下修改。
- 用**全词**masked以及 **Ngram** masked策略来选择候选token来masked，单词级别的unigram到4-gram的比例为40％，30％，20％，10％。
- 不用\[MASK] token进行mask，因为在token 微调阶段**从未出现过**\[MASK]，提议用**类似单词**进行masking。 通过使用基于word2vec(Mikolov et al。，2013)相似度计算的同义词工具包(Wang and Hu，2017)获得相似的单词。 如果选择一个N-gram进行masked，分别找到相似的单词。 在极少数情况下，当没有相似的单词时，会降级以使用**随机单词**替换。
- 对15％比例的输入单词进行masking，其中80％替换为相似的单词，10％将替换为随机单词，其余10％则保留原始单词。
对于类似NSP的任务，执行ALBERT (Lan等人，2019)引入的句子顺序预测(SOP)任务，其中通过切换两个连续句子的原始顺序来创建负样本。

## 其它

UniLM、MASS 、SpanBERT 和 ELECTRA

## 资料

- [Bert时代的创新（应用篇）：Bert在NLP各领域的应用进展](https://zhuanlan.zhihu.com/p/68446772)


# 结束