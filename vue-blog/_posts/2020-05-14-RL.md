---
layout: post
title:  "强化学习-Reinforcement Learning"
date:   2020-05-14 17:34:00
categories: 深度学习
tags: 深度学习 强化学习 增强学习 动态规划 贝尔曼方程 量化交易 游戏 蒙特卡洛
excerpt: AI=DL+RL，那么RL强化学习是什么，包含哪些内容，有哪些典型应用？
author: 鹤啸九天
mathjax: true
permalink: /rl
---

* content
{:toc}

# 总结

- 【2022-2-17】北大老师写的[深度强化学习](https://deepreinforcementlearningbook.org/)，电子版（[中文简体](https://deepreinforcementlearningbook.org/assets/pdfs/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0(%E4%B8%AD%E6%96%87%E7%89%88-%E5%BD%A9%E8%89%B2%E5%8E%8B%E7%BC%A9).pdf)、繁体、英文都有）
- 更多[Demo地址](http://wqw547243068.github.io/demo)
- 强化学习圣经：《强化学习导论》第二版（附[PDF](http://www.incompleteideas.net/book/the-book.html)下载），配套[python代码](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)
- [无需博士学位的TensorFlow深度强化学习教程](https://www.bilibili.com/video/av23286922/) by Martin Gorner

<iframe src="//player.bilibili.com/player.html?aid=23286922&bvid=BV1MW411F7yA&cid=38785810&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

- 李宏毅深度强化学习(国语)课程(2018)，[B站视频](https://www.bilibili.com/video/av24724071)

<iframe src="//player.bilibili.com/player.html?aid=24724071&bvid=BV1MW411w79n&cid=41583412&page=4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>
- 【2021-3-18】[大年初一，一起来深度强化学习](https://zhuanlan.zhihu.com/p/350506752)，Thomas Simonini 的 Deep Reinforcement Learning Course的课程笔记
- 【2020-7-4】【(EEML2020)强化学习教程(Colab)】“[EEML2020 RL Tutorial](https://github.com/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb)” 
- 【2020-7-5】XRL：可解释强化学习《[XRL: eXplainable Reinforcement Learning](https://towardsdatascience.com/xrl-explainable-reinforcement-learning-4cd065cdec9a)》by Meet Gandhi
- 【2019】强化学习课程：[2019斯坦福大学最新强化学习课程：CS234](https://www.bilibili.com/video/BV1Nb411s7pP)，系列，B站视频，CSDN笔记[总结](https://blog.csdn.net/solo95/category_9298323.html)
  - <iframe src="//player.bilibili.com/player.html?aid=47812079&bvid=BV1Nb411s7pP&cid=83748673&page=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" width="100%" height="600"> </iframe>
- 【2021-2-27】【[杜伦大学10小时强化学习课程](https://www.bilibili.com/video/BV1vN411X7qB/)】“Reinforcement Learning Lectures  Durham University” by Chris G. Willcocks
  - <iframe src="//player.bilibili.com/player.html?aid=501767046&bvid=BV1vN411X7qB&cid=302763703&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>
- 【2021-3-17】[强化学习的“神话”和“鬼话”](https://zhuanlan.zhihu.com/p/196421049)，Csaba Szepesvári在2020年数据挖掘顶会KDD的Deep Learning Day做了题为Myths and Misconceptions in Reinforcement Learning的讲座。Csaba Szepesvári是阿尔伯塔大学计算机系教授，在Deepmind领导Foundations团队。他于2010年出版Algorithms for Reinforcement Learning. 最近出版Bandit Algorithms. 他于2006年发表题为Bandit based Monte-Carlo Planning的论文，提出UCT算法，对AlphaGo的研发起到了关键作用。
  - 1）要不要学习RL
    - 误解：有人说，RL就是指一些特定的算法，比如TD、DQN、PPO，等等；而像ES(evolutionary search, 进化搜索)、随机搜索、SSL(self-supervised learning, 自我监督学习)等等这些就不是RL.
    - 三类基本的RL问题：Online RL（智能体直接与环境交互学习）, Batch RL（历史数据中学习）, Planning/simulation optimization（规划/仿真优化）. 当然有很多变种。
  - 2）RL是不是有很多问题
  - 3）RL与相邻学科的关系如何
  - 然后讨论一些“Meta consideration”
  - [原版ppt及youtube视频](https://sites.ualberta.ca/~szepesva/talks.html)，[model based RL ppt](https://www.dropbox.com/s/8b4eivix82tfp2z/ModelBasedRL2.pptx?dl=0)
- 【2021-3-19】人工智能拒绝内卷，AI研习社文章：[开局一头狼六只羊，这个狼吃羊的AI火了！傻狼拒绝内卷：抓羊可太累了，我只想自杀......](https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&mid=2247585920&idx=1&sn=ad78a94ded54c54f092d4694e11ece25&chksm=90959913a7e210053febe37a27bff456a136aec2c8e3dd4fc15d71cdcea648a5663105f553ca&mpshare=1&scene=1&srcid=0319JgBKEY71NSi5iVRaOgZS&sharer_sharetime=1616158813661&sharer_shareid=b8d409494a5439418f4a89712efcd92a&version=3.1.0.6189&platform=mac#rd)
  - ![](https://wx3.sinaimg.cn/mw690/007dt7foly1gog7nqut5hj30v81glqb8.jpg)
  - ![](https://mmbiz.qpic.cn/mmbiz_png/cNFA8C0uVPs54GiaOqXqvGBOQiaLTD7aZfbvfibRcRUqz4PDXWSFTBWn2JApWS5uPsnFqxtRPDEaKsZic0QBGls3ww/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
  - ![](https://pic2.zhimg.com/80/v2-abc6b3d9a109ae9c1a182f322f069363_1440w.jpg?source=1940ef5c)
    - 知乎曾伊言的回答[这样的AI是真的吗？](https://www.zhihu.com/question/448931860/answer/1779097110) ：这远不是通用人工智能，回报函数(reward function)的设计者若理解期望折扣回报(expected discounted return)，就不会犯这种错，也不需要研究三天。
  - ①狼就是打工人…每秒扣的是青春和时间，羊是永远达不到的“升职、加薪、迎娶白富美、走上人生巅峰”，撞石头就是躺平摸鱼
  - ②阿西莫夫三定律是多么重要啊，不然以效率和结果为一切指标的ai，非常可能在很多场景中计算后发现，人类+自己毁灭后效益最大，然后采取灭世措施。
  - [微博原贴](https://weibo.com/6611961566/K5J8t8XfM?type=comment#_rnd1616126336679)
  - 星尘研表示狼自杀的错误是很多东西共同影响产生的，最主要的一个错误是迭代次数太少，20W次完全不够学，后面提高到100W次起步, 效果直线上升。
  - ![](https://mmbiz.qpic.cn/mmbiz_gif/cNFA8C0uVPs54GiaOqXqvGBOQiaLTD7aZf8IMo27fyqEvd9WG6ZTYc8l4OvicctJD2GWkKDc89VBxHmqBib5ZQKaOw/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)
  - [今天微博上好像有一个内卷AI狼...](https://www.bilibili.com/video/BV16X4y1V7Yu?from=search&seid=10508566856251031934)
  - <iframe src="//player.bilibili.com/player.html?aid=714560902&bvid=BV16X4y1V7Yu&cid=309180940&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"  width="100%" height="600"> </iframe>

- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
- Richard S. Sutton and Andrew G. Barto的[Reinforcement Learning: An Introduction, Second edition](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)


# 什么是强化学习

## 定义

- 强化学习的思想为：**agent**通过与**环境**的交互（其实就是试错）得到**奖励**（可正可负，负的奖励就是惩罚）作为反馈，从而做出对应的**action**。这个思想很符合自然经验，小时候学走路，摔了会痛（奖励为负），走得稳了有糖吃（奖励为正），为了多吃点糖（取得更多的奖励），你最终学会了走路。
  - ![](https://pic1.zhimg.com/80/v2-74f512eaf6fc5f68d424ec37dc7bb144_720w.jpg)
- 接下来给出强化学习的正式定义：

> Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.

## 基本组件

- 强化学习强化学习系统由**智能体**（Agent）、**状态**（state）、**奖赏**（reward）、**动作**（action）和**环境**（Environment）五部分组成，如下图所示。
  - ![](https://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavlKDwRa4UIy5IjcjDAm1CPvPMxyK50fy1B5VEd2pt4NeCRaeLlkXMibyiam9cRsmPo3VLpZfpkuNLw/640?wx_fmt=other)
  - `Agent`：智能体是整个强化学习系统核心。它能够感知环境的状态（State），并且根据环境提供的奖励信号（Reward），通过学习选择一个合适的动作（Action），来最大化长期的Reward值。简而言之，Agent就是根据环境提供的Reward作为反馈，学习一系列的环境状态（State）到**动作**（Action）的映射，动作选择的原则是最大化未来累积的Reward的概率。选择的动作不仅影响当前时刻的Reward，还会影响下一时刻甚至未来的Reward，因此，Agent在学习过程中的基本规则是：如果某个动作（Action）带来了环境的正回报（Reward），那么这一动作会被加强，反之则会逐渐削弱，类似于物理学中条件反射原理。
  - `Environment`：环境会接收Agent执行的一系列的动作（Action），并且对这一系列的动作的好坏进行评价，并转换成一种可量化的（标量信号）Reward反馈给Agent，而不会告诉Agent应该如何去学习动作。Agent只能靠自己的历史（History）经历去学习。同时，环境还像Agent提供它所处的状态（State）信息。
  - `Reward`：环境提供给Agent的一个可量化的标量反馈信号，用于评价Agent在某一个时间步所做action的好坏。强化学习就是基于一种最大化累计奖赏假设：强化学习中，Agent进行一系列的动作选择的目标是最大化未来的累计奖赏。
  - `State`：状态指Agent所处的环境信息，包含了智能体用于进行Action选择的所有信息，它是历史（History）的一个函数：St = f（Ht）。
- 强化学习的主体是Agent和环境Environment。Agent为了适应环境，做出的一系列的动作，使最终的奖励最高，同时在此过程中更新特定的参数。实际上可以把强化学习简单理解成是一种循环，具体的工作方式如下：
  - 智能体从环境中获取一个状态St；
  - 智能体根据状态St采取一个动作at；
  - 受到at的影响，环境发生变化，转换到新的状态St+1；
  - 环境反馈给智能体一个奖励（正向为奖励，负向则为惩罚）。
- 参考：[强化学习在智能对话上的应用](https://blog.csdn.net/Tencent_TEG/article/details/88859179)

- **动作空间**
  - 动作空间为在一个环境中所有可能动作的集合，可以是离散空间，比如有的游戏只能上下左右移动，也可以是连续空间，比如自动驾驶时的转角可能是无穷的。
- **奖励**与**折扣**
  - 奖励就是反馈，它能让agent知道动作是好是坏，每个时刻的累积奖励可以写为
    - ![](https://www.zhihu.com/equation?tex=%5C%5BR%28%5Ctau+%29+%3D+%7Br_%7Bt+%2B+1%7D%7D+%2B+%7Br_%7Bt+%2B+2%7D%7D+%2B+%7Br_%7Bt+%2B+3%7D%7D+%2B+...%5C%5D)
  - 其中τ代表状态和动作的序列。但是实际上我们并不直接这样相加，因为预测太远的事情总是不准的，我们一般选择更看重眼前的奖励，所以需要对未来的奖励进行衰减
    - ![](https://www.zhihu.com/equation?tex=%5C%5BR%28%5Ctau+%29+%3D+%7Br_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7Br_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7Br_%7Bt+%2B+3%7D%7D+%2B+...%5C%5D)
  - 其中γ是衰减因子来权衡我们对长远奖励的重视程度。![](https://www.zhihu.com/equation?tex=R_%7Bt%7D%3Dr_%7Bt%2B1%7D%2B%5Cgamma+r_%7Bt%2B2%7D%2B%5Cgamma%5E%7B2%7D+r_%7Bt%2B3%7D%2B%5Ccdots%3D%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D+%5Cgamma%5E%7Bk%7D+r_%7Bt%2Bk%2B1%7D+%5Ctag+%7B2%7D+)
- 强化学习假设
  - ① **奖励假说**：所有的目标可以被描述为**最大化期望回报**，为了做出最好的决策，需要去最大化期望回报。
  - ② **马尔可夫决策过程**：agent每次只需根据**当前**的状态就能做出决策，而不依赖于**过去**的状态和动作信息。
- 强化学习的任务有两类：
  - **周期**任务（比如你在玩超级玛丽，每次从一条新的生命开始，直到游戏人物死亡或者到达终点结束）
  - **连续**任务（比如做自动化股票交易，这里就没有起始和终点这一说了，agent在做出做好决策的同时不断和环境进行交互）
- Exploration/ Exploitation的权衡
  - **Exploration**就是去探索更多的环境信息，**Exploitation**就是利用已知的环境信息最大化奖励。在强化学习中，我们既要Exploration又要Exploitation，这样才能做出更好的决策。这里举了个饭店选择的例子：Exploration就是去尝试你没吃过的饭店，这样可能会踩雷，但是也可能挖掘到更好吃的饭店！Exploitation就是每天去你知道的好吃的饭店吃饭，但这样可能错过一些更好吃的饭店。

强化学习里面的“**Model**”指的是 S->S'的**概率分布**，即在当前状态下实施某个动作会抵达什么新状态。
Model-free的设定中，不知道S-S'的概率分布，那么怎么才能得到v或者q呢？很简单，做**取样**（sampling

# 算法

## 求解方法

- 既然需要最大化期望回报来做出更好的决策，那么这一过程如何进行的呢？
- 假设agent接收到一个状态然后给出相应的动作，那么这一过程就可以理解成是一个控制律（一个从状态到动作的函数）
  - ![](https://pic3.zhimg.com/80/v2-009413beffb5736398eacebd6811c8fe_720w.jpg)
  - 控制律π好比agent的大脑，也是我们希望去学习出来的，我们的目标就是找到最好的控制律使得期望回报最大化。
- （1）**直接法**
  - 给定状态并告诉agent应该采取什么动作，也就是直接学习这个控制律，称为**policy-based**方法。
  - 控制率是确定的（即状态和动作一一对应） ![](https://www.zhihu.com/equation?tex=a%3D%5Cpi%28s%29)，如动作的概率分布![](https://www.zhihu.com/equation?tex=%5Cpi%28a%7Cs%29%3DP%28A%7Cs%29)
  - ![](https://pic1.zhimg.com/80/v2-8350cacc652c29f252d0b27371b0cee8_720w.jpg)
- （2）**间接法**
  - 不告诉agent什么动作是最好的，而是告诉它什么状态是更有价值的，而能到达更好价值的状态需要采取什么动作也就知道了，称为**value-based**方法。
  - 需要训练一个函数能将当前状态映射成（某一控制律下）期望回报
    - ![](https://www.zhihu.com/equation?tex=%5C%5B%7Bv_%5Cpi+%7D%28s%29+%3D+%5Cmathbb%7BE%7D%7B_%5Cpi+%7D%5Cleft%5B+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7BR_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7BR_%7Bt+%2B+3%7D%7D+%2B+...%7C%7BS_t%7D+%3D+s%7D+%5Cright%5D%5C%5D)
  - ![](https://pic3.zhimg.com/80/v2-6e1a3720ceeedfdb1d518ddc0f0cc232_720w.jpg)
- 总结
  - 无论是采用policy还是value-based方法，都会有一个policy，在policy-based方法中这个policy是通过训练直接得到的，而在value-based方法中我们不需要去训练policy，这里的policy只是个简单的函数，比如贪婪策略根据最大值来选择动作。两者之间的对应关系可以如下所示
  - ![](https://pic4.zhimg.com/80/v2-40781e074536a63315e3d3230c3e6ac7_720w.jpg)

## 基本概念

### 值函数（value-function）的定义

- **Q-learning**是一种value-based的强化学习方法，首先我们需要定义两个value函数：
  - 一个是state-value函数，即
    - ![](https://pic1.zhimg.com/80/v2-678f6e45b921862ac2ed112a7e33433c_720w.jpg)
    - 如从期望价值为-7的状态出发，根据贪婪策略，动作为右、右、右、右、下、下、右、右。
    - ![](https://pic1.zhimg.com/80/v2-15d39e8e4870b99693bc56b07dae4bd0_720w.jpg)
  - 另一个是action-value函数，在动作-价值函数中，函数的输入为动作和状态的pair，输出仍然是期望回报
    - ![](https://pic3.zhimg.com/80/v2-f299bc1a427cc355e67893f4d6ac5992_720w.jpg)
- action-value函数与state-value函数的区别
  - 函数值由状态和动作同时决定，也就是说一个状态四个动作对应的value可以是不一样的。
  - ![](https://pic2.zhimg.com/80/v2-6a2a0b713bda8a6f59d87ccc4327133d_720w.jpg)

### 动态规划

- 什么是**动态规划**（Dynamic Programming，DP）？
  - Dynamic means the sequential or temporal component to the problem，“动态”指的是该问题的时间序贯部分； 
  - Programming means optimising a "program", i.e. a policy，“规划”指的是去优化一个策略。
- 是不是所有问题都能用动态规划求解呢？
  - 不是的，动态规划方法需要问题包含以下两个性质：
    - **最优子结构**（Optimal substructure）：保证问题能够使用最优性原理（多阶段决策过程的最优决策序列具有这样的性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言，其后各阶段的决策序列必须构成最优策略），从而问题的最优解可以分解为子问题最优解； 
    - **重叠子问题**（Overlapping subproblems）：子问题重复出现多次，因而可以缓存并重用子问题的解。
- 恰巧，MDP满足上面两个性质
  - **贝尔曼方程**是递归的形式，把问题分解成子问题
  - 值函数保存和重用问题的解
- “规划”指的是在了解整个MDP的基础上求解最优策略，也就是清楚模型结构的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，我们可以用规划来进行预测和控制。
- 参考：[搬砖的旺财](https://zhuanlan.zhihu.com/p/51393982)

### Bellman贝尔曼方程

- 函数值如何计算？这里需要引入Bellman方程了，因为期望回报是从当前状态一直计算到终止的，对于每个状态都这样计算是繁琐的，而Bellman方程将相邻状态的函数值关联起来了，即**当前状态的函数值，等于从当前状态转移一步的直接奖励加上下一个状态的折扣函数值**，这也就是Bellman方程的**核心思想**，recursive递归
  - ![](https://pic1.zhimg.com/80/v2-30a49ed93dcfd3f80076da0c04f77ca8_720w.jpg)
  - 当前时刻Q的目标值其实是未来reward 按照$\gamma$衰减的和。如果$\gamma=0$，则说明当前状态的Q值更新，只和跳转的下一状态有关；如果$\gamma=1$，则说明未来决策的所有reward对当前状态的Q值更新有影响，且影响程度一样。
- 贝尔曼方程(Bellman Equation)，百度百科关于贝尔曼方程的介绍

> **贝尔曼方程**（Bellman Equation）也被称作**动态规划方程**（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是**动态规划**（Dynamic Programming）这些数学最佳化方法能够达到最佳化的**必要**条件。此方程把“决策问题在特定时间怎么的值”以“来自初始选择的报酬比从初始选择衍生的决策问题的值”的形式表示。借此这个方式把动态最佳化问题变成简单的子问题，而这些子问题遵守从贝尔曼所提出来的“最佳化还原理”。

- 总结：贝尔曼方程就是用来简化强化学习或者马尔可夫决策问题
- value function可以分为两部分：
  - 立即回报![](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D)
  - 后继状态的折扣价值函数![](https://www.zhihu.com/equation?tex=%5Cgamma+v%28S_%7Bt%2B1%7D%29)

![](https://pic2.zhimg.com/v2-7e5c05147555f1eaf91a257781f4cfd5_b.jpg)
![](https://pic4.zhimg.com/v2-ead0588f9d7e27a5bb0911a7f0fdc1b3_b.jpg)

- [马尔科夫决策过程之Bellman Equation（贝尔曼方程）](https://zhuanlan.zhihu.com/p/35261164)


以Q值函数为例：
- Bellman**期望**方程：![](https://pic3.zhimg.com/v2-eea4a54b293b6cbb17995413216a4a66_b.jpg)
- Bellman**最优**方程：![](https://pic1.zhimg.com/v2-4f02c51f82ed8ff9ee80e1c9592bb2d0_b.jpg)
 
其中：
- （1）Model-based的解决方案中，基于动态规划，有基于Bellman**期望**方程的Policy iteration算法；也有基于Bellman**最优**方程的Value Iteration算法；
- （2）但是在Model-free中，似乎只有基于Bellman**最优**方程的Q-learning、Sarsa等算法。

### MDP马尔科夫决策过程

Markov是一个俄国的数学家，为了纪念他在马尔可夫链所做的研究，所以以他命名了“Markov Decision Process”，以下用MDP代替。

![](https://pic3.zhimg.com/80/v2-c6cceda3bb97c4c8c572f31910dbcb26_1440w.jpg)

MDP核心思想就是下一步的State只和当前的状态State以及当前状态将要采取的Action有关，只回溯一步。比如上图State3只和State2以及Action2有关，和State1以及Action1无关。

演变关系：
- Markov Property 马尔科夫性质：![[公式]](https://www.zhihu.com/equation?tex=P%28S_%7Bt%2B1%7D%7CS_%7Bt%7D%29+%3D+P%28S_%7Bt%2B1%7D%7CS_%7B1%7D%2C....%2CS_%7Bt%7D%29)
  - 根据公式也就是说给定当前状态 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bt%7D) ,将来的状态与t时刻之前的状态已经没有关系
  - State Transition Matrix 状态转移矩阵
- 马尔科夫链：动作序列，所有序列组成马尔科夫过程
- Markov Process 马尔科夫过程：一个无记忆的随机过程，是 马尔科夫过程 一些具有马尔科夫性质的随机状态序列构成，可以用一个元组<S,P>表示，其中S是有限数量的状态集，P是状态转移概率矩阵。
- MDP (Markov Decision Processes)马尔科夫决策过程
  - 一个强化学习任务如果满足马尔可夫性则被称为马尔可夫决策过程。MDP是一个序贯决策过程，可以由一个5元组来表示： ![[公式]](https://www.zhihu.com/equation?tex=%3CS%2CA%2C%5Cmathcal%7BP%7D_%7Bs+s%5E%7B%5Cprime%7D%7D%5E%7Ba%7D%2C%5Cmathcal%7BR%7D_%7Bs+s%5E%7B%5Cprime%7D%7D%5E%7Ba%7D%2C%5Cgamma%3E)
  - 如果状态空间和行动空间都是有限的，则称为**有限马尔可夫过程**（finite MDP）
- MRP 马尔科夫奖励过程

![](https://pic4.zhimg.com/80/v2-752441d4371f8fd2435e54cd40c18fc7_1440w.jpg)

### Monte Carlo 和 Temporal Difference Learning

- 由于强化学习的agent通过与环境交互得到提升，我们需要考虑如何利用交互的经验信息来更新值函数，从而得到更好的控制律。这里主要考虑两种方法：**蒙特卡洛**（MC）法采用的是完整的一轮经验，而**时序差分**（TD）法采用的只是一步经验，下面以value-based方法举例说明两种方法的区别
- （1）**蒙特卡洛**（MC）法利用一条完整的采样来更新函数，比如下图所示，所有的状态函数值为0，采用学习率0.1，不对回报进行衰减，采用贪婪加随机的控制策略，当老鼠走超过十步时停止，最终的路径如箭头所示
  - ![](https://pic4.zhimg.com/80/v2-8f9a0c03bd8803ac3091f20ebc031383_720w.jpg)
- 得到了一系列的状态、动作、奖励，那么我们需要计算出它的回报，从而根据下面公式更新state-value函数
  - ![](https://pic3.zhimg.com/80/v2-0ab47f5e0eda7179929f955f57eb9496_720w.jpg)
- 假设吃到一个奶酪的奖励为1，则回报为 Gt = 1+0+0+0+0+0+1+1+0+0=3，对值函数进行更新为 V(S0)= V(S0)+lr*[Gt- V(S0)]=0.3。
- （2）**时序差分**（TD）法则不需要等一整个过程结束才更新函数值，而是每一步都能更新，但是由于只走了一步我们并不知道后面的奖励，因此TD采取了自助的方式，利用上一次的估计值V(St+1)来替代。
  - ![](https://pic3.zhimg.com/80/v2-638b08a88f3a37e7c7cb4e95bf03f682_720w.jpg)
  - 当老鼠走了一步后即可更新值函数：V(S0)=V(S0)+lr*[ R1+gamma*V(S1)-V(S0) ]=0.1

## 蒙特卡洛

【2022-5-6】[「详细原理」蒙特卡洛树搜索入门教程](https://www.toutiao.com/article/6788317604403479052)

蒙特卡洛树搜索在2006年被Rémi Coulom第一次提出，应用于Crazy Stone的围棋游戏。
*   Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search

蒙特卡洛树搜索大概的思想就是给定一个游戏状态，去选择一个最佳的策略/动作。
 
### 1.1 有限双人零和序贯博弈
 
**蒙特卡洛树搜索**实际上是一个应用非常广泛的**博弈框架**，这里将其应用于 `有限双人序贯零和博弈` 问题中。像**围棋**、**象棋**、**Tic-Tac-Toe**都是`有限双人序贯零和博弈`游戏。
 
### 1.2 怎样去表示一个游戏?
 
采用 `博弈树` (Game Tree)来表示一个游戏：
- 每个结点都代表一个 `状态` (state)，从一个 `结点` (node)移动一步，将会到达它的 `子节点` (children node)。
- 子节点的个数叫作 `分支因子` (branching factor)。 
- `根节点` (Root node)表示`初始状态`(initial state)。 
- `终止节点` (terminal nodes)没有子节点了。
 
在 tic-tac-toe 游戏中表示如下图所示：
- ![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/9207d5877ea040d8a3be9e7f516c61fa?from=pc)

*   每次都是从 初始状态 、树的 根结点 开始。在 tic-tac-toe 游戏里面初始状态就是一张空的棋盘。
*   从一个节点转移到另一个节点叫作一个 move 。
*   分支因子 (branching factor)， tic-tac-toe 中树越深，分支因子也越少，也就是 children node 的数量越少
*   游戏结束表示 终止节点 。
*   从根节点到终止节点一次表示一个单个游戏 playout 。

不需要关系怎么来到这个 node ，只需要做好之后的事情就好了。
 
### 1.3 最佳策略是什么？minimax和alpha-beta剪枝
 
希望找到的就是 `最佳策略` ( the most promising next move )。如果知道对手的策略那你可以争对这个策略求解，但是大多数情况下是不知道对手的策略的，所以我们需要用 minimax 的方法，假设你的对手是非常机智的，每次他都会采取最佳策略。
 
假设A与B博弈，A期望最大化自己的收益，因为是零和博弈，所以B期望A的收益最小，Minimax算法可描述为如下形式：
*   和 是玩家 和 的效益函数。
*   move 表示从当前状态 和采取的动作 转移到下一个状态。
*   eval 评估最终的游戏分数。
*   是最终的游戏状态。
 
简单地说，就是给定一个状态 期望找到一个动作 在对手最小化你的奖励的同时找到一个最大化自己的奖励。
 
Minimax 算法最大的弱点 就是需要扩展整棵树，对于高分支因子的游戏，像围棋、象棋这种，算法就很难处理。
 
对于上述问题的一种解决方法就是扩展树结构到一定的阈值深度( expand our game tree only up to certain threshold depth d )。因此我们需要一个评估函数，评估 非终止节点 。这对于我们人类来说依据当前棋势判断谁输谁赢是很容易做到的。计算机的解决方法可以参考原文中的：
*   Chess position evaluation with convolutional neural network in Julia

另一种解决树扩展太大的方法就是 alpha-beta剪枝算法 。它会避免一些分支的展开，它最好的结果就是与minimax算法效果相同，因为它减少了搜索空间。
 
### 2 蒙特卡洛树搜索(MCTS)基本概念

蒙特卡洛通过多次模拟仿真，预测出最佳策略。最核心的东西就是**搜索**。
- 搜索是对整棵博弈树的组合遍历，单次的遍历是从根结点开始，到一个**未完全展开**的节点(a node that is not full expanded)。未完全展开的意思就是它至少有一个孩子节点未被访问，或者称作未被探索过。当遇到未被完全展开过的节点，选择它的一个未被访问的childre node做根结点，进行**一次模拟**(a single playout/simulation)。仿真的结果反向传播(propagated back)用于更新当前树的根结点,并更新博弈树节点的统计信息。当整棵博弈树搜索结束后，就相当于拿到了这颗博弈树的策略。
 
再理解一下以下几个关键概念：
*   怎么解释 展开 或 未完全展开 (not fully unexpanded)的博弈树节点？
*   搜索过程中的 遍历 (traverse down)是什么?子节点如何选择？
*   什么是 模拟仿真 (simulation)?
*   什么是 反向传播 (backpropagation)?
*   扩展的树节点中反向传播、更新哪些统计( statistics )信息？
*   怎么依据策略(博弈树)选择动作？
 
#### 2.1 模拟/Simulation/Playout
 
Playout/simulation是与游戏交互的一个过程，从当前节点移动到终止节点。在simulation过程中move的选择基于rollout policy function：
 
Rollout Policy也被称作快速走子策略，基于一个状态 选择一个动作 。为了让这个策略能够simulation快，一般选用随机分布(uniform random)。如下图所示
 
![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/639804569eda4e079c480faffe8bbf3f?from=pc)
 
#### 2.1.1 Alpha Zero中的Playout/Simulation
 
在AlphaGo Zero里面DeepMind‘s直接用一个CNN残差网络输出position evaluation和moves probability。
 
#### 2.2 博弈树节点的扩展-全扩展、访问节点
 
一个节点如果被访问过了，意味着某个某个simulation是以它为起点的。如果一个节点的所有子节点都被访问过了，那这个节点就称为是完全扩展的，否则就是未完全扩展的。如下图对比所示：
- ![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/d0e291d9998b40bfa75706596654333d?from=pc)
 
在实际过程中，一开始根节点的所有子节点都未被访问，从中选一个，第一次simulation就开始了。
 
Simulation过程中rollout policy选择子节点是不被考虑为这个子节点被访问过了， 只有Simulation开始的节点被标记为访问过的 。
 
#### 2.3 反向传播Simulation结果
 
从一个近期访问过的节点(有时候也叫做叶结点(left node))做Simulation，当他Simulation完成之后，得出来的结果就需要反向传播回当前博弈树的根结点，Simulation开始的那个节点被标记为访问过了。
 
![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/be8c53cc57b345559569353c86117695?from=pc)

反向传播是从叶结点(simulation 开始的那个节点)到根结点。在这条路径上所有的节点统计信息都会被计算更新。
 
### 2.4 Nodes’ statistics
 
拿到simulation的结果主要更新两个量：所有的simulation reward 和所有节点 (包括simulation开始的那个节点)的访问次数 。
* 表示一个节点 的 simulation reward和 ，最简单形式的就是所有考虑的节点的模拟结果之和。
* 表示节点的另一个属性，表示这个节点在反向传播路径中的次数(也表示它有多少次参与了total simulation reward)的计算。

### 2.5 遍历博弈树
 
搜索开始时，没有被访问过的节点将会首先被选中，然后simulation，结果反向传播给根结点，之后根节点就可以被认为是全展开的。
 
为了选出我们路径上的下一个节点来开始下一次模拟，我们需要考虑 的所有子节点 , , , 和其本身节点 的信息，如下图所示：
- ![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/7f08609b449a4610b9b082147a9b8c58?from=pc)

当前的状态被标记为蓝色，上图它是全展开的，因此它被访问过了并且存储了节点的统计信息：总的仿真回报和访问次数，它的子节点也具备这些信息。这些值组成了我们最后一个部分：树的置信度上界（Upper Confidence Bound applied to Trees，UCT）。
 
### 2.6 置信度上界
 
UCT是蒙特卡罗树搜索中的一个核心函数，用来选择下一个节点进行遍历：
 
蒙特卡洛树搜索的过程中选UCT最大的那个遍历。
 
UCT 中第一部分是 ，也被称作 exploitation component ，可以看作是子节点 的胜率估计(总收益/总次数=平均每次的收益)。
 
看起来这一项已经有足够说服力，因为只要选择胜率高的下一步即可，但是为什么不能只用这一个成分呢？这是因为这种贪婪方式的搜索会很快导致游戏结束，这往往会导致搜索不充分，错过最优解。因此UCT中的第二项称为exploration component。这个成分更倾向于那些未被探索的节点( 较小)。在蒙特卡洛树搜索过程中第二项的取值趋势大概如下图所示，随着迭代次数的增加其值也下降：
- ![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/5a872a59d63c4ec79a0af534faf7c8d9?from=pc)
 
参数
 
用于平衡MCTS中的exploitation和exploration。
 
#### 2.6.1 UCT in Alpha Go and Alpha Zero
 
在AlphaGo Lee和Alpha Zero算法里面，UCT公式如下：
 
是来自策略网络的move先验概率，策略网络是从状态得到move分布的函数，目的是为了提升探索的效率。
 
当游戏视角发生变化的时候exploitation component 也会发生变化。
 
#### 2.6.2 Alpha Go和Alpha Zero中的策略网络
 
在 AlphaGo 算法里，有两个policy网络：
*   SL Policy Network ：基于人类数据的监督学习所得的网络。
*   RL Policy Network ：基于强化学习自我博弈改进SL Policy Network。

Interestingly – in Deepmind’s Monte Carlo Tree Search variant – SL Policy Network output is chosen for prior move probability estimation as it performs better in practice (authors suggest that human-based data is richer in exploratory moves). What is the purpose of the RL Policy Network then? The stronger RL Policy Network is used to generate 30 mln positions dataset for Value Network training (the one used for game state evaluation)
 
在Alpha Zero里面只有一个网络 ，它不仅是值网络还是策略网络。 It is trained entirely via self-play starting from random initialization. There is a number of networks trained in parallel and the best one is chosen for training data generation every checkpoint after evaluation against best current neural network.
 
### 2.7 终止MCTS
 
什么时候结束MCTS过程？如果你开始玩，那么你的“思考时间”可能是有限的(“thinking time” is probably limited)，计算能力也是有限的(computational capacity has its boundaries, too)。因此最保险的做法是在你资源允许的情况下尽可能全展开遍历搜索。
 
当MSCT程序结束时，最佳的移动通常是访问次数最多的那个节点。因为在多次访问的情况下，评估它的 值必须很高。
- ![「详细原理」蒙特卡洛树搜索入门教程](https://p26.toutiaoimg.com/origin/pgc-image/02c843fdba7d433997122aa53eeb6dc4?from=pc)
 
当你使用蒙特卡洛树搜索选择了一个动作，在对手眼里，你的这个选择将会变成状态的一部分。反过来对你也是一样的，当对手选择了一个状态之后，你的蒙特卡洛树搜索就可以开始工作了。利用之前的统计信息直接搜索就可以得出结果了。
 
### 3 MCTS总结
 
 代码

```python
def monte_carlo_tree_search(root):
    while resources_left(time, computational power):
        leaf = traverse(root) # leaf = unvisited node 
        simulation_result = rollout(leaf)
        backpropagate(leaf, simulation_result)    
        return best_child(root)def traverse(node):
    while fully_expanded(node):
        node = best_uct(node)    
        return pick_univisted(node.children) or node # in case no children are present / node is terminal def rollout(node):
    while non_terminal(node):
        node = rollout_policy(node)    
        return result(node) 
 
def rollout_policy(node):
    return pick_random(node.children)def backpropagate(node, result):
   if is_root(node) return 
   node.stats = update_stats(node, result) 
   backpropagate(node.parent)def best_child(node):
    pick child with highest number of visits
```


## Q-learning


### 什么是Q-learning？

> **Q-Learning** is an off-policy value-based method that uses a TD approach to train its action-value function

- 本质上，Q-learning就是去训练action-value函数，也就是Q函数（Q的含义是Quality，表示在某一状态下那个动作的质量有多高），从而得到对应的action。
  - ![](https://pic3.zhimg.com/80/v2-448b0e27e650190f8116ae6df18320c2_720w.jpg)
- 对于离散问题来说，Q函数其实就是一张Q表格，每一个cell代表对应的状态-动作的函数值，有限个状态，动作空间有限，对应的Q-table就是
  - ![](https://pic3.zhimg.com/80/v2-8e21b6ba5277fa94ce1409236da96406_720w.jpg)
- 只要给定输入（状态+动作），Q表就能输出对应的Q值。当训练完成得到一个最优的Q函数（表）后，也就有了最佳的控制律，因为已经知道了每一个状态下哪个动作的函数值比较大，关于动作空间最大化Q函数即可得到最优的控制律。
  - ![](https://pic4.zhimg.com/80/v2-40781e074536a63315e3d3230c3e6ac7_720w.jpg)

- Q-learning是强化学习中的一种，在Q-learning中，需要维护一张Q值表，表的维数为：状态数S * 动作数A，表中每个数代表在态s下可以采用动作a可以获得的未来收益的折现和——Q值。不断迭代Q值表使其最终收敛，然后根据Q值表我们就可以在每个状态下选取一个最优策略。参照《[极简Q-learning教程](https://zhuanlan.zhihu.com/p/29213893)》
  - 更新公式如下，R+γmax部分称为**Q-target**，即使用贝尔曼方程加贪心策略认为实际应该得到的奖励，目标是使Q值不断的接近Q-target值
  - ![](https://upload-images.jianshu.io/upload_images/4155986-23e17f9c5b81efce.png?imageMogr2/auto-orient/strip\|imageView2/2/w/992/format/webp)

### 算法流程

- 整体
  - ![](https://pic2.zhimg.com/80/v2-4e606c07b3b240419c8b896f58900f51_720w.jpg)
- 详情
  - 第一步，初始化Q表，比如全部为0
    - ![](https://pic4.zhimg.com/80/v2-09afc546990e75e6d309bf6e2547a56f_720w.jpg)
  - 第二步，就是根据Epsilon Greedy策略选择动作（既要利用已知的又要探索未知的）
    - ![](https://pic3.zhimg.com/80/v2-d4494bb7dc8eb4c09b0ac0f359378c96_720w.jpg)
    - 在训练的一开始，应该多探索，随着训练越久，Q表越好，应该不断减小探索的概率。
    - ![](https://pic3.zhimg.com/80/v2-cf4c7d7a5c7be299daa573f0adb73a92_720w.jpg)
  - 第三步是执行动作，得到新的奖励，进入下一状态
    - ![](https://pic4.zhimg.com/80/v2-ce2990d388cc1a991a5632a78ee24943_720w.jpg)
  - 第四步就是更新Q表
    - ![](https://pic3.zhimg.com/80/v2-4b0911f8cb15e29a8a3547f619d959ca_720w.jpg)
- 总结
  - 计算TD Target的时候采用的是贪婪策略（对动作空间求Q值得最大），由于动作的实施和更新策略不同，所以Q-learning称为off-policy！因此对应的也有on-policy，比如说Sarsa算法
    - ![](https://pic4.zhimg.com/80/v2-519e8fd6d095a3efcd6729fd7ff5a8cf_720w.jpg)
  - Q-learning的核心就是Q表的更新，但是当问题规模一大，这种简单粗暴的方法显然是不太现实的，因此就有了 Deep Q-learning 的出现了。


## DQN

- 经典强化学习的Q-learning需要创建一个**Q表**来寻找最佳动作，而深度Q-learning则是采用一个神经网络来近似q值，避免了大型表的记录和存储。
  - ![](https://pic4.zhimg.com/80/v2-c545c9ef6c679b899b81e881a6afe297_720w.jpg)
- DQN是深度学习与强化学习的结合，即使用神经网络代替Q-learning中Q表。在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，但是当状态和动作空间是**高维**或者**连续**时，使用Q-Table不现实，而神经网络恰好擅长于此。因此DQN将Q-Table的更新问题变成一个函数拟合问题，相近的状态得到相近的输出动作。如有一个Q值表，神经网络的作用就是给定一个状态s和动作a，预测对应的Q值，使得神经网络的结果与Q表中的值接近。不过DQN的方式肯定不能继续维护一个Q表，所以将上次反馈的奖励作为逼近的目标，如下式，通过更新参数 θ 使Q函数逼近最优Q值。因此，DQN就是要设计一个神经网络结构，通过函数来拟合Q值
- 问题：
  - 神经网络需要**大量带标签的样本**进行监督学习，但是强化学习只有reward返回值，如何构造有监督的数据成为第一个问题，而且伴随着噪声、延迟（过了几十毫秒才返回）、稀疏（很多State的reward是0）等问题；
  - 神经网络的前提是样本**独立同分布**，而强化学习前后state状态和反馈有**依赖**关系——**马尔科夫决策**；
  - 神经网络的目标**分布固定**，但是强化学习的分布一直变化，比如你玩一个游戏，一个关卡和下一个关卡的状态分布是不同的，所以训练好了前一个关卡，下一个关卡又要重新训练；
  - 过往的研究表明，使用非线性网络表示值函数时出现不稳定等问题。
- 针对以上问题的具体解决方案如下：
  - **构造标签**：通过Q-Learning使用reward来构造标签（对应问题1），如上所述，用神经网络来预测reward，将问题转化为一个**回归**问题；
  - **经验回放**：通过experience replay（经验池）的方法来解决**相关性**及**非静态分布**问题（对应问题2、3）；具体做法是把每个时间步agent与环境交互得到的转移样本 (st,at,rt,st+1) 储存到回放记忆单元，要训练时就随机拿出一些（minibatch）来训练。（其实就是将游戏的过程打成碎片存储，训练时随机抽取就避免了相关性问题）
  - 双网络结构：使用一个神经网络产生当前Q值，使用另外一个神经网络产生Target Q值（对应问题4）。在Nature 2015版本的DQN中提出了这个改进，使用另一个网络（这里称为target_net）产生Target Q值。具体地，Q(s,a;θi) 表示当前网络eval_net的输出，用来评估当前状态动作对的值函数；Q(s,a;θ−i) 表示target_net的输出，代入上面求 TargetQ 值的公式中得到目标Q值。根据上面的Loss Function更新eval_net的参数，每经过N轮迭代，将MainNet的参数复制给target_net。引入target_net后，再一段时间里目标Q值使保持不变的，一定程度降低了当前Q值和目标Q值的相关性，提高了算法稳定性。
- 参考：[实战深度强化学习DQN-理论和实践](https://www.jianshu.com/p/10930c371cac?from=singlemessage)

### 代码实现

- 两种DQN的实现方式
  - 一种是将s和a输入到网络，得到q值
  - 另一种是只将s输入到网络，输出为s和每个a结合的q值。
- 莫烦的Demo采用第二种，[Github地址](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)
  - ![](https://upload-images.jianshu.io/upload_images/4155986-fd3c59c61907c4ad.png?imageMogr2/auto-orient/strip\|imageView2/2/w/542/format/webp)
- DQN的Tensorflow版本实现，Gym环境
    - [用gym库Classic control实现deep Q learning](https://blog.csdn.net/winycg/article/details/79468320)

- （1）CartPole实例
    - 运载体在一根杆子下无摩擦的跟踪。系统通过施加+1和-1推动运载体。杆子的摇摆在初始时垂直的，目标是阻止它掉落运载体。每一步杆子保持垂直可以获得+1的奖励。episode将会终结于杆子的摇摆幅度超过了离垂直方向的15°或者是运载体偏移初始中心超过2.4个单位。
    - ![](https://img-blog.csdn.net/20180307114338879)
    - 效果图：
        - ![](https://img-blog.csdn.net/20180308173917866)

```python
#https://blog.csdn.net/winycg/article/details/79468320
import numpy as np
import random
import tensorflow as tf
import gym
 
max_episode = 100
env = gym.make('CartPole-v0')
env = env.unwrapped
 
class DeepQNetwork(object):
    def __init__(self,
                 n_actions,
                 n_features,
                 learning_rate=0.01,
                 reward_decay=0.9,  # gamma
                 epsilon_greedy=0.9,  # epsilon
                 epsilon_increment = 0.001,
                 replace_target_iter=300,  # 更新target网络的间隔步数
                 buffer_size=500,  # 样本缓冲区
                 batch_size=32,
                 ):
        self.n_actions = n_actions
        self.n_features = n_features
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = epsilon_greedy
        self.replace_target_iter = replace_target_iter
        self.buffer_size = buffer_size
        self.buffer_counter = 0  # 统计目前进入过buffer的数量
        self.batch_size = batch_size
        self.epsilon = 0 if epsilon_increment is not None else epsilon_greedy
        self.epsilon_max = epsilon_greedy
        self.epsilon_increment = epsilon_increment
        self.learn_step_counter = 0  # 学习计步器
        self.buffer = np.zeros((self.buffer_size, n_features * 2 + 2))  # 初始化Experience buffer[s,a,r,s_]
        self.build_net()
        # 将eval网络中参数全部更新到target网络
        target_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='target_net')
        eval_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='eval_net')
        with tf.variable_scope('soft_replacement'):
            self.target_replace_op = [tf.assign(t, e) for t, e in zip(target_params, eval_params)]
        self.sess = tf.Session()
        tf.summary.FileWriter('logs/', self.sess.graph)
        self.sess.run(tf.global_variables_initializer())
 
    def build_net(self):
        self.s = tf.placeholder(tf.float32, [None, self.n_features])
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features])
        self.r = tf.placeholder(tf.float32, [None, ])
        self.a = tf.placeholder(tf.int32, [None, ])
 
        w_initializer = tf.random_normal_initializer(0., 0.3)
        b_initializer = tf.constant_initializer(0.1)
        # q_eval网络架构，输入状态属性，输出4种动作
        with tf.variable_scope('eval_net'):
            eval_layer = tf.layers.dense(self.s, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                         bias_initializer=b_initializer, name='eval_layer')
            self.q_eval = tf.layers.dense(eval_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer1')
        with tf.variable_scope('target_net'):
            target_layer = tf.layers.dense(self.s_, 20, tf.nn.relu, kernel_initializer=w_initializer,
                                           bias_initializer=b_initializer, name='target_layer')
            self.q_next = tf.layers.dense(target_layer, self.n_actions, kernel_initializer=w_initializer,
                                          bias_initializer=b_initializer, name='output_layer2')
        with tf.variable_scope('q_target'):
            # 计算期望价值，并使用stop_gradient函数将其不计算梯度，也就是当做常数对待
            self.q_target = tf.stop_gradient(self.r + self.gamma * tf.reduce_max(self.q_next, axis=1))
        with tf.variable_scope('q_eval'):
            # 将a的值对应起来，
            a_indices = tf.stack([tf.range(tf.shape(self.a)[0]), self.a], axis=1)
            self.q_eval_a = tf.gather_nd(params=self.q_eval, indices=a_indices)
        with tf.variable_scope('loss'):
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval_a))
        with tf.variable_scope('train'):
            self.train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)
 
            # 存储训练数据
 
    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s, a, r, s_))
        index = self.buffer_counter % self.buffer_size
        self.buffer[index, :] = transition
        self.buffer_counter += 1
 
    def choose_action_by_epsilon_greedy(self, status):
        status = status[np.newaxis, :]
        if random.random() < self.epsilon:
            actions_value = self.sess.run(self.q_eval, feed_dict={self.s: status})
            action = np.argmax(actions_value)
        else:
            action = np.random.randint(0, self.n_actions)
        return action
 
    def learn(self):
        # 每学习self.replace_target_iter步，更新target网络的参数
        if self.learn_step_counter % self.replace_target_iter == 0:
            self.sess.run(self.target_replace_op)
            # 从Experience buffer中选择样本
        sample_index = np.random.choice(min(self.buffer_counter, self.buffer_size), size=self.batch_size)
        batch_buffer = self.buffer[sample_index, :]
        _, cost = self.sess.run([self.train_op, self.loss], feed_dict={
            self.s: batch_buffer[:, :self.n_features],
            self.a: batch_buffer[:, self.n_features],
            self.r: batch_buffer[:, self.n_features + 1],
            self.s_: batch_buffer[:, -self.n_features:]
        })
        self.epsilon = min(self.epsilon_max, self.epsilon + self.epsilon_increment)
        self.learn_step_counter += 1
        return cost
 

RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        # x是车的水平位移，theta是杆离垂直的角度
        x, x_dot, theta, theta_dot = observation_
        # reward1是车越偏离中心越少
        reward1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8
        # reward2为杆越垂直越高
        reward2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5
        reward = reward1 + reward2
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost = RL.learn()
            print('cost: %.3f' % cost)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1

# mountain car
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
plt.plot(np.arange(len(cost)), cost)
plt.show()
```

- （2） MountainCar实例
    - car的轨迹是一维的，定位在两山之间，目标是爬上右边的山顶。可是car的发动机不足以一次性攀登到山顶，唯一的方式是car来回摆动增加动量。
    - ![](https://img-blog.csdn.net/20180308211123582)
    - 输出图：
        - ![](https://img-blog.csdn.net/2018032423074127)
    - 代码如下：

```python
RL = DeepQNetwork(n_actions=env.action_space.n,
                  n_features=env.observation_space.shape[0])
total_step = 0
for episode in range(max_episode):
    observation = env.reset()
    episode_reward = 0
    while True:
        env.render()  # 表达环境
        action = RL.choose_action_by_epsilon_greedy(observation)
        observation_, reward, done, info = env.step(action)
        #
        position, velocity = observation_
        reward=abs(position+0.5)
        RL.store_transition(observation, action, reward, observation_)
        if total_step > 100:
            cost_ = RL.learn()
            cost.append(cost_)
        episode_reward += reward
        observation = observation_
        if done:
            print('episode:', episode,
                  'episode_reward %.2f' % episode_reward,
                  'epsilon %.2f' % RL.epsilon)
            break
        total_step += 1
 
plt.plot(np.arange(len(cost)), cost)
plt.show()
```

## PG（Policy Gradients）策略梯度算法

- 基于Policy Gradients（策略梯度法，简称PG）的深度强化学习方法，思想上与基于Q-learning的系列算法有本质的不同
- 普通的PG算法，只能用于解决一些小的问题，比如经典的让杆子竖起来，让小车爬上山等。如果想应用到更复杂的问题上，比如玩星际争霸，就需要更复杂的一些方法，比如后期出现的Actor Critic，Asynchronous Advantage Actor-Critic (A3C)等等

### 解决什么问题

- （1）很多决策的行动空间是**高维甚至连续（无限）**的
  - 比如自动驾驶中，汽车下一个决策中方向盘的行动空间，就是一个从[-900°，900°]（假设方向盘是两圈半打满）的无限空间中选一个值，如果我们用Q系列算法来进行学习，则需要对每一个行动都计算一次reward，那么对无限行动空间而言，哪怕是把行动空间离散化，针对每个离散行动计算一次reward的计算成本也是当前算力所吃不消的。这是对Q系列算法提出的第一个挑战：无法遍历行动空间中所有行动的reward值。
- （2）决策往往是带有**多阶段**属性的，“不到最后时刻不知输赢”。
  - 以即时策略游戏（如：星际争霸，或者国内流行的王者荣耀）为例，玩家的输赢只有在最后游戏结束时才能知晓，谁也没法在游戏进行过程中笃定哪一方一定能够赢。甚至有可能发生：某个玩家的每一步行动看起来都很傻，但是最后却能够赢得比赛，比如，Dota游戏中，有的玩家虽然死了很多次，己方的塔被拆了也不管，但是却靠着偷塔取胜（虽然这种行为可能是不受欢迎的）。诸如此类的情形就对Q系列算法提出了第二个挑战，Agent每执行一个动作（action）之后的奖励（reward）难以确定，这就导致Q值无法更新。

- 由此衍生出了基于PG的系列深度强化学习算法

### 算法介绍

- 流程
  - ![](https://img-blog.csdnimg.cn/20200823095929950.png)
- 图解
  - ![](https://img-blog.csdnimg.cn/20200823150933700.png)
- 注意
  - 神经网络设计过程中，最后一层一般采用Softmax函数(离散动作) 或者 高斯函数激活（连续动作）。
- 奖励分配代码示例

```python
def _discount_and_norm_rewards(self):
##该函数将最后的奖励，依次分配给前面的回合，越往前，分配的越少。除此之外，还将分配后的奖励归一化为符合正太分布的形式。
    discounted_ep_rs = np.zeros_like(self.ep_rs) #self.ep_rs就是一局中每一回合的奖励，一般前面回合都是0，只有最后一个回合有奖励（一局结束）
    running_add = 0
    for t in reversed(range(0, len(self.ep_rs))):
        running_add = running_add * self.gamma + self.ep_rs[t] #self.gamma，是衰减系数，该系数越大，前面回合分配到的奖励越少（都衰减了嘛）
        discounted_ep_rs[t] = running_add

    discounted_ep_rs = discounted_ep_rs - np.mean(discounted_ep_rs)
    discounted_ep_rs = discounted_ep_rs / np.std(discounted_ep_rs)
    return discounted_ep_rs
```

### PG与Q系列算法

- PG深度强化学习算法与Q系列算法相比，优势主要有[2]：
  - 可以处理连续动作空间或者高维离散动作空间的问题。
  - 容易收敛，在学习过程中，策略梯度法每次更新策略函数时，参数只发生细微的变化，但参数的变化是朝着正确的方向进行迭代，使得算法有更好的收敛性。而价值函数在学习的后期，参数会围绕着最优值附近持续小幅度地波动，导致算法难以收敛。
- 缺点主要有：
  - 容易收敛到局部最优解，而非全局最优解；
  - 策略学习效率低；
  - 方差较高：这个可谓是普通PG深度强化学习算法最不可忍受的缺点了，由于PG算法参数更新幅度较小，导致神经网络有很大的随机性，在探索过程中会产生较多的无效尝试。另外，在处理回合结束才奖励的问题时，会出现不一致的问题：回合开始时，同样的状态下，采取同样的动作，但是由于后期采取动作不同，导致奖励值不同，从而导致神经网络参数来回变化，最终导致Loss函数的方差较大。

- 参考
  - [一图看懂Policy Gradients深度强化学习算法](https://blog.csdn.net/xz15873139854/article/details/108179193)


# 基于模型的RL

前面介绍的是基于**价值**的强化学习(Value Based RL)和基于**策略**的强化学习模型(Policy Based RL)都属于model free，直接从价值函数+策略函数中学习，不用学习环境的状态转化概率模型，下面介绍另一种强化学习流派，基于**模型**的强化学习(Model Based RL)，以及基于模型的强化学习算法框架**Dyna**。

参考：
- UCL强化学习课程的第8讲和Dyna-2的[论文](https://www.davidsilver.uk/wp-content/uploads/2020/03/dyna2_compressed.pdf)
- [model-based RL（一）——基本框架](https://zhuanlan.zhihu.com/p/266131804)
- [上交张伟楠副教授：基于模型的强化学习算法，基本原理以及前沿进展](https://zhuanlan.zhihu.com/p/162787188)

## model-based（MBRL）与model free (MFRL)

不建立环境模型，仅依靠实际环境的采样数据进行训练学习的强化学习算法称为**无模型强化学习**（Model-Free Reinforcement Learning，MFRL）算法，也即是不依赖于环境模型的强化学习算法。

MFRL发展中遇到的一个困境：**数据采集效率**（Sample Efficiency）太低。在有监督或无监督学习中，人们构建一个目标函数，通过梯度下降（或上升）的方式，不断趋近理想结果。与有监督/无监督学习不同的是，强化学习属于一种试错的学习范式，当前策略的采样结果如果无法有效帮助当前策略进行提升，则可以认为当前试错的采样结果是无效采样。在MFRL训练过程中，智能体有大量的交互采样属于无效采样，这些采样没有对行动策略的改进产生明显的影响。为了解决无模型强化学习中的这一数据效率低下的问题，人们开始转向基于模型强化学习（Model-Based Reinforcement Learning，MBRL）的方法。

MBRL的基本思想在于首先建立一个环境的动态模型，然后在建立的环境模型中训练智能体的行动策略，通过这种方式，实现数据效率的提升。
MFRL存在如下特点：
1. 相比于MBRL，MFRL拥有最好的渐进性能（Asymptotic Performance），当策略与环境交互达到收敛状态时，相比于MBRL，MFRL下训练所得策略所最终达到的性能会更好，能够避免出现复合误差的问题，因而在实际环境中表现会更为优异。
2. MFRL非常适合使用深度学习框架去采样超大规模的数据，并进行网络训练。
3. MFRL经常采用Off-Policy训练方法，这种情况下会有偏差（Bias）导致的训练效果不稳定（instability）的问题。
4. MFRL需要进行超大量的数据采样，因而需要超高的算力要求，这种算力要求是很多科研院所或者企业所无法负担的。
MBRL存在如下特点：
1. 环境模型一旦建立起来，便可以采用on-policy的训练方法，利用当前采样得到的数据训练当前的策略，在这种情形下，采样效率是最高的。
2. 建立环境模型后，便可以选择性地不再与实际场景交互，在模型中进行训练学习，完成训练后再在实际场景中投入使用（off-line RL，也称为batch RL）。
3. 相比于MFRL，MBRL数据采样效率会往往有较大的提升。
4. 存在模型与实际环境之间的复合误差问题（Compounding Error），模型向后推演的幅度越长，推演误差就会越大，直至模型完全失去作用。


### MBRL

MBRL的进一步分类，其主要包括**黑盒模型**与**白盒模型**两类：
- 黑盒模型中，环境模型的构造是未知的，仅作为数据的采样来源。由于采样数据来自于黑盒模型，而不是和真实环境交互得到，因此这些来自模型的采样数据不计入数据采样效率的计算中。虽然从计算结果来看MFBL的数据采样效率较高，但由于训练过程中使用了大量基于模型采样的数据，因此从采样数据总量上来看，实际采样了更多的数据。常用的基于黑盒模型的MBRL算法包括**Dyna-Q**、**MPC**、**MBPO**等。
![](https://pic4.zhimg.com/80/v2-1942d467f25fbf74f4f6ca566b8860eb_720w.jpg)
图1：基于黑盒模型的MBRL算法
- 白盒模型中，环境模型的构造是已知的，可以将模型中状态的价值函数直接对策略的参数进行求导，从而实现对策略的更新。常用的基于白盒模型的MBRL算法包括**MAAC**、**SVG**、**PILCO**等。
![](https://pic2.zhimg.com/80/v2-9f43425e5a85498daa68c291986fc865_720w.jpg)

对比：
![](https://pic1.zhimg.com/80/v2-b857c15be4302c5ad4dd820f8dcd41fc_720w.jpg)

基于黑盒模型的MBRL：
![](https://pic2.zhimg.com/80/v2-7977cd1fddfb6f81a57a994ef21ba8d9_720w.jpg)

- 首先，当前的价值函数Q(s, a)以及策略函数π(a\|s)与真实环境进行交互，完成交互后采样出环境反馈的数据Experience{(s, a, r, s’)}。然后通过采样出的数据来训练建立的环境模型p(s’, r\|s, a )，环境的模型本质上是通过输入的当前状态State以及采取的动作Action，来预测产生的Reward以及下一步的状态State，很多情况下Reward是根据先验规则或领域知识生成，这时模型只预测下一步的状态State即可。接下来在当前模型中进行Plannning，也就是通过当前模型进行数据的采样，通过数据采样的结果去训练新一轮的价值函数Q(s, a)以及策略函数π(a\|s)。
- Q-Planning是最简单的MBRL算法，它通过与真实环境交互的数据来训练环境模型，然后基于当前的环境模型来进行一步Planning，从而完成Q函数的训练。其步骤是，首先采集智能体与环境交互的State以及采用的Action，将数据传入建立的黑盒模型中，采集并得到模型虚拟出来的Reward以及下一步的Next_State，接着将传入模型的State、Action以及模型虚拟出来的Reward、Next_State进行一步Q-Learning训练，这样就完成了一步Q-Planning算法的更新。期间智能体仅仅通过与环境模型交互来进行数据的采样，得到虚拟的Reward以及下一步的Next_State，并进行策略的训练和更新，智能体未通过与实际环境交互数据来进行策略的更新。
- 还可以将Q-Planning与Q-Learning结合在一起，形成 Dyna算法。Dyna算法提出于90年代早期，完整的流程示意图如下所示。可以看到，将中间的direct RL步骤去掉后，就是刚刚讨论的Q-Planning算法。
![](https://pic4.zhimg.com/80/v2-39b0ed9d4d6e34a4386d552409c2bd0f_720w.jpg)

在Dyna算法中，首先通过智能体与实际环境的交互进行一步正常的Q-Learning操作，然后通过与实际环境交互时使用的State、Action，传入环境模型中进行Q-Planning操作，真实环境中进行一步Q-Learning操作，对应环境模型中进行n步Q-Planning操作。实际环境中的采样与模型中的采样具有1：n的关系，通过这种方式来提升训练过程中的Sample efficiency。

### MBRL问题

MBRL算法的发展面临着三个关键问题，同时这也是发展的三个思路：
1. 环境模型的建立是否真的有助于Sample Efficiency的提高？
2. 所建立的模型基本都是基于神经网络建立，不可避免的会出现泛化性误差的问题，那么人们什么时候可以相信建立的模型并使用模型进行策略的训练与更新？
3. 如何适当地把握模型的使用尺度，来获得最好的或者至少获得正向的策略训练结果？



model-free方法有两种方式
- **value-based**方法先学习**值函数**（MC或TD）再更新策略
- **policy-based**方法直接将真实轨迹数据（real experience）更新策略。

而model-based方法先将着重点放在**环境模型**(environment dynamics)上，通过采样先学习一个对环境的建模，再根据学习到的环境模型做值函数/策略优化。在model-based方法中，planning步骤至关重要，正是通过在learned model基础上做planning才提高了整个强化学习算法迭代的效率。

![](https://pic1.zhimg.com/80/v2-b70189e8c85af6605c39eda35aec2398_720w.jpg)


完成了对环境的建模后，在model-based大类方法中同样有两种路径
- 一种是通过学到的model生成一些仿真轨迹，通过仿真轨迹估计值函数来优化策略；
- 另一种是通过学到的model直接优化策略，这是目前model-based方法常走的路线。

- Pros：先学习model的一个显而易见的好处，就是解决model-free强化学习方法中样本效率的问题(sample efficiency)。
- Cons：由于模型是在过程中学习的，难免存在偏差；难以保证收敛

model可以理解为由**状态转移分布**和**回报函数**组成的元组 ![[公式]](https://www.zhihu.com/equation?tex=M%3D%28P%2C+R%29) 。其中 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D+%5Csim+P_%7B%5Ceta%7D%28S_%7Bt%2B1%7D%7CS_t%2C+A_t%29) ， ![[公式]](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%5Csim+R_%7B%5Ceta%7D%28R_%7Bt%2B1%7D%7CS_t%2C+A_t%29)

如何学习模型？
- **状态-动作**对到**回报**的映射可以看作是一个**回归**问题，loss函数可以设置为均方误差；
- 学习状态-动作对到下一状态的映射可以看作**密度估计**问题，loss函数可以设置为KL散度。

模型的构造也有很多选择（神经网络、高斯过程等等），从而派生出不同的算法。

![](https://pic2.zhimg.com/80/v2-e8e93fa230d39cb0e33bb69e3892b025_720w.jpg)

RL是基于**经验**的学习方法。**实际经验**是真实的智能体和环境交互得到的数据。比如一个机械臂尝试从一堆物品中抓取物品，无人车在道路上行驶。最直接的作用是用来学习**值函数**或者**策略**，称之为**直接强化学习**（direct RL）。还可以利用实际经验来学习模型，叫做**模型学习**（model learning）。当然我们的模型不一定非得要学，根据物理定理也能建立起模型，但基于数据驱动的模型学习也是一个方法。那学习了模型之后呢？怎么用它？我们可以用学习的模型来产生经验，叫做**仿真经验**。然后利用仿真经验来辅助策略学习，叫做**间接强化学习**（indirect RL）。

Direct RL VS. Indirect RL, 谁好谁坏？直接RL和间接RL都有自己的优缺点。
- 间接RL可以充分的利用经验，减少和环境的交互。可以类比为人类的冥想，mental rehearsal。没必要都去试一下才知道怎么做，想一想可能就知道答案了，这样就节省了体力。
- 直接RL更简单，并且不会受到模型偏差的影响。试想如果一个精神病患者，你让他冥想，不知道他会做出什么举动来。

这种争论在AI和心理学领域可以归结为：**认知**和**试错**学习的争执，精细编程和反应式决策的争执。还在于：model-based 和model-free的争执。

## 表格型：Dyna-Q框架

把模型学习，规划和决策融合在一起，在线的进行规划学习

Dyna-Q中，**直接RL**就使用**Q学习**（one-step tabular Q-learning）。规划算法就使用上一节提到的随机采样Q-规划算法(one-step random sample tabular Q-planning)。目前为止，所有算法都是针对于**表格型**的问题。也就是说状态动作都是有限离散的。因此在学习模型的时候，我们就可以把所有的转移动作对 [公式] 存起来。所示实际上模型学习就是存储，规划就是从存的数据里面拿数据，然后用Q-learning。
整个Dyna-Q的学习框图
![](https://pic3.zhimg.com/80/v2-939c424679c7dc29f12cd4a2a645b3a6_720w.jpg)

如迷宫问题上，考察Dyna-Q的效果。其实可以想象，引入规划无非就是为了增加学习效率。所以Dyna-Q和Q-learning相比学的更快。实际的效果如下：
 
![](https://pic3.zhimg.com/80/v2-51d76b47dd52c1182a57f92e776205d6_720w.jpg)
 
规划的步数越多，学习的就越快。纵轴表示每个episode的步数。如果是学到了一个策略，自然步数是最小的（不走弯路）。对比一下2个episode后的策略：
 
![](https://pic2.zhimg.com/80/v2-d7050c5d548e51c7fe712cefcf41c699_720w.jpg)
 
可以发现，有规划的算法值函数的更新更有效。

摘自：[8.2 Dyna:融合规划，决策和学习](https://zhuanlan.zhihu.com/p/59896757)

## model-based策略优化

![](https://pic2.zhimg.com/80/v2-cc7f6fbcb5039810051c7f0e8d4970e9_720w.jpg)

- 如果环境模型已知，轨迹优化问题就是一个最优控制问题
  - ![[公式]](https://www.zhihu.com/equation?tex=min%5Csum_%7Bt%3D1%7D%5E%7BT%7D%7Bc%28s_t%2Ca_t%29%7D)
  - ![[公式]](https://www.zhihu.com/equation?tex=s.t.++%5Cquad+s_t%3Df%28s_%7Bt-1%7D%2Ca_%7Bt-1%7D%29)
- 如果环境模型未知，把model learning和trajectory optimization结合起来

## [Deep Reinforcement Learning Algorithms with PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)

This repository contains PyTorch implementations of deep reinforcement learning algorithms and environments. 

## **Algorithms Implemented** 

1. *Deep Q Learning (DQN)* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>  
1. *DQN with Fixed Q Targets* <sub><sup> ([Mnih et al. 2013](https://arxiv.org/pdf/1312.5602.pdf)) </sup></sub>
1. *Double DQN (DDQN)* <sub><sup> ([Hado van Hasselt et al. 2015](https://arxiv.org/pdf/1509.06461.pdf)) </sup></sub>
1. *DDQN with Prioritised Experience Replay* <sub><sup> ([Schaul et al. 2016](https://arxiv.org/pdf/1511.05952.pdf)) </sup></sub>
1. *Dueling DDQN* <sub><sup> ([Wang et al. 2016](http://proceedings.mlr.press/v48/wangf16.pdf)) </sup></sub>
1. *REINFORCE* <sub><sup> ([Williams et al. 1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)) </sup></sub>
1. *Deep Deterministic Policy Gradients (DDPG)* <sub><sup> ([Lillicrap et al. 2016](https://arxiv.org/pdf/1509.02971.pdf) ) </sup></sub>
1. *Twin Delayed Deep Deterministic Policy Gradients (TD3)* <sub><sup> ([Fujimoto et al. 2018](https://arxiv.org/abs/1802.09477)) </sup></sub>
1. *Soft Actor-Critic (SAC)* <sub><sup> ([Haarnoja et al. 2018](https://arxiv.org/pdf/1812.05905.pdf)) </sup></sub>
1. *Soft Actor-Critic for Discrete Actions (SAC-Discrete)* <sub><sup> ([Christodoulou 2019](https://arxiv.org/abs/1910.07207)) </sup></sub> 
1. *Asynchronous Advantage Actor Critic (A3C)* <sub><sup> ([Mnih et al. 2016](https://arxiv.org/pdf/1602.01783.pdf)) </sup></sub>
1. *Syncrhonous Advantage Actor Critic (A2C)*
1. *Proximal Policy Optimisation (PPO)* <sub><sup> ([Schulman et al. 2017](https://openai-public.s3-us-west-2.amazonaws.com/blog/2017-07/ppo/ppo-arxiv.pdf)) </sup></sub>
1. *DQN with Hindsight Experience Replay (DQN-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *DDPG with Hindsight Experience Replay (DDPG-HER)* <sub><sup> ([Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf) ) </sup></sub>
1. *Hierarchical-DQN (h-DQN)* <sub><sup> ([Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Stochastic NNs for Hierarchical Reinforcement Learning (SNN-HRL)* <sub><sup> ([Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf)) </sup></sub>
1. *Diversity Is All You Need (DIAYN)* <sub><sup> ([Eyensbach et al. 2018](https://arxiv.org/pdf/1802.06070.pdf)) </sup></sub>

All implementations are able to quickly solve Cart Pole (discrete actions), Mountain Car Continuous (continuous actions), 
Bit Flipping (discrete actions with dynamic goals) or Fetch Reach (continuous actions with dynamic goals). I plan to add more hierarchical RL algorithms soon.

## **Environments Implemented**

1. *Bit Flipping Game* <sub><sup> (as described in [Andrychowicz et al. 2018](https://arxiv.org/pdf/1707.01495.pdf)) </sup></sub>
1. *Four Rooms Game* <sub><sup> (as described in [Sutton et al. 1998](http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)) </sup></sub>
1. *Long Corridor Game* <sub><sup> (as described in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)) </sup></sub>
1. *Ant-{Maze, Push, Fall}* <sub><sup> (as desribed in [Nachum et al. 2018](https://arxiv.org/pdf/1805.08296.pdf) and their accompanying [code](https://github.com/tensorflow/models/tree/master/research/efficient-hrl)) </sup></sub>

## **Results**

#### 1. Cart Pole and Mountain Car

Below shows various RL algorithms successfully learning discrete action game [Cart Pole](https://github.com/openai/gym/wiki/CartPole-v0)
 or continuous action game [Mountain Car](https://github.com/openai/gym/wiki/MountainCarContinuous-v0). The mean result from running the algorithms 
 with 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparameters
 used can be found in files `results/Cart_Pole.py` and `results/Mountain_Car.py`. 
 
![Cart Pole and Mountain Car Results](results/data_and_graphs/CartPole_and_MountainCar_Graph.png) 


#### 2. Hindsight Experience Replay (HER) Experiements

Below shows the performance of DQN and DDPG with and without Hindsight Experience Replay (HER) in the Bit Flipping (14 bits) 
and Fetch Reach environments described in the papers [Hindsight Experience Replay 2018](https://arxiv.org/pdf/1707.01495.pdf) 
and [Multi-Goal Reinforcement Learning 2018](https://arxiv.org/abs/1802.09464). The results replicate the results found in 
the papers and show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only difference 
between them was whether hindsight was used or not. 

![HER Experiment Results](results/data_and_graphs/HER_Experiments.png)

#### 3. Hierarchical Reinforcement Learning Experiments

The results on the left below show the performance of DQN and the algorithm hierarchical-DQN from [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf)
on the Long Corridor environment also explained in [Kulkarni et al. 2016](https://arxiv.org/pdf/1604.06057.pdf). The environment
requires the agent to go to the end of a corridor before coming back in order to receive a larger reward. This delayed 
gratification and the aliasing of states makes it a somewhat impossible game for DQN to learn but if we introduce a 
meta-controller (as in h-DQN) which directs a lower-level controller how to behave we are able to make more progress. This 
aligns with the results found in the paper. 

The results on the right show the performance of DDQN and algorithm Stochastic NNs for Hierarchical Reinforcement Learning 
(SNN-HRL) from [Florensa et al. 2017](https://arxiv.org/pdf/1704.03012.pdf). DDQN is used as the comparison because
the implementation of SSN-HRL uses 2 DDQN algorithms within it. Note that the first 300 episodes of training
for SNN-HRL were used for pre-training which is why there is no reward for those episodes. 
 
![Long Corridor and Four Rooms](results/data_and_graphs/Four_Rooms_and_Long_Corridor.png)


# 应用

## 量化交易

### 强化学习量化交易分类

- 【2022-3-14】[【RL-Fintech】强化学习在金融量化领域的最新进展](https://zhuanlan.zhihu.com/p/472254985)
- RL在量化交易里面的应用，大致分为4种：
  - Portfolio Management（投资组合管理）—— 低频
    - 通过灵活分配资产权重获得更高超额收益的问题。现实中的应用例子比如选股和指数增强基金。一般的解决方案是对股票的涨价潜力进行打分，买入具有上涨潜力的股票并增加权重，卖出可能跌或者相对弱势的股票。这是一个多时间序列（Multiple Time Series）上的权重再分配问题。
    - 两篇baseline是 AlphaStock（2019） 和 DeepTrader（2021）
  - Single-asset trading signal （单资产交易信号） —— 中高频
    - 单资产的交易信号问题是指对单一资产进行买卖操作以获得比单纯持有更高利润的问题。监督学习在这类问题中取得了比较大的成功。
    - 前面介绍的PM问题进来的研究热点在于时空关系的发掘方面，并没有很多RL算法设计上的独特创新。事实上RL独特的reward设计在2000-2006年左右研究得比较多，当时受制于算力，一般都还是单一资产的交易问题上进行应用。
  - Execution（交易执行）—— 高频
  - Option hedging（期权对冲和定价）。
  - 其中PM一般是**低频**交易，单股交易信号一般是中高频，交易执行一般是**高频**tick级数据上的策略，至于期权定价则是理论和实践统一起来的工作。
  - ![](https://pic1.zhimg.com/80/v2-8e107d2e5429947601312abd7d61f498_720w.jpg)

### 如何应用RL

- 【2021-4-6】[强化学习（Reinforcement Learning）在量化交易领域如何应用](https://www.zhihu.com/question/45116323/answer/758082798)

> 讲个大实话：这个问题的答案其实都不用看，肯定都不靠谱，靠谱的肯定不会告诉你

感兴趣的朋友可以在[BigQuant AI](https://bigquant.com/%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer)平台上动手实践一下
 
1\. 相比（无）监督学习，强化学习在量化领域应用时，首先需要建立一个环境，在环境中定义state，action，以及reward等。定义的方式有多种选择，比如：
- **state**: 可以将n天的价格，交易量数据组合成某一天的state,也可以用收益率或是其他因子组合作为某一天的state，如果想要定义有限个的state,可以定义为appreciated/ hold_value/ depreciated这样3类。
- **action**: 可以定义为buy/sell两种, 也可以定义为buy/sell/hold三种，或者定义为一个（-1,1）之间的一个连续的数，-1和1分别代表all out 和 holder两个极端。
- **reward**: 可以定义为新旧总资产价值之间的差，或是变化率，也可以将buy时的reward定义为0，sell时的定义为买卖价差。
 
2\. 需要选择一个具体的强化学习方法：
- 1) Q-table (具体可参考：[Reinforcement Learning Stock Trader](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer))
  - ![](https://pic1.zhimg.com/50/v2-32586bdcd7b20c55790c2447583e3b2a_hd.jpg?source=1940ef5c)
  - ![](https://pic1.zhimg.com/80/v2-32586bdcd7b20c55790c2447583e3b2a_720w.jpg?source=1940ef5c)
  - Q-table里state是有限的，而我们定义的state里面的数据往往都是连续的，很难在有限个state里面去很好的表达。
- 2) Deep Q Network（参考：[Reinforcement Learning for Stock Prediction](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer) ）
  - ![](https://pic2.zhimg.com/50/v2-d3b707c6c64979f4309f1227728c8ab4_hd.jpg?source=1940ef5c)
  - ![](https://pic2.zhimg.com/80/v2-d3b707c6c64979f4309f1227728c8ab4_720w.jpg?source=1940ef5c)
  - 在1的基础上，将Q-table的功能用一个深度学习网络来实现，解决了有限个state的问题。
- 3) Actor Critic （参考：[Deep-Reinforcement-Learning-in-Stock-Trading](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer)）
  - ![](https://pic1.zhimg.com/50/v2-cb2603beb4737f06c371cbbe0f07d3a4_hd.jpg?source=1940ef5c)
  - ![](https://pic1.zhimg.com/80/v2-cb2603beb4737f06c371cbbe0f07d3a4_720w.jpg?source=1940ef5c)
  - 用两个模型，一个同DQN输出Q值，另一个直接输出行为。但由于两个模型参数更新相互影响，较难收敛。
- 4) DDPG（参考：[ml-stock-prediction](https://link.zhihu.com/?target=https%3A//bigquant.com/community/t/topic/169658%3Futm_source%3Dzhihu%26utm_medium%3Dzhihu_answer%26utm_campaign%3D190723_758082798_zhihu_answer)）
  - ![](https://pic1.zhimg.com/50/v2-5caa0ee24f9e5466f54cda356c241985_hd.jpg?source=1940ef5c)
  -  ![](https://pic1.zhimg.com/80/v2-5caa0ee24f9e5466f54cda356c241985_720w.jpg?source=1940ef5c)
  - 加入了不及时更新参数的模型，解决难收敛的问题
 
3\. 由于多数方法中都用到了深度神经网络，我们还需要对神经网络的模型，深度，还有其他参数进行一个选择。

4\. 比较简单的应用逻辑是对单个股票某一时间段进行择时，如果需要也可以在这个基础上进行一些调整，对某个股票池的股票进行分析，调整为一个选股策略。

## 游戏

### 雅达利（吃豆子）

DeepMind开发出过一个能在57款雅达利游戏上都超越人类玩家的智能体，背后依靠的同样是强化学习算法。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/18aa479e3b6f4d6196905e8cc3636d6d~noop.image)

### 赛车

【2022-9-7】[怎样从零开始训练一个AI车手？](https://www.toutiao.com/article/7138644640294683172)
- 一个智能体（你的猫）在与环境（有你的你家）互动的过程中，在奖励（猫条）和惩罚（咬头）机制的刺激下，逐渐学会了一套能够最大化自身收益的行为模式（安静，躺平）
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/c5cf85edc3544651833d93825813ed15~noop.image)
- 如何训练AI司机
- 借用一个道具：来自亚马逊云科技的Amazon DeepRacer。一辆看上去很概念的小车，跟真车的比例是1比18。车上安装了处理器、摄像头，甚至还可以配置激光雷达，为的就是实现自动驾驶——当然，前提就是我们先在车上部署训练好的强化学习算法。算法的训练需要在虚拟环境中进行，为此Amazon DeepRacer配套了一个管理控制台，里面包含一个3D赛车模拟器，能让人更直观地看到模型的训练效果。
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/65cd202b74434959b401da2ede132212~noop.image)
- ![](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/c22635fcd5694544839c34d230c468c0~noop.image)

### 混沌球

[混沌球背后的核心技术](https://rct.ai/zh-hans/blog/the-key-technology-behind-morpheus-engine)
- [视频地址](https://rct.ai/static/images/395a257365304e399533516544b18b3c.mp4)

<video width="620" height="440" controls="controls" autoplay="autoplay">
  <source src="https://rct.ai/static/images/395a257365304e399533516544b18b3c.mp4" type="video/mp4" />
</video>

<iframe src="https://rct.ai/static/images/395a257365304e399533516544b18b3c.mp4" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" height="600" width="100%"> </iframe>

混沌球算法提升游戏交互体验
- 传统的叙事，无论是单线的故事，还是现在几乎所有的所谓 “交互式电影”，都仍然是基于 “**事件**” 作为叙事的基本单元，也就是什么事情发生了，然后什么事情发生了。传统的交互式数字娱乐内容，无非是让用户可以自由的从给定的两到三个选项中，选择不同的接下来会发生的事件，整个叙事仍然是基于预先定义好的路径来往前推进的。
- 而混沌球与传统的叙事方式完全不同，我们将 “事件” 替换为一个又一个明确定义了入口和出口的**黑盒**，在每一个切片的混沌球里，开始和结局（一个或者多个）是确定的，但是玩家每一次如何从开始到达结局，则是**混沌**的，是路径不明确的。这个路径只有当玩家不断的和虚拟世界里的虚拟人物 NPC 作出交互，这些 NPC 根据深度强化学习训练后的模型作出动态且实时的反应来推动剧情发展之后，才会被确定下来。这也是我们为什么命名为**混沌球**算法的原因。因此，做到真正的交互式叙事的关键，在于将叙事的中心，从故事本身，转移到故事里的所有可能参与者身上，由所有可能参与者的逻辑来共同推动和串联不同的剧情可能性。
- ![](https://rct.ai/static/images/88e5ceea1dd64e12803b3e411adf6e23.png)

仿真引擎工作方式
- ![](https://rct.ai/static/images/af5afdef214a4fe18d1a96f1dfea50b7.png)

### 公园散步

机器人的公园漫步
- 并非是在实验室的模拟环境，而是在真实的室内外地形中，作者采用强化学习和机器人控制器相结合的方法，在短短20分钟内成功让机器人学会四足行走
- [项目地址](https://github.com/ikostrikov/walk_in_the_park)
- [论文地址](https://arxiv.org/abs/2208.07860)
- [A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning](https://sites.google.com/berkeley.edu/walk-in-the-park)，含机器狗的演示[视频](https://www.youtube.com/embed/YO1USfn6sHY)
- <iframe width="560" height="315" src="https://www.youtube.com/embed/YO1USfn6sHY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

# 问题

## 为什么说强化学习在近年不会被广泛应用

- 【2021-3-11】[为什么说强化学习在近年不会被广泛应用？](https://mp.weixin.qq.com/s/keXcOu5CMip-rwFQ_oQLyg)，[知乎帖子](https://www.zhihu.com/question/404471029)
  - （1）**数据收集过程不可控**
    - 不同于监督学习，强化学习的数据来自agent跟环境的各种交互。对于数据平衡性问题，监督学习可以通过各种补数据加标签来达到数据平衡。但这件事情对强化学习确实非常难以解决的，因为数据收集是由policy来做的，无论是DQN的Q-network还是AC架构里的actor，它们在学习过程中，对于任务激励信号的理解的不完善的，很可能绝大部分时间都在收集一些无用且重复的数据。虽然有prioritized replay buffer来解决训练优先级的问题，但实际上是把重复的经验数据都丢弃了。在实际应用中，面对一些稍微复杂点的任务还是需要收集一大堆重复且无用的数据。这也是DRL sample efficiency差的原因之一。
  - （2）**环境限制**
    - DRL问题中，环境都是从初始状态开始，这限制了很多可能的优化方向。比如在状态空间中，可以针对比较“新”的状态重点关注，控制环境都到这个状态。但目前的任务，很多环境的state-transition function都是stochastic的，都是概率函数。即便记录下来之前的action序列，由于环境状态转移的不确定性，也很难到达类似的状态。更别提policy本身也是stochastic的，这种双重stochastic叠加，不可能针对“重点”状态进行优化。
    - 同时这种限制也使得一些测试场景成为不可能。比如自动驾驶需要测试某个弯道，很难基于当前的policy在状态空间中达到类似的状态来重复测试policy在此状态下的鲁棒性。
  - （3）玄之又玄，**可解释性较差**
    - 本来Q-learning就是一个通过逐步学习来完善当前动作对未来收益影响作出估计的过程。加入DNN后，还涉及到了神经网络近似Q的训练。这就是“不靠谱”上又套了一层“不靠谱”。如何验证策略是正确的？如何验证Q function是最终收敛成为接近真实的估计？这些问题对于查表型的Q-learning来说，是可以解决的，无非就是工作量的问题。但对于大规模连续状态空间的DQN来说，基本上没法做。论证一个policy有效，也就是看看render以后的效果，看看reward曲线，看看tensorborad上的各个参数。连监督学习基本的正确率都没有。然后还要根据这些结果来调reward function，基本上都在避免回答why这个问题。
  - （4）**随机探索**
    - DRL的探索过程还是比较原始。现在大多数探索，epsilon-greedy，UCB都是从多臂老虎机来的，只针对固定state的action选择的探索。扩展到连续状态空间上，这种随机探索还是否有效，在实际落地过程中，还是要打个问号。因为也不能太随机了。大家都说PPO好，SAC强，探索过程也只不过是用了stochastic policy，做了个策略分布的熵的最大化。本质还是纯随机。虽然有些用好奇心做探索的工作，但也还是只把探索任务强加给了reward，指标不治本。
  - 当前DRL的实际科研的进步速度要远远慢于大众对于AI=DL+RL的期望。能解决的问题：
    - **固定场景**：状态空间不大，整个trajectory不长
    - **问题不复杂**：没有太多层次化的任务目标，奖励好设计
    - **试错成本低**：咋作都没事
    - **数据收集容易**：百万千万级别的数据量，如果不能把数据收集做到小时级别，那整个任务的时间成本就不太能跟传统的监督相比
    - **目标单纯**：容易被reward function量化，比如各种super-human的游戏。对于一些复杂的目标，比如几大公司都在强调拟人化，目前没有靠谱的解决方案
  - 落地领域也就游戏了，而且是简单游戏，比如固定场景、小地图上的格斗，比如街霸、王者之类。要是大地图、开放世界的话，光捡个枪、开个宝箱就能探索到猴年马月了。也没想象中那么fancy，基本没有图像类输入，全是传感器类的内部数据，所以同类型任务的训练难度还没到Atari级别，这几年时间，DOTA2和星际基本上是游戏领域内到顶的落地
    - DeepMind近期的一篇论文：[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)。这篇论文对原始DQN框架做了一些渐进式改进，证明他们的RainbowDQN性能更优。
  - 做强化的同学们一点信息：
    - 强化学习岗位很少，因为落地难+烧钱，基本只有几个头部游戏公司会养一个规模不大的团队。
    - 纯强化的技术栈不太好跳槽，除了游戏外，别的领域很难有应用。
    - 20年huawei的强化夏令营，同时在线也有好几万人，想想这规模，未来几年这些研究生到job market会多卷。
- **强化学习是唯一一个可以明目张胆地在测试集上进行训练的机器学习网络**。都2020了强化学习除了能玩游戏还能做什么？强化学习的特点是面向目标的算法，过程基本很难拆解，没法管控，如果目标没法在商业公司被很好的认可
- 每当有人问我强化学习能否解决他们的问题时，我会说“不能”。而且我发现这个回答起码在70%的场合下是正确的。
- 更多参考：[深度强化学习的弱点和局限](https://zhuanlan.zhihu.com/p/34089913)，[原文:Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
- Deep RL实现进一步发展的一些条件：[原文地址](www.alexirpan.com/2018/02/14/rl-hard.html)，[中文版本: 深度强化学习的弱点和局限](https://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247485609&idx=1&sn=6b71f5f8ebd4e920384f07b97ce92a9c&chksm=eb4eec6adc39657c81169f1ae9ce477e4da692941238c35deb26a11ed7ec70073784cfd935a8#rd)
  - 易于产生近乎无限的经验；
  - 把问题简化称更简单的形式；
  - 将自我学习引入强化学习；
  - 有一个清晰的方法来定义什么是可学习的、不可取消的奖励；
  - 如果奖励必须形成，那它至少应该是种类丰富的。
- 下面是我列出的一些关于未来研究趋势的合理猜测 ，希望未来Deep RL能带给我们更多惊喜。
  - 局部最优就足够了。我们一直以来都追求全局最优，但这个想法会不会有些自大呢？毕竟人类进化也只是朝着少数几个方向发展。也许未来我们发现局部最优就够了，不用盲目追求全局最优；
  - 代码不能解决的问题，硬件来。我确信有一部分人认为人工智能的成就源于硬件技术的突破，虽然我觉得硬件不能解决所有问题，但还是要承认，硬件在这之中扮演着重要角色。机器运行越快，我们就越不需要担心效率问题，探索也更简单；
  - 添加更多的learning signal。稀疏奖励很难学习，因为我们无法获得足够已知的有帮助的信息；
  - 基于模型的学习可以释放样本效率。原则上来说，一个好模型可以解决一系列问题，就像AlphaGo一样，也许加入基于模型的方法值得一试；
  - 像参数微调一样使用强化学习。第一篇AlphaGo论文从监督学习开始，然后在其上进行RL微调。这是一个很好的方法，因为它可以让我们使用一种速度更快但功能更少的方法来加速初始学习；
  - 奖励是可以学习的。如果奖励设计如此困难，也许我们能让系统自己学习设置奖励，现在模仿学习和反强化学习都有不错的发展，也许这个思路也行得通；
  - 用迁移学习帮助提高效率。迁移学习意味着我们能用以前积累的任务知识来学习新知识，这绝对是一个发展趋势；
  - 良好的先验知识可以大大缩短学习时间。这和上一点有相通之处。有一种观点认为，迁移学习就是利用过去的经验为学习其他任务打下一个良好的基础。RL算法被设计用于任何马尔科夫决策过程，这可以说是万恶之源。那么如果我们接受我们的解决方案只能在一小部分环境中表现良好，我们应该能够利用共享来解决所有问题。之前Pieter Abbeel在演讲中称Deep RL只需解决现实世界中的任务，我赞同这个观点，所以我们也可以通过共享建立一个现实世界的先验，让Deep RL可以快速学习真实的任务，作为代价，它可以不太擅长虚拟任务的学习；
  - 难与易的辨证转换。这是BAIR（Berkeley AI Research）提出的一个观点，他们从DeepMind的工作中发现，如果我们向环境中添加多个智能体，把任务变得很复杂，其实它们的学习过程反而被大大简化了。让我们回到ImageNet：在ImageNet上训练的模型将比在CIFAR-100上训练的模型更好地推广。所以可能我们并不需要一个高度泛化的强化学习系统，只要把它作为通用起点就好了。



# 结束


