---
layout: post
title:  "并行计算系列 GPU/TPU"
date:   2020-10-28 19:25:00
categories: 编程语言
tags: CPU GPU TPU Tensorflow Pytorch 并行计算 加速 分布式
excerpt: 高性能计算知识点，如CPU/GPU/TPU，及代码实现
author: 鹤啸九天
mathjax: true
permalink: /gpu
---

* content
{:toc}

# 总结

- 【2020-10-28】[CUDA编程入门极简教程](https://blog.csdn.net/xiaohu2022/article/details/79599947)

# 简介

- 2006年，NVIDIA公司发布了`CUDA`，CUDA是建立在NVIDIA的CPUs上的一个通用并行计算平台和编程模型，基于CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。
- 近年来，`GPU`最成功的一个应用就是**深度学习**领域，基于GPU的并行计算已经成为训练深度学习模型的标配。截止2018年3月，最新的CUDA版本为CUDA 9。
- GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此说GPU并行计算时，其实是指的**基于CPU+GPU的异构计算架构**。
- 在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为**主机端**（host），而GPU所在位置称为**设备端**（device），如下图所示。
  - ![](https://img-blog.csdn.net/20180318132344300?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- GPU包括更多的运算核心，其特别适合**数据并行**的**计算密集型**任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。
- 另外，CPU上的线程是重量级的，**上下文切换开销大**，但是GPU由于存在很多核心，其线程是轻量级的。
- 因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。
- ![](https://img-blog.csdn.net/20180318132422473?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


- [在显卡界，对于「A 卡」和「N 卡」的信仰是如何形成的？](https://www.zhihu.com/question/28520691)
- ![](https://pic4.zhimg.com/80/8bfb07ebbf27f2e486a0ae933515a58d_720w.jpg?source=1940ef5c)

- 显卡性能天梯图
  - [如何通过显卡名字判断显卡的好坏？](https://www.zhihu.com/question/34970298)


## CPU

- CPU 如何来执行这样的大型矩阵运算任务呢？一般 CPU 是基于冯诺依曼架构的通用处理器，这意味着 CPU 与软件和内存的运行方式如下

## CUDA

- CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语言的支持，如C/C++，Python，Fortran等语言，这里我们选择CUDA C/C++接口对CUDA编程进行讲解。开发平台为Windows 10 + VS 2013，Windows系统下的CUDA安装教程可以参考[这里](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html)
- CUDA编程模型支持的编程语言
    - ![](https://img-blog.csdn.net/2018031813244714?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L3hpYW9odTIwMjI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)
- CUDA编程模型基础
    - 在给出CUDA的编程实例之前，这里先对CUDA编程模型中的一些概念及基础知识做个简单介绍。CUDA编程模型是一个异构模型，需要CPU和GPU协同工作。在CUDA中，host和device是两个重要的概念，我们用host指代CPU及其内存，而用device指代GPU及其内存。CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。同时，host与device之间可以进行通信，这样它们之间可以进行数据拷贝。典型的CUDA程序的执行流程如下：
        - 分配host内存，并进行数据初始化；
        - 分配device内存，并从host将数据拷贝到device上；
        - 调用CUDA的核函数在device上完成指定的运算；
        - 将device上的运算结果拷贝到host上；
        - 释放device和host上分配的内存。
- 参考：[CUDA编程入门极简教程](https://blog.csdn.net/xiaohu2022/article/details/79599947)

## GPU

### 行情

GPU市场上，NVIDIA占了大部分（**N卡**），AMD（**A卡**）次之，接着是**苹果**（好像是intel），详见[图](https://upload-images.jianshu.io/upload_images/64542-4715aee0485de0ea.jpeg)
- <img src="https://upload-images.jianshu.io/upload_images/64542-4715aee0485de0ea.jpeg" width = "400" height = "300" alt="设备对比" align=center />
- [2019显卡天梯图汇总](http://www.ppnames.com/html/360.html)
- ![](http://www.ppnames.com/img2019/20181017/102.png)

### GPU验证

① NVIDIA自带
- NVIDIA自带的驱动检测方法，能看到GPU的配置，可以看到Google Colab的GPU是Tesla T4，显存15G，强于上一版本K80 

```shell
#命令
nvidia-smi
# 结果如下：
Wed Jun  5 03:02:36 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   57C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

# 动态监控
watch -n 1 nvidia-smi
```

GPU监控
- 动态监控GPU使用率
- 按Ctrl+C退出监控
- [参考GPU进程查看管理](https://blog.csdn.net/Kaige_Zhao/article/details/79748079)

② GPU监控工具-[gpustat](https://www.ctolib.com/wookayin-gpustat.html)

- 仅适用于N卡（NVIDIA ），A卡不行（AMD）
- 效果示例：[图](https://github.com/wookayin/gpustat/raw/master/screenshot.png)
   - ![](https://github.com/wookayin/gpustat/raw/master/screenshot.png)
   - []里的数字是GPU编号，即常见的多机多卡里的“卡”

```shell
#pip install gpustat # 安装
echo "gpustat工具统计-`hostname`"
gpustat -cp # 检测
# 结果如下
528e504a63a3  Wed Jun  5 03:16:16 2019
#[0] Tesla T4         | 48'C,   0 % |     0 / 15079 MB |
# Google的Colab配置了Tesla T4的显卡一个，显存15G
# 动态监控
watch --color -n1 gpustat -cpu
```

③ Pytorch的GPU测试方法

```python
import torch

torch.cuda.is_available() # true
torch.backends.cudnn.enabled # true
```

④ Tensorflow的GPU测试代码

```python
import tensorflow as tf
device_name = tf.test.gpu_device_name()
print('Detect GPU:', device_name) # ('Detect GPU:', '/device:GPU:0')
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name)) # Found GPU at: /device:GPU:0
```

### tf里GPU参数设置

GPU与CPU的不同
- GPU: 计算密集型任务
- CPU: I/O, 逻辑运算

- [gpu使用方法](https://www.jianshu.com/p/26ac409dfb38)
- 通过tf.device指定运行设备，不管CPU多少个，一律标记为/cpu0，而GPU不同，分别是/gpu:1-n
- Tensorflow使用GPU时默认占满所有可用GPU的显存，但只在第一个GPU上进行计算
  - 多GPU时，只有一块GPU真正在工作，如果不加以利用，另一块GPU就白白浪费了
  - GPU是一种相对昂贵的计算资源，虽然正值矿难，相比之前动辄八九千一块1080Ti的价格低了不少，但也不是一般人能浪费的起的
- 因此如何有效提高GPU特别是tensorflow上的利用率就成为了一项重要的考量。
- 解决一：设置可见的GPU，CUDA_VISIBLE_DEVICES, 0表示GPU编号(0~n-1)

Shell代码：

```sh
export CUDA_VISIBLE_DEVICES=0
```
Python代码：

```python
import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"
```

- 缓解tf占满整个GPU

```python
config = tf.ConfigProto(allow_soft_placement=True,allow_grouth=True)
config.gpu_options.per_process_gpu_memory_fraction = 0.9 #占用90%显存
sess = tf.Session(config=config)
```

- 解决二：有时候我们更需要利用好已有的卡，来或得线性的加速比，以便在更短的时候获取参考结果，上面的方法就无能无力，需要自己写代码实现多GPU编程
- 参考：[tensorflow 多GPU编程 完全指南](https://blog.csdn.net/minstyrain/article/details/80986397)


### GPU 如何工作

- 为了获得比 CPU 更高的吞吐量，GPU 使用一种简单的策略：在单个处理器中使用成千上万个 ALU。现代 GPU 通常在单个处理器中拥有 2500-5000 个 ALU，意味着同时执行数千次乘法和加法运算。

### Parallelism-GPU并行

- There are two types of parallelism:
- **模型**并行，Model parallelism - Different GPUs run different part of the code. Batches of data pass through all GPUs.
    - 适用于大规模
- **数据**并行，Data parallelism - We use multiple GPUs to run the same TensorFlow code. Each GPU is feed with different batch of data. 每个节点是各有一份模型copy，称为tower
    - 同步+异步，同步适用于设备差异不大，小数据，异步抖动，适用大数据
    - 图内+图间

## TPU

资料
- 【2019-08-11】[TPU灵魂三问：What？Why？How？](https://www.sohu.com/a/333008988_505915)
- [TPU是如何超越GPU，成为深度学习首选处理器的](https://www.toutiao.com/a6596935089408442883/?tt_from=mobile_qq&utm_campaign=client_share&timestamp=1536065103&app=news_article&utm_source=mobile_qq&iid=42461222229&utm_medium=toutiao_android&group_id=6596935089408442883)

### 介绍

- **张量处理单元**（TPU）是一种定制化的 ASIC 芯片，它由谷歌从头设计，并专门用于机器学习工作负载。TPU 为谷歌的主要产品提供了计算支持，包括翻译、照片、搜索助理和 Gmail 等。Cloud TPU 将 TPU 作为可扩展的云计算资源，并为所有在 Google Cloud 上运行尖端 ML 模型的开发者与数据科学家提供计算资源
- 在 Google Next’18 中，我们宣布 TPU v2 现在已经得到用户的广泛使用，包括那些免费试用用户，而 TPU v3 目前已经发布了内部测试版。

- 谷歌设计 TPU 时，构建了一种领域特定的架构。这意味着，没有设计一种通用的处理器，而是专用于神经网络工作负载的矩阵处理器。TPU 不能运行文本处理软件、控制火箭引擎或执行银行业务，但它们可以为神经网络处理大量的乘法和加法运算，同时 TPU 的速度非常快、能耗非常小且物理空间占用也更小。
- 其主要助因是对冯诺依曼瓶颈的大幅度简化。因为该处理器的主要任务是矩阵处理，TPU 的硬件设计者知道该运算过程的每个步骤。因此他们放置了成千上万的乘法器和加法器并将它们直接连接起来，以构建那些运算符的物理矩阵。这被称作脉动阵列（Systolic Array）架构。在 Cloud TPU v2 的例子中，有两个 128X128 的脉动阵列，在单个处理器中集成了 32768 个 ALU 的 16 位浮点值。
- 我们来看看一个脉动阵列如何执行神经网络计算。首先，TPU 从内存加载参数到乘法器和加法器的矩阵中。

### 实践

参考：[Colab提供了免费TPU，机器之心帮你试了试](https://www.jiqizhixin.com/articles/2018-10-11-5)

开启笔记本的GPU开关
- 操作：修改→笔记本设置→勾选GPU

Colab环境
- 查看配置信息
- 输出 TPU 地址及 TPU 设备列表 ，表名配置了TPU资源，否则会报错
- Colab 为「TPU 运行时」分配 CPU 和 TPU，其中分配的 TPU 工作站有八个核，后面配置的 TPU 策略会选择 8 条并行 shards

测试代码

```python
"""在TPU运行时下测试有没有分配TPU计算资源"""
import os
import pprint
import tensorflow as tf

print("Colab环境信息")
print(os.environ)
# {'COLAB_TPU_ADDR': '10.68.94.74:8470', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'CLOUDSDK_CONFIG': '/content/.config',
#  'CUDA_VERSION': '10.0.130', 'PATH': '/usr/local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin', 'HOME': '/root', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64',
#  'LANG': 'en_US.UTF-8', 'SHELL': '/bin/bash', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'CUDA_PKG_VERSION': '10-0=10.0.130-1',
#  'SHLVL': '1', 'NCCL_VERSION': '2.4.2', 'NVIDIA_VISIBLE_DEVICES': 'all', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 
#  'DEBIAN_FRONTEND': 'noninteractive', 'CUDNN_VERSION': '7.5.0.56', 'JPY_PARENT_PID': '37', 'PYTHONPATH': '/env/python',
# 'DATALAB_SETTINGS_OVERRIDES': '{"kernelManagerProxyPort":6000,"kernelManagerProxyHost":"172.28.0.3","jupyterArgs":["--ip=\\"172.28.0.2\\""]}',
# 'NO_GCE_CHECK': 'True', 'GLIBCXX_FORCE_NEW': '1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', '_': '/tools/node/bin/forever', 'LD_PRELOAD': '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.0 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=410,driver<411', 'OLDPWD': '/', 'HOSTNAME': '59881c4943e6', 'ENV': '/root/.bashrc', 'COLAB_GPU': '0', 'PWD': '/', 'XRT_TPU_CONFIG': 'tpu_worker;0;10.68.94.74:8470', 'GLIBCPP_FORCE_NEW': '1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'TPU_NAME': 'grpc://10.68.94.74:8470', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}

# 检查GPU是否启用
if 'COLAB_TPU_ADDR' not in os.environ:
  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')
else:
  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print ('TPU address is', tpu_address)

  with tf.Session(tpu_address) as session:
    devices = session.list_devices()
    
  print('TPU devices:')
  pprint.pprint(devices)
```

TPU Hello world
- TPU在contrib中，tf.contrib.tpu, 从一个简单的代码开始

```python
import tensorflow as tf 
import numpy as np
import timeit
​
tf.reset_default_graph()
img = np.random.randn(128, 256, 256, 3).astype(np.float32)
w = np.random.randn(5, 5, 3, 256).astype(np.float32)
conv = tf.nn.conv2d(img, w, [1,2,2,1], padding='SAME')
#=======================
with tf.Session() as sess:
    # with tf.device("/gpu:0") as dev:
    %timeit sess.run(conv)
#输出：loop, best of 3: 2.32 s per loop
#========================   
# 指定gpu环境
# with tf.Session() as sess:
#     with tf.device("/gpu:0") as dev:
#         %timeit sess.run(conv)
#输出：loop, best of 3: 2.34 s per loop
```

### TPU对比

思考
- 分别选择 CPU、GPU 和 TPU 作为运行时状态，运行代码并迭代一次耗时为：2.44 s、280 ms、2.47 s

|设备|耗时|备注|
|---|---|---|
|CPU|2.44s|-|
|GPU|280ms|-|
|TPU|2.47s|-|

- 使用tpu和gpu都没有差异吗？
- 原因：以上代码并未启用TPU或GPU
   - 启用TPU 似乎需要特定的函数与运算，不像 CPU 和 GPU 可以共用代码。
   - 仅修改运行时状态，并不会真正调用 TPU 资源，实际运行的还是 CPU
-  TF 存在一个神奇的类 tf.contrib.tpu，调用 TPU 资源必须用它改写模型
-  tf.contrib.tpu 类提供了两种使用 TPU 的简单方法
   - 直接使用 Keras 接口
      - tf.contrib.tpu.keras_to_tpu_model 方法可以直接将 Keras 模型与对应的权重复制到 TPU，并返回 TPU 模型
   - 用 TPUEstimator 构建模型
       -  tf.contrib.tpu.TPUEstimator
       - TPUEstimator 类继承自 Estimator 类，因此它不仅支持在 TPU 上运算，同时还支持 CPU 和 GPU 的运算，更方便
修正TPU代码
- 结果意外，卷积运算每一次迭代只需要 1.22 ms。
- 如下图所示，很可能存在变量缓存等其它因素造成了一定程度的缓慢，但 TPU 的速度无可置疑地快，就是需要修改代码
- ![](https://image.jiqizhixin.com/uploads/editor/4b93c3b3-e368-473a-aa0f-f7c6eb62d8f7/1539240581969.png)

```python
import numpy as np

def add_op(x, y):
  return x + y
  
x = tf.placeholder(tf.float32, [10,])
y = tf.placeholder(tf.float32, [10,])
# tpu选项在contrib中
tpu_ops = tf.contrib.tpu.rewrite(add_op, [x, y])

session = tf.Session(tpu_address)
try:
  print('Initializing...')
  session.run(tf.contrib.tpu.initialize_system())
  print('Running ops')
  print(session.run(tpu_ops, {x: np.arange(10), y: np.arange(10)}))
finally:
  # For now, TPU sessions must be shutdown separately from
  # closing the session.
  session.run(tf.contrib.tpu.shutdown_system())
  session.close()
```

### 总结

- Colab 提供的 TPU 要比 GPU 快 3 倍左右，一般 TPU 训练 5 个 Epoch 只需要 40 多秒，而 GPU 需要 2 分多钟。
- Colab 确实提供了非常强劲的免费 TPU，而且使用 Keras 或 TPUEstimator 也很容易重新搭建或转换已有的 TensorFlow 模型

# GPU环境准备

## 测试：Colab实现

- 【2019-06-05】[colab笔记代码](https://colab.research.google.com/drive/1uqhBy1zNF7uU7EdrQqwlMSoykgxvubyg)
- [gpu使用方法](https://www.jianshu.com/p/26ac409dfb38)

### tf 2 版本

参考：[Tensorflow如何使用GPU训练](https://blog.csdn.net/qq_31554953/article/details/107302404)

```python
import tensorflow as tf

tf.debugging.set_log_device_placement(True)

# Create some tensors, 自动选择gpu
a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print('结果:', c)
# Place tensors on the CPU # 指定gpu
with tf.device('/CPU:0'):
    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
c = tf.matmul(a, b)
print('结果:', c)
# TensorFlow默认映射该进程可见的几乎所有GPU的所有GPU内存（取决于CUDA_VISIBLE_DEVICES）。 
# 这样做是为了通过减少内存碎片来更有效地使用设备上相对宝贵的GPU内存资源。 
# 要将TensorFlow限制为一组特定的GPU，用tf.config.experimental.set_visible_devices方法。
gpus = tf.config.experimental.list_physical_devices('GPU')
print('物理可用gpu: ', gpus)
if gpus:
  # Restrict TensorFlow to only use the first GPU
  try:
    tf.config.experimental.set_visible_devices(gpus[0], 'GPU') # 限制可用gpu
    # 限制各GPU内存增速
    # Currently, memory growth needs to be the same across GPUs
    # ① 方法一：
    #       使用set_memory_growth
    #       或：环境变量TF_FORCE_GPU_ALLOW_GROWTH设置为true
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    # ② 方法二：虚拟GPU设备，set_virtual_device_configuration
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],
        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
    # 显示逻辑可用GPU
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(f"物理Physical GPUs: {len(gpus)}, 逻辑Logical GPUs: {len(logical_gpus)}")
  except RuntimeError as e:
    # Visible devices must be set before GPUs have been initialized
    print(e)
```

### tf 1 版本

```python
import tensorflow as tf
# ---- tf 1.*版本 -----
import tensorflow.compat.v1 as tf 
tf.compat.v1.disable_eager_execution() # 关闭即时执行模式

import os

#GPU:计算密集型任务, CPU: I/O, 逻辑运算
if tf.test.is_gpu_available(): # 检测GPU是否可用
    print('GPU可用: %s'%(tf.test.gpu_device_name()))
# 环境变量设置，默认有限选择GPU，占用所有GPU的所有显存
config = tf.ConfigProto()
os.environ['TF_CPP_MIN_LOG_LEVEL']='5'#日志级别
#----自定义使用哪些GPU----
os.environ["CUDA_VISIBLE_DEVICES"] = "2"#只使用第三块GPU，默认选用id最小的gpu
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"#只使用第2-3块GPU
#----自定义显存分配----
config.gpu_options.allow_growth = True#刚开始分配较少内存，逐步按需分配，注意：系统不自动释放内存，以免可能导致更严重的内存碎片情况
config.gpu_options.per_process_gpu_memory_fraction = 0.4#按比例分配,40%，方便多任务并行

a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')
b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')
c = a + b
#通过tf.device指定运行设备。不管CPU多少个，一律标记为/cpu0,而GPU不同，分别是/gpu:1-n
#注意：不宜包含过多设备约束，不同设备实现方式不同，多了会限制移植能力
with tf.device('/gpu:0'):
    #将tf.Variable强制放在GPU上会报错,GPU只在部分数据类型上支持tf.Variable操作（variable_ops.cc）
    #除非启动自动更改设备的参数allow_soft_placement
    a_gpu = tf.Variable(0, name="a_gpu")
# 通过log_device_placement参数来输出运行每一个运算的设备。
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# 通过allow_soft_placement参数自动将无法放在GPU上的操作放回CPU上。
sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))
#sess.run(tf.initialize_all_variables())
sess.run(tf.global_variables_initializer())
print(sess.run(c))
#jupyter中显示不出来
```

没GPU时的结果：

```shell
2018-03-26 19:29:38.606679: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
Device mapping: no known devices.
2018-03-26 19:29:38.606972: I tensorflow/core/common_runtime/direct_session.cc:299] Device mapping:
add: (Add): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610106: I tensorflow/core/common_runtime/placer.cc:874] add: (Add)/job:localhost/replica:0/task:0/device:CPU:0
b: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610137: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:CPU:0
a: (Const): /job:localhost/replica:0/task:0/device:CPU:0
2018-03-26 19:29:38.610142: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:CPU:0
[ 2.  4.  6.]
```

有GPU时的结果：

```
2018-03-26 19:27:46.864593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: Tesla P40 major: 6 minor: 1 memoryClockRate(GHz): 1.531
pciBusID: 0000:02:00.0
totalMemory: 22.38GiB freeMemory: 745.94MiB
2018-03-26 19:27:46.864673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1)
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1
2018-03-26 19:27:47.193611: I tensorflow/core/common_runtime/direct_session.cc:297] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P40, pci bus id: 0000:02:00.0, compute capability: 6.1
add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194462: I tensorflow/core/common_runtime/placer.cc:874] add: (Add)/job:localhost/replica:0/task:0/device:GPU:0
b: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194498: I tensorflow/core/common_runtime/placer.cc:874] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0
a: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2018-03-26 19:27:47.194526: I tensorflow/core/common_runtime/placer.cc:874] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0
[2. 4. 6.]
```

# 并行

- 【2022-2-23】贝壳，onnxruntime优化过后的bert模型，cpu推理延迟能从300ms降到100ms以内
- 【2021-10-13】[OpenAI 研究员最新博客：如何在多GPU上训练真正的大模型？](https://mp.weixin.qq.com/s?__biz=MzU5ODg0MTAwMw==&mid=2247504041&idx=1&sn=a6a8ceaf1cb091d7832351bcddae6ffb&chksm=febc936dc9cb1a7bbcdeef42f304107d7fe221e7999f2a1a508c6164267dc12dd12ee29ad0eb&mpshare=1&scene=23&srcid=1013pNjTo5fSHOxkjfW5JoFs&sharer_sharetime=1634137253709&sharer_shareid=b8d409494a5439418f4a89712efcd92a#rd)，[原文链接](lilianweng.github.io/lil-log/2021/09/24/train-large-neural-networks.html)
- 单个GPU卡的内存有限，许多大模型的大小已经超过了单个GPU，为解决此类问题，训练深且大的神经网络的主要方法有训练**并行**加速、各种模型**架构**以及内存**节省**设计等。
  - （1）并行加速方法有以下几种：
    - **数据**并行性：将相同的模型权重复制到多个worker中，并将一部分数据分配给每个worker以同时进行处理。
    - **模型**并行性
    - **流水线**并行
    - **张量**并行
  - （2）模型架构方面主要有专家混合（MoE）方法。
  - （3）节省内存的设计方法，如：CPU卸载、激活重新计算、混合精度训练、压缩以及内存高效优化器等等。

## 并行加速

### 数据并行

数据并行性（Data parallelism (DP)）最简单的方法是：将相同的**模型权重**复制到多个worker中，并将一部分数据分配给每个worker以同时进行处理。
- 如果模型规模大于单个GPU的内存，Naive DP无法正常工作时。GeePS（Cui 等人，2016 年）之类的方法将暂时未使用的参数卸载回 CPU，以使用有限的 GPU 内存。数据交换传输在后端进行，且不干扰训练计算。
 
在每个小批量结束时，workers需要同步梯度或权重，以替换旧参数。常见有两种主要的同步方法，它们都有明确的优缺点：
- 1）大容量**同步**并行（ Bulk synchronous parallels (BSP)）：workers在每个小批量结束时同步数据。这种方法可以防止模型权重过时，同时获得良好的学习效率，但每台机器都必须停止并**等待**其他机器发送梯度。
- 2）**异步**并行（Asynchronous parallel (ASP)）：每个GPU工作进程异步处理数据，无需等待或暂停。然而，这种方法很容易导致网络使用陈旧的权重参数，从而**降低**统计学习效率。即使它增加了计算时间，也可能不会加快收敛的训练时间。
 
中间的某个地方是在每次x迭代时，全局同步梯度（x＞1）。自Pytorch v1.5版（Li等人，2021年）以来，该特征在平行分布数据（DDP）中被称为“梯度累积”。Bucket 梯度计算方法避免了梯度的立即AllReduce，而是将多个梯度变化值存储到一个AllReduce中以提高吞吐量，可以基于计算图进行计算和通信调度优化。

### 模型并行（大模型）

模型并行性（Model parallelism: MP）目的是解决**模型权重不能适应单个节点**的情况，通过将计算和模型参数分布在多台机器上进行训练。在数据并行中，每个worker承载整个模型的**完整副本**，而在MP中，每个worker上只分配模型参数的一小部分，从而减少了内存使用和计算。

由于深度神经网络通常包含一堆垂直层，因此将一个大型模型逐层拆分感觉很简单，其中一组连续的小层被分组到一个工作层上的一个分区中。然而，通过多个具有顺序依赖性的工作线程来运行每个数据批，会导致大量的**等待时间**和计算资源**利用率低下**的问题。

### 流水线并行（综合模型+数据）

通道并行（Pipeline parallelism: PP）将模型并行与数据并行相结合，以减少部分训练过程中出现的空闲时间。其主要思想是将一个小批量拆分为多个**微批次**，并使worker在每个阶段中能够同时处理一个微批次。需要注意的是，每个微批次需要**两次传递**，一次向前，一次向后。worker之间的通信仅传输激活（向前）和梯度（向后）。这些通道的调度方式以及梯度的聚合方式在不同的方法中有所不同。分区（workers）的数量也称为通道深度。

### 张量并行

模型并行和流水线并行都将一个模型垂直分割，可以将一个张量操作的计算水平分割到多个设备上，称为**张量并行**（tensor parallelism，TP）。
 
以当下比较流行的transformer为例，transformer模型主要由多层MLP和自我注意块组成。Megatron-LM（Shoeybi et al.2020）等人采用了一种简单的方法来并行多层计算MLP和自我注意。变压器中的MLP层包含GEMM（通用矩阵乘法）和非线性GeLU传输，按列拆分权重矩阵A


## 模型架构

**专家混合**（MoE）方法最近吸引了很多关注，因为研究人员（主要来自谷歌）试图突破模型大小的限制。该想法的核心是整合学习：多个弱学习模型组合以后会形成能力出众的学习模型。

Shazeer 等人于2017年发表了名为“稀疏门控专家混合”（MoE）层的文章，提出了在一个深度神经网络中可以通过连接多个专家的门控机制来实现输出控制的方法。

## 内存优化

### CPU卸载
 
当GPU内存已满时，一种选择是将暂时未使用的数据卸载到CPU，并在以后需要时将其读回（Rhu等人，2016）。数据卸载到CPU 的想法很简单，但由于它会延长训练时间，所以近年来不太流行。
 
### 激活重新计算
 
激活重新计算（Activation recomputation (also known as “activation checkpointing” or “gradient checkpointing”，Chen等人，[2016年](https://arvix.org/abs/1604.06174)）是一个以计算时间为代价减少内存占用的聪明而简单的想法

### 混合精度训练
 
Narang&Micikevicius等人（2018年）介绍了一种使用半精度浮点（FP16）数字训练模型而不损失模型精度的方法。


三种避免以半精度丢失关键信息的技术：
- 1）全精度原始权重。维护累积梯度的模型权重的全精度 (FP32) 副本， 对于向前和向后传递，数字四舍五入到半精度。主要是为了防止每个梯度更新（即梯度乘以学习率）可能太小而无法完全包含在 FP16 范围内（即 2-24 在 FP16 中变为零）的情况。
- 2）损失缩放。扩大损失以更好地处理小幅度的梯度（见图 16）， 放大梯度有助于将权重移动到可表示范围的右侧部分（包含较大值）占据更大的部分，从而保留否则会丢失的值。
- 3）算术精度。对于常见的网络算法（例如向量点积，向量元素相加减少），可以将部分结果累加到 FP32 中，然后将最终输出保存为 FP16，然后再保存到内存中。可以在 FP16 或 FP32 中执行逐点操作。

### 压缩
 
中间结果通常会消耗大量内存，尽管它们只在一次向前传递和一次向后传递中需要。这两种使用之间存在明显的时间差距。因此Jain等人（2018年）提出了一种数据编码策略，将第一次使用后的中间结果在第一次传递中进行压缩，然后将其解码回来进行反向传播。

### 内存高效优化器
 
优化器内存消耗。以流行的 Adam 优化器为例，它内部需要保持动量和方差，两者都与梯度和模型参数处于同一规模，但是需要节省 4 倍的模型权重内存。

# 分布式机器学习

【2022-6-2】[分布式机器学习](https://zhuanlan.zhihu.com/p/365662727)
- ![](https://pic1.zhimg.com/v2-8e4eefe63cc256d4420a881a00f2851f_1440w.jpg)

在深度学习时代，训练数据特别大的时候想要**单卡**完成训练基本是不可能的。所以就需要进行**分布式**深度学习。

## 基本原理

无论哪种机器学习框架，分布式训练的基本原理都是相同的。可以从**并行模式**、**架构模式**、**同步范式**、**物理架构**、**通信技术**等五个不同的角度来分类。

更多信息见优质paper，把 DP(Data Parallel)、MP(Model Parallel)、PP(Pipeline Parallel)各个方面讲的很透彻
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://zhuanlan.zhihu.com/p/106783111)

### 并行模式

分布式训练的目的在于将原本巨大的训练任务拆解成**多个子任务**，每个子任务在独立的机器上单独执行。

大规模深度学习任务的难点在于：
- 1) 训练**数据量巨大**：将数据拆解成多个小模型分布到不同的node上。→ **数据并行**
- 2) 训练模型的**参数巨大**：将数据集拆解分布到不同的node上。→ **模型并行**
  - NLP的预训练模型实在太大了

#### 数据并行

数据并行相对简单，N个node(也称为worker)构成一个**分布式集群**，每个worker处理1/N的数据。
- 理论情况下能达到**线性**的加速效果。TF、torch、Horovod都可以在原生支持或者微小的改动实现数据并行模式。

数据并行是在每个worker上存储一个模型的备份，在各个worker 上处理不同的**数据子集**。然后需要**规约**(reduce)每个worker的结果，在各节点之间同步模型参数。
- 这一步会成为数据并行的瓶颈，因为如果worker很多的情况下，worker之间的数据传输会有很大的时间成本。

参数同步后，需要采用不同的方法进行参数更新：
- **参数平均法**：最简单的一种数据平均化
- **更新式方法**

若采用**参数平均法**，训练的过程如下所示：基于模型的配置随机初始化网络模型参数
- 将当前这组参数分发到各个工作节点
- 在每个工作节点，用数据集的一部分数据进行训练
- 将各个工作节点的参数的**均值**作为**全局参数值**
- 若还有训练数据没有参与训练，则继续从第二步开始

**更新式**方法与**参数平均化**类似，主要区别在于，在**参数**服务器和**工作**服务器之间传递参数时，更新式方法只传递**更新信息**(梯度和张量)。

#### 模型并行

模型并行相对复杂，原理是分布式系统中的不同worker负责网络模型的不同部分，例如说，神经网络的不同层被分布到不同worker或者同一层的不同参数被分配到不同worker上。对于TF这种框架，可以拆分计算图成多个最小依赖子图到不同的worker上。同时在多个子图之间通过通信算子来实现模型并行。

但是**模型并行**实现起来比较复杂。工业界还是以**数据并行**为主。

补充：
- Model Parallel主要分两种：**intra-layer**拆分 和 **inter-layer**拆分
  - inter-layer拆分：对模型做网络上的拆分。将每一层或者**某几层**放在一个worker上单独训练。
    - 缺点：模型训练串行，整个模型的效率取决于最慢的那一层，存在资源浪费
- intranet-layer拆分：深度学习的网络结构基本都是一层一层的。常规的卷积、池化、BN等等。如果对某一层进行了拆分，那么就是intra-layer拆分。对单层的拆分其实就是拆分这一层的matrix运算。参考论文：Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism

#### 混合并行

随着训练设备的增加，多个worker之间的通信成本增加，模型Reduce的成本也越来越大，数据并行的瓶颈也随之出现。故有学者提出**混合并行**(数据并行+模型并行)


### 架构模式

分布式训练上会频繁用到**规约**(AllReduce)操作。主流的**分布式架构**主要分为`参数服务器`(ParameterServer) 和`基于规约`(Reduce)两种模式。早期还有基于`MPI`的方式，不过现在已经很少用了。

#### PS：参数服务器

ParameterServer模式是一种基于reduce和broadcat算法的经典架构。
- 其中一个/一组机器作为PS架构的**中心节点**，用来**存储参数和梯度**。
- 在更新梯度的时候，先全局reduce接受其他worker节点的数据，经过本地计算后(比如参数平均法)，再broadcast回所有其他worker。

PS架构的问题在于多个worker与ps通信，PS本身可能存在**瓶颈**。
- 随着worker数量的增加，整体通信量也线性增加，加速比也可能停滞在某个点位上。
- ![](https://pic3.zhimg.com/80/v2-eee6e2ad8aa00a8679298ff297508a16_1440w.jpg)

#### 基于规约 Reduce模式

基于规约的模式解决了上述的问题，最典型的是百度提出的 Ring-AllRuduce。
- 多个Worker节点连接成一个环，每个Worker依次把自己的梯度同步给下一个Worker，经过至多 2*(N-1) 轮同步，就可以完成所有Worker的梯度更新。
- 这种方式下所有节点的地位是平等的，因此不存在某个节点的**负载瓶颈**，随着Worker的增加，整体的通信量并不随着增加。加速比几乎可以跟机器数量成线性关系且不存在明显瓶颈。
- ![](https://pic1.zhimg.com/80/v2-5c777ca6d8ce4972d51f6ce73f3a044c_1440w.jpg)

目前，越来越多的分布式训练采用**Reduce**这种模式。Horovod中主要就是用的这种分布式架构。
- 更多资料参考: [兰瑞Frank：腾讯机智团队分享--AllReduce算法的前世今生](https://zhuanlan.zhihu.com/p/79030485)

### 同步范式

实际训练过程中可能遇到各种问题，比如：部分节点资源受限、卡顿、网络延时等等，因此梯度同步时就存在“**木桶**“效应，即集群中的某些worker比其他worker更慢，导致整个训练pipeline需要等待慢的worker，整个集群的训练速度受限于最慢机器的速度。

因此梯度的同步有“**同步**”(sync)、“**异步**”(Async)和**混合**三种范式。
- **同步**范式：只有所有worker完成当前的计算任务，整个集群才会开始下一次迭代。
  - TF中同步范式使用SyncReplicasOptimizer优化器
- **异步**模式刚好相反，每个worker只关心知己的进程，完成计算后就尝试更新，能与其他多个worker同步梯度完成取决于各worker当前时刻的状态。其过程不可控，有可能出现模型正确性问题。(可在训练时logging对比)
- **混合**范式结合以上两种情况，各个worker都会等待其他worker的完成，但不是永久等待，有timeout的机制。如果超时了，则此情况下相当于异步机制。并且没来得及完成计算的worker，其梯度则被标记为“stale”而抛弃或另做处理。

### 物理架构

物理架构主要是 **GPU架构**，即：单机单卡、单机多卡、多机单卡、多机多卡（最典型）
- 单机单卡：常规操作
- 单机**多卡**：利用一台GPU上的多块GPU进行分布式训练。数据并行和模型并行皆可。整个训练过程一般只有一个进程，多GPU之间的通信通过多线程的方式，模型参数和梯度在进程内是共享的(基于NCCL的可能不大一样)。这种情况下基于Reduce的架构比PS架构更合适一些，因为不需要一个显式的PS，通过进程内的Reduce即可完成梯度同步。
- **多机**单卡：操作上与多机多卡基本一致
- 多机**多卡**：多机多卡是最典型的分布式架构，所以它需要较好的进程间的通讯机制(多worker之间的通信)。

### 通信技术

分布式条件下的多进程、多worker之间的通信技术，常见的主要有：MPI、NCCL，GRPC等。
- **MPI**主要是被应用在超算等大规模计算领域，机器学习场景下使用较少。主要是openMPI原语等。
- **NCCL**是NVIDIA针对GPU设计的一种规约库，可以实现多GPU间的直接数据同步，避免内存和显存的，CPU和GPU间的数据拷贝成本。当在TensorFlow中选择单机多卡训练时，其默认采用的就是NCCL方式来通信。
- **GRPC**是比较成熟的通信技术了，spark等框架内也都有用到。


内容：
- 分布式训练的基本原理
- TensorFlow的分布式训练
- PyTorch的分布式训练框架
- Horovod分布式训练

## Tensorflow分布式实现

- 黄文坚的[Tensorflow分布式实战](https://blog.csdn.net/CodeMaster_/article/details/76223835)

### TF分布式训练方法

TensorFlow主要的分布式训练的方法有三种：
1. Customer Train Loop：最原始，由框架工程师自己开发
1. Estimator + Strategy：高级API，不用关心底层硬件
1. Keras + Strategy：最新出的keras的高级API

> - 实际的开发工作中，分布式的工作最好是交给框架，而工程师本身只需要关注任务模型的pipeline就行了。
> - 最经典的是Spark框架，工程师只需要关注数据处理的workflow，分布式的大部分工作都交给框架。深度学习的开发同样如此。

各种方式评价
- 第一种方式太过原生，整个分布式的训练过程完全交给工程师来处理，代码模块比较复杂，这里不做赘述。
- 第二种方式，Estimator是TF的一个高级API，在分布式场景下，其最大的特点是**单机和分布式代码一致**，且不需要考虑底层的硬件设施。Strategy是tensorflow根据分布式训练的复杂性，抽象出的多种分布式训练策略。TF1.x和TF2.x接口变化较大，不同版本名字可能不一样，以实际使用版本为准。用的比较多的是：
  - **MirroredStrategy**：适用于单机多卡、数据并行、同步更新的分布式训练，采用Reduce的更新范式，worker之间采用NCCL进行通信。
  - **MultiWorkerMirrored**Strategy：与上面的类似，不同的是这种策略支持多机多卡、数据并行、同步更新的分布式策略、Reduce范式。在TF 1.15版本里，这个策略叫CollectiveAllReduceStrategy。
  - **ParameterServer**Strategy：经典的PS架构，多机多卡、数据并行、同步/异步更新
  - 使用Estimator+Strategy 实现分布式训练，参考[代码](https://github.com/kubeflow/tf-operator/blob/master/examples/v1/distribution_strategy/estimator-API/keras_model_to_estimator.py)
- 第三种方式 Keras + Strategy 是Tensorflow最新官方推荐的方案。主要是利用keras的高级API，配合Strategy实现多模式的分布式训练。
  - [代码](https://github.com/kubeflow/tf-operator/blob/master/examples/v1/distribution_strategy/keras-API/multi_worker_strategy-with-keras.py)

后两种方法都需要传入TF_CONFIG参数，没有就是单机的训练方式。Strategy会自动读取环境变量并应用相关信息。

TF_CONFIG的配置如下
- ![](https://pic2.zhimg.com/80/v2-dc8c2f647b9e359661e2a6f288ac1525_1440w.jpg)

### 单机单卡

- 单机单卡是最普通的情况，当然也是最简单的，示例代码如下：

```python
#coding=utf-8
#单机单卡，对于单机单卡，可以把参数和计算都定义再gpu上，不过如果参数模型比较大，显存不足等情况，就得放在cpu上
import  tensorflow as tf
with tf.device('/cpu:0'):#也可以放在gpu上
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/gpu:0'):
    addwb=w+b
    mutwb=w*b
init=tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)
    np1,np2=sess.run([addwb,mutwb])
    print np1,np2
```

### 单机多卡

- 单机多卡，只要用device直接指定设备，就可以进行训练，SGD采用各个卡的平均值
- 问题：除了取均值，还有别的方式吗？

```python
#coding=utf-8
#单机多卡：一般采用共享操作定义在cpu上，然后并行操作定义在各自的gpu上，比如对于深度学习来说，我们一般把参数定义、参数梯度更新统一放在cpu上，各个gpu通过各自计算各自batch数据的梯度值，然后统一传到cpu上，由cpu计算求取平均值，cpu更新参数。具体的深度学习多卡训练代码，请参考：https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py
import  tensorflow as tf
  
with tf.device('/cpu:0'):
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/gpu:0'):
    addwb=w+b
with tf.device('/gpu:1'):
    mutwb=w*b
  
ini=tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(ini)
    while 1:
        print sess.run([addwb,mutwb])
```
- 多个 GPU 上运行 TensorFlow，则可以采用多塔式方式构建模型，其中每个塔都会分配给不同 GPU。例如：

```python
# Creates a graph.
c = []
for d in ['/device:GPU:2', '/device:GPU:3']:
  with tf.device(d):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])
    c.append(tf.matmul(a, b))
with tf.device('/cpu:0'):
  sum = tf.add_n(c)
# Creates a session with log_device_placement set to True.
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
# Runs the op.
print(sess.run(sum))
```

- 【2020-5-20】每个gpu的梯度要累加起来，单独计算

```python
        # train op def
        tower_grads = []
        for i in xrange(FLAGS.num_gpus):
            with tf.device('/gpu:{}'.format(i)):
                with tf.name_scope('tower_{}'.format(i)):
                    next_batch = dhs.get_next_batch()
                    cnn.inference(
                        next_batch[0], next_batch[1], next_batch[2],
                        dropout_keep_prob=FLAGS.dropout_keep_prob,
                        input_dropout_keep_prob=FLAGS.input_dropout_keep_prob,
                        phase_train=True)
                    grads = optimizer.compute_gradients(cnn.loss)
                    tower_grads.append(grads)
        grads = average_gradients(tower_grads)
        train_op = optimizer.apply_gradients(grads, global_step=global_step)

def average_gradients(tower_grads):
    """
    Calculate the average gradient for each shared variable across all towers.
    Note that this function provides a synchronization point across all towers.
    NOTE: This function is copied from cifar codes in tensorflow tutorial with minor
    modification.
    Args:
        tower_grads: List of lists of (gradient, variable) tuples. The outer list
            is over individual gradients. The inner list is over the gradient
            calculation for each tower.
    Returns:
       List of pairs of (gradient, variable) where the gradient has been averaged
       across all towers.
    """
    average_grads = []
    for grad_and_vars in zip(*tower_grads):
        # Note that each grad_and_vars looks like the following:
        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))
        grads = []
        for g, _ in grad_and_vars:
            # Add 0 dimension to the gradients to represent the tower.
            # NOTE: if batch norm applied, the grad of conv-maxpool-n/b will be
            #       None
            if g is None:
                continue
            expanded_g = tf.expand_dims(g, 0)

            # Append on a 'tower' dimension which we will average over below.
            grads.append(expanded_g)

        # Average over the 'tower' dimension.
        if grads:
            grad = tf.concat(axis=0, values=grads)
            grad = tf.reduce_mean(grad, 0)
        else:
            grad = None

        # Keep in mind that the Variables are redundant because they are shared
        # across towers. So .. we will just return the first tower's pointer to
        # the Variable.
        v = grad_and_vars[0][1]
        grad_and_var = (grad, v)
        average_grads.append(grad_and_var)
    return average_grads
```

- 参考官网：[TensorFlow with multiple GPUs](https://jhui.github.io/2017/03/07/TensorFlow-GPU/)

### 多机多卡

- 一、基本概念
    - Cluster、Job、task概念：三者可以简单的看成是层次关系
    - task相当于每台机器上的一个进程，多个task组成job；
    - job又有两种：ps参数服务、worker计算服务，组成cluster。
- 二、同步SGD与异步SGD
    - 1、同步更新：各个用于并行计算的电脑，计算完各自的batch 后，求取梯度值，把梯度值统一送到ps服务机器中，由ps服务机器求取梯度平均值，更新ps服务器上的参数。
        - 如下图所示，可以看成有四台电脑，第一台电脑用于存储参数、共享参数、共享计算，可以简单的理解成内存、计算共享专用的区域，也就是ps job；另外三台电脑用于并行计算的，也就是worker task。
        - 这种计算方法存在的缺陷是：每一轮的梯度更新，都要等到A、B、C三台电脑都计算完毕后，才能更新参数，也就是迭代更新速度取决与A、B、C三台中，最慢的那一台电脑，所以采用同步更新的方法，建议A、B、C三台的计算能力都不想。
    - 2、异步更新：ps服务器收到只要收到一台机器的梯度值，就直接进行参数更新，无需等待其它机器。这种迭代方法比较不稳定，收敛曲线震动比较厉害，因为当A机器计算完更新了ps中的参数，可能B机器还是在用上一次迭代的旧版参数值。

代码编写
- 1、定义集群
    - 比如假设上面的图所示，我们有四台电脑，名字假设为：A、B、C、D，那么集群可以定义如下

```python
#coding=utf-8
#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练
import  tensorflow as tf
#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "A_IP:2222",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
        "B_IP:1234"#第二台机器的IP地址 /job:worker/task:1
        "C_IP:2222"#第三台机器的IP地址 /job:worker/task:2
    ],
    "ps": [
        "D_IP:2222",#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
```

然后需要写四分代码，这四分代码文件大部分相同，但是有几行代码是各不相同的。

- 2、在各台机器上，定义server
    - 比如A机器上的代码server要定义如下：
```python
server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
```

- 3、在代码中，指定device
```python
with tf.device('/job:ps/task:0'):#参数定义在机器D上
    w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
    b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
with tf.device('/job:worker/task:0/cpu:0'):#在机器A cpu上运行
    addwb=w+b
with tf.device('/job:worker/task:1/cpu:0'):#在机器B cpu上运行
    mutwb=w*b
with tf.device('/job:worker/task:2/cpu:0'):#在机器C cpu上运行
    divwb=w/b
```

在深度学习训练中，一般图的计算，对于每个worker task来说，都是相同的，所以我们会把所有图计算、变量定义等代码，都写到下面这个语句下：
```python
with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:indexi',cluster=cluster))
```

函数replica_deviec_setter会自动把变量参数定义部分定义到ps服务中(如果ps有多个任务，那么自动分配)。下面举个例子，假设现在有两台机器A、B，A用于计算服务，B用于参数服务，那么代码如下：

```python
#coding=utf-8
#上面是因为worker计算内容各不相同，不过再深度学习中，一般每个worker的计算内容是一样的，
# 以为都是计算神经网络的每个batch 前向传导，所以一般代码是重用的
import  tensorflow as tf
#现在假设我们有A、B台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "192.168.11.105:1234",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
    ],
    "ps": [
        "192.168.11.130:2223"#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
  
#不同的机器，下面这一行代码各不相同，server可以根据job_name、task_index两个参数，查找到集群cluster中对应的机器
  
isps=False
if isps:
    server=tf.train.Server(cluster,job_name='ps',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    server.join()
else:
    server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:0',cluster=cluster)):
        w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
        b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
        addwb=w+b
        mutwb=w*b
        divwb=w/b

saver = tf.train.Saver()
summary_op = tf.merge_all_summaries()
init_op = tf.initialize_all_variables()
sv = tf.train.Supervisor(init_op=init_op, summary_op=summary_op, saver=saver)
with sv.managed_session(server.target) as sess:
    while 1:
        print sess.run([addwb,mutwb,divwb])
```

把该代码在机器A上运行，你会发现，程序会进入等候状态，等候用于ps参数服务的机器启动，才会运行。

因此接着我们在机器B上运行如下代码：

```python
#coding=utf-8
#上面是因为worker计算内容各不相同，不过再深度学习中，一般每个worker的计算内容是一样的，
# 以为都是计算神经网络的每个batch 前向传导，所以一般代码是重用的
#coding=utf-8
#多台机器，每台机器有一个显卡、或者多个显卡，这种训练叫做分布式训练
import  tensorflow as tf
#现在假设我们有A、B、C、D四台机器，首先需要在各台机器上写一份代码，并跑起来，各机器上的代码内容大部分相同
# ，除了开始定义的时候，需要各自指定该台机器的task之外。以机器A为例子，A机器上的代码如下：
cluster=tf.train.ClusterSpec({
    "worker": [
        "192.168.11.105:1234",#格式 IP地址：端口号，第一台机器A的IP地址 ,在代码中需要用这台机器计算的时候，就要定义：/job:worker/task:0
    ],
    "ps": [
        "192.168.11.130:2223"#第四台机器的IP地址 对应到代码块：/job:ps/task:0
    ]})
  
#不同的机器，下面这一行代码各不相同，server可以根据job_name、task_index两个参数，查找到集群cluster中对应的机器
  
isps=True
if isps:
    server=tf.train.Server(cluster,job_name='ps',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    server.join()
else:
    server=tf.train.Server(cluster,job_name='worker',task_index=0)#找到‘worker’名字下的，task0，也就是机器A
    with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:0',cluster=cluster)):
        w=tf.get_variable('w',(2,2),tf.float32,initializer=tf.constant_initializer(2))
        b=tf.get_variable('b',(2,2),tf.float32,initializer=tf.constant_initializer(5))
        addwb=w+b
        mutwb=w*b
        divwb=w/b
  
saver = tf.train.Saver()
summary_op = tf.merge_all_summaries()
init_op = tf.initialize_all_variables()
sv = tf.train.Supervisor(init_op=init_op, summary_op=summary_op, saver=saver)
with sv.managed_session(server.target) as sess:
    while 1:
        print sess.run([addwb,mutwb,divwb])
```

- [Tensorflow官方指南](https://www.tensorflow.org/versions/master/how_tos/distributed/index.html)
- 分布式训练需要熟悉的函数：
    - tf.train.Server
    - tf.train.Supervisor
    - tf.train.SessionManager
    - tf.train.ClusterSpec
    - tf.train.replica_device_setter
    - tf.train.MonitoredTrainingSession
    - tf.train.MonitoredSession
    - tf.train.SingularMonitoredSession
    - tf.train.Scaffold
    - tf.train.SessionCreator
    - tf.train.ChiefSessionCreator
    - tf.train.WorkerSessionCreator

## Pytorch的分布式训练

相对Tensorflow，Pytorch简单的多。分布式训练主要有两个API：
- DataParallel(DP): **PS模式**，会有一张卡为reduce（parame server），实现简单，就一行代码
  - 将数据分割到多个GPU上。这是数据并行的典型，需要将模型复制到每个GPU上，并且一但GPU0计算出梯度，则需要同步梯度，这需要大量的GPU数据传输（类似PS模式）
- DistributedDataParallel(DDP): **All-Reduce模式**，单机多卡/多级多卡皆可。官方建议API
  - 每个GPU的进程中创建模型副本，并只让数据的一部分对改GPU可用。因为每个GPU中的模型是独立运行的，所以在所有的模型都计算出梯度后，才会在模型之间同步梯度（类似All-reduce）
  - DDP每个batch只需要一次数据传输；而DP可能存在多次数据同步(不用worker之间可能快慢不一样)。

### 1、DataParallel

代码

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import os

input_size = 5
output_size = 2
batch_size = 30
data_size = 30

class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len

rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)

class Model(nn.Module):
    # Our model

    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print("  In Model: input size", input.size(),
              "output size", output.size())
        return output
model = Model(input_size, output_size)

if torch.cuda.is_available():
    model.cuda()

if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    # 就这一行！！！！
    model = nn.DataParallel(model)

for data in rand_loader:
    if torch.cuda.is_available():
        input_var = Variable(data.cuda())
    else:
        input_var = Variable(data)
    output = model(input_var)
    print("Outside: input size", input_var.size(), "output_size", output.size())
```

### 2、DDP（官方建议）

DDP采用All-Reduce架构，单机多卡、多机多卡都能用。

需要注意的是：DDP并不会自动shard数据
1. 如果自己写数据流，得根据torch.distributed.get_rank()去shard数据，获取自己应用的一份
2. 如果用Dataset API，则需要在定义Dataloader的时候用DistributedSampler去shard

```python
sampler = DistributedSampler(dataset) # 这个sampler会自动分配数据到各个gpu上
DataLoader(dataset, batch_size=batch_size, sampler=sampler)
```

完整代码如下：

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import os
from torch.utils.data.distributed import DistributedSampler
# 1) 初始化
torch.distributed.init_process_group(backend="nccl")

input_size = 5
output_size = 2
batch_size = 30
data_size = 90

# 2） 配置每个进程的gpu
local_rank = torch.distributed.get_rank()
torch.cuda.set_device(local_rank)
device = torch.device("cuda", local_rank)

class RandomDataset(Dataset):
    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size).to('cuda')

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len

dataset = RandomDataset(input_size, data_size)
# 3）使用DistributedSampler
rand_loader = DataLoader(dataset=dataset,
                         batch_size=batch_size,
                         sampler=DistributedSampler(dataset))

class Model(nn.Module):
    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print("  In Model: input size", input.size(),
              "output size", output.size())
        return output

model = Model(input_size, output_size)

# 4) 封装之前要把模型移到对应的gpu
model.to(device)

if torch.cuda.device_count() > 1:
    print("Let's use", torch.cuda.device_count(), "GPUs!")
    # 5) 封装
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank)

for data in rand_loader:
    if torch.cuda.is_available():
        input_var = data
    else:
        input_var = data

    output = model(input_var)
    print("Outside: input size", input_var.size(), "output_size", output.size())
```

执行脚本：

```shell
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 torch_ddp.py
```

apex加速(混合精度训练、并行训练、同步BN)可[参考](https://zhuanlan.zhihu.com/p/158375055)

## Horovod分布式训练

Horovod是Uber开源的跨平台的分布式训练工具，名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与Horovod设备之间的通信模式很像，有以下几个特点：
- 兼容TensorFlow、Keras和PyTorch机器学习框架。
- 使用Ring-AllReduce算法，对比Parameter Server算法，有着无需等待，负载均衡的优点。
- 实现简单，五分钟包教包会。

Horovod环境准备以及示例代码，可参考[上一篇](https://zhuanlan.zhihu.com/p/351693076)


# 结束


